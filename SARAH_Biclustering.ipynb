{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SARAH_Clustering.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPi1oQtmhKUncrjRBBJIdaf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sibylse/Diss/blob/master/SARAH_Biclustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qP-WgMgIEFuV",
        "colab_type": "text"
      },
      "source": [
        "Code from the [proximal SGD blogpost](http://pmelchior.net/blog/proximal-matrix-factorization-in-pytorch.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ky8-gaL_E4T",
        "colab_type": "text"
      },
      "source": [
        "From the [alternating minimization pytorch](https://medium.com/@rinabuoy13/explicit-recommender-system-matrix-factorization-in-pytorch-f3779bb55d74) blog:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORWXvgCNj07r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e741ad9d-35b0-41d3-e59d-9c755d9a9100"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.autograd import Variable\n",
        "import collections\n",
        "rng = np.random.default_rng()\n",
        "cuda = torch.cuda.is_available()\n",
        "dev = torch.device(\"cuda\") if cuda else torch.device(\"cpu\")\n",
        "dev"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDvXlyYIgaUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generateBinaryFactor(n, r, q):\n",
        "  B=np.zeros((n,r))\n",
        "  l= int(np.ceil(n*0.01)) #lower bound of uniquely assigned ones per cluster\n",
        "  t = r*l # end of block-diagonal part\n",
        "  for s in range(r):\n",
        "    B[s*l:(s+1)*l,s]=1 #create block for diagonal\n",
        "    B[t:,s]= rng.binomial(1, q-0.01, n-t)\n",
        "  return B"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5etI81jivmO",
        "colab_type": "text"
      },
      "source": [
        "Generate a uniformly distributed or diagonal-dominant middle factor matrix C."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgxbHezFICT0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "2308cc73-1e32-44a9-a594-ab3bcb1d75e4"
      },
      "source": [
        "r=3\n",
        "m=300 \n",
        "n=200\n",
        "Y = generateBinaryFactor(m, r, 0.2)\n",
        "X = generateBinaryFactor(n, r, 0.2)\n",
        "# Make uniformly distributed matrix:\n",
        "C = np.round(rng.uniform(0,5,(r,r)),2) \n",
        "# Make diagonal-dominant matrix:\n",
        "# C= np.round(rng.uniform(0,0.5,(r,r)),2) + np.diag(rng.normal(1,0.1,r))\n",
        "C"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4.21, 4.23, 0.5 ],\n",
              "       [1.01, 3.82, 2.7 ],\n",
              "       [4.78, 1.05, 2.86]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqzl_Sq-OWW-",
        "colab_type": "text"
      },
      "source": [
        "The data matrix $D=YCX^\\top+N$ where $N$ is the noise matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Msa2_SFGKHxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "D = Y@C@X.T + np.round(rng.normal(0,0.1,(m,n)),2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPZ3ajhHOHL7",
        "colab_type": "text"
      },
      "source": [
        "The loss of the ground truth is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReEPmVOIhtAZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "637a88e0-8ea2-4c46-d0c0-90e5b3128d0f"
      },
      "source": [
        "np.mean((D-Y@C@X.T)**2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.010031036666666666"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0jVMLmP3Rv4",
        "colab_type": "text"
      },
      "source": [
        "Custom dataset class on the basis of [this](https://www.kaggle.com/pinocookie/pytorch-dataset-and-dataloader) code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp6AZSgG28q-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataMatrix(Dataset):\n",
        "        \n",
        "    def __init__(self, D):\n",
        "        self.data_ = torch.from_numpy(D).double()\n",
        "        self.m_ = D.shape[0]\n",
        "        self.n_ = D.shape[1]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.m_*self.n_\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        j = int(index/self.n_)\n",
        "        i = index%self.n_       \n",
        "        return torch.tensor([j,i]), self.data_[j,i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACKRfxSwOsj2",
        "colab_type": "text"
      },
      "source": [
        "Initialize batch loaders which partition the set of all tupels $(j,i)$ which are indices of the data matrix ($1\\leq i\\leq n, 1\\leq j\\leq m$) every epoch into batches containing $\\texttt{bs}$ elements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01gENeceG05J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2c545426-3899-4880-d328-014889f62d9f"
      },
      "source": [
        "bs = int(n*m*30/100) #originally 10%\n",
        "train_loader = DataLoader(DataMatrix(D), batch_size=bs, shuffle=True)\n",
        "test_loader = DataLoader(DataMatrix(Y@C@X.T), batch_size=1000)\n",
        "#I_b=torch.eye(bs)\n",
        "print(bs, len(train_loader))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18000 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NX3qb0-cVVEP",
        "colab_type": "text"
      },
      "source": [
        "Models: Matrix Factorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7fn9nYy_TPl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MatrixFactorization(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self, m, n, r=20):\n",
        "        super().__init__()\n",
        "        self.Y = torch.nn.Embedding(m, r).double()\n",
        "        self.X = torch.nn.Embedding(n, r).double()\n",
        "        self.C = torch.nn.Embedding(r, r).double()\n",
        "        torch.nn.init.uniform_(self.Y.weight)\n",
        "        torch.nn.init.uniform_(self.X.weight)\n",
        "        torch.nn.init.uniform_(self.C.weight)\n",
        "        \n",
        "    def forward(self, idx):\n",
        "      #j and i are torch tensors, denoting a set of indices i and j\n",
        "      YC =torch.matmul(self.Y(idx[:,0]),self.C.weight) \n",
        "      YCX_batch = (YC * self.X(idx[:,1])).sum(1,keepdim=True)\n",
        "      return YCX_batch.squeeze() #reduces every 1x dimension for tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRM1jHSta7-Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "95cd56af-6c48-46e3-ab85-26aeeec7656b"
      },
      "source": [
        "model = MatrixFactorization(n, m, r=2)\n",
        "model.to(dev)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MatrixFactorization(\n",
              "  (Y): Embedding(200, 2)\n",
              "  (X): Embedding(300, 2)\n",
              "  (C): Embedding(2, 2)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCWSS3UTt-I9",
        "colab_type": "text"
      },
      "source": [
        "Get batch-stepsizes by Lipschitz constants of the gradients:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kdLgfbIEGUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stepsizeX(model,batch):\n",
        "  with torch.no_grad():\n",
        "    if batch is None:\n",
        "      YC = torch.matmul(model.Y.weight,model.C.weight)\n",
        "      L = 2*torch.sqrt((torch.matmul(torch.transpose(YC,0,1),YC)**2).sum())/n/m\n",
        "    else:\n",
        "      n_array.fill_(0)\n",
        "      YC = torch.matmul(model.Y.weight,model.C.weight)\n",
        "      y = (YC**2).sum(1)[batch[:,0]]\n",
        "      n_array.index_add_(0,batch[:,1],y)\n",
        "      L= n_array.max().item()/bs*2\n",
        "    return 1/2/max(L,0.001)\n",
        "def stepsizeY(model,batch):\n",
        "  with torch.no_grad():\n",
        "    if batch is None:\n",
        "      XCt = torch.matmul(model.X.weight,torch.transpose(model.C.weight,0,1))\n",
        "      L = 2*torch.sqrt((torch.matmul(torch.transpose(XCt,0,1),XCt)**2).sum())/n/m\n",
        "    else:\n",
        "      m_array.fill_(0)\n",
        "      XCt = torch.matmul(model.X.weight,torch.transpose(model.C.weight,0,1))\n",
        "      x = (XCt**2).sum(1)[batch[:,1]]\n",
        "      m_array.index_add_(0,batch[:,0],x)\n",
        "      L= m_array.max().item()/bs*2\n",
        "    return 1/2/max(L,0.001)\n",
        "def stepsizeC(model,batch):\n",
        "  with torch.no_grad():\n",
        "    if batch is None:\n",
        "      L = 2*(model.X.weight**2).sum()*(model.Y.weight**2).sum()/n/m\n",
        "    else:\n",
        "      x = (model.X.weight**2).sum(1)[batch[:,1]]\n",
        "      y = (model.Y.weight**2).sum(1)[batch[:,0]]\n",
        "      L = torch.dot(x,y).item()\n",
        "      L = L/bs*2\n",
        "    return 1/2/max(L,0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TB3THgGj4DER",
        "colab_type": "text"
      },
      "source": [
        "Prox operators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCL1QdbU6qCt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def phi(x):\n",
        "    return 1-torch.abs(1-2*x)\n",
        "def prox_binary_(x,lambdas,lr,alpha=-1e-8):\n",
        "  with torch.no_grad():\n",
        "    idx_up = x>0.5\n",
        "    idx_down = x<=0.5\n",
        "    x[idx_up] += 2*lr*lambdas[idx_up]\n",
        "    x[idx_down] -= 2*lr*lambdas[idx_down]\n",
        "    x[x>1] = 1 \n",
        "    x[x<0] = 0 \n",
        "    lambdas.add_(phi(x)-1,alpha=alpha)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW4RelrWe4Y6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prox_pos_(X,weight,lr,alpha=0):\n",
        "  with torch.no_grad():\n",
        "    X[X<0] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi-dOp83VYG6",
        "colab_type": "text"
      },
      "source": [
        "Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMryWfaHR74l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.optim.optimizer import Optimizer,required\n",
        "\n",
        "class SARAH(Optimizer):\n",
        "    r\"\"\"Implements SARAH\n",
        "    \"\"\"\n",
        "    def __init__(self, params, params_prev, lr=required, weight_decay=0):\n",
        "\n",
        "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
        "        super(SARAH, self).__init__(params, defaults)\n",
        "        self.grad_buff = None\n",
        "        self.params_prev = params_prev\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(SARAH, self).__setstate__(state)\n",
        "    \n",
        "    def zero_grad(self):\n",
        "        super(SARAH, self).zero_grad()\n",
        "        for p in self.params_prev:\n",
        "            if p.grad is not None:\n",
        "                p.grad.detach_()\n",
        "                p.grad.zero_()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            weight_decay = group['weight_decay']\n",
        "            for x, prev_x in zip(group['params'],self.params_prev):\n",
        "                if x.grad is None:\n",
        "                    continue\n",
        "                if weight_decay != 0:\n",
        "                    x.grad.add_(x, alpha=weight_decay)\n",
        "                # x.grad gradient has same shape like the parameter but \n",
        "                # zeros at the positions which are not updated.\n",
        "                # grad_p has same size as batch times r \n",
        "                #param_state = self.state[x]\n",
        "                if self.grad_buff is None: #do full gradient update\n",
        "                    self.grad_buff = torch.clone(x.grad).detach() \n",
        "                else:\n",
        "                    self.grad_buff.add_(x.grad, alpha=1)\n",
        "                    self.grad_buff.add_(prev_x.grad, alpha=-1) #g_t = g_t-1  +  grad_t - grad_t-1\n",
        "                prev_x.mul_(0).add_(torch.clone(x),alpha=1)  \n",
        "                x.add_(x.grad, alpha=-group['lr'])\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NCBk6tYdQj28",
        "colab": {}
      },
      "source": [
        "def train(epoch,alpha):\n",
        "  model.train()\n",
        "  model_prev.train()\n",
        "  cum_loss = 0.\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "      lr_mean, lambda_mean =0,0\n",
        "      if cuda:\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "      data, target = Variable(data), Variable(target.double())\n",
        "      \n",
        "      for group in param_list:\n",
        "        optimizer = group[\"optimizer\"]\n",
        "        optParam = optimizer.param_groups[0]\n",
        "        stepsize = group[\"step\"]\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        output_prev = model_prev(data)\n",
        "        loss = loss_func(output, target)\n",
        "        loss_prev = loss_func(output_prev, target)\n",
        "        loss.backward()\n",
        "        loss_prev.backward()\n",
        "        #print(\"grad:\",optParam['params'][0].grad)\n",
        "        #print(\"grad nonzero:\",(optParam['params'][0].grad !=0)*reg_weight*optParam['lr'])\n",
        "        optParam['lr'] =stepsize(model,data)#/2\n",
        "        lr_mean += optParam['lr']\n",
        "        optimizer.step()\n",
        "        if \"prox\" in group:\n",
        "            prox = group[\"prox\"]\n",
        "            #The whole factor matrix is proxed for one batch? \n",
        "            #Most of the time, this is ok because not often there is no tupel for one row/column.\n",
        "            prox(optParam['params'][0].data,group[\"lambda\"],optParam['lr'], alpha=alpha) \n",
        "            lambda_mean += torch.mean(group[\"lambda\"])\n",
        "        cum_loss+=loss.item()/3\n",
        "        if batch_idx % 2 +1 == 0:\n",
        "            print('Train Epoch:\\t\\t\\t {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "  if epoch % 3 == 0:\n",
        "    print('==Train Epoch:\\t\\t\\t {} \\tLoss: {:.6f}\\t lambda: {:.3e}\\t lr: {:.3f}'.format(\n",
        "        epoch, cum_loss/len(train_loader),lambda_mean/2,lr_mean/3))\n",
        "\n",
        "#\n",
        "# Train full grad\n",
        "#\n",
        "def train_full_grad(epoch,alpha):\n",
        "  model.train()\n",
        "  cum_loss = 0.\n",
        "  lambda_mean, lr_mean=0,0\n",
        "  #for every factor matrix - optimizer\n",
        "  for group in param_list: \n",
        "      optimizer = group[\"optimizer\"]\n",
        "      optimizer.grad_buff =None #Sign that we do a full grad update\n",
        "      optimizer.zero_grad()\n",
        "      optParam = optimizer.param_groups[0]\n",
        "      stepsize = group[\"step\"]\n",
        "      for batch_idx, (data, target) in enumerate(train_loader):\n",
        "          if cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "          data, target = Variable(data), Variable(target.double())\n",
        "            \n",
        "          output = model(data)\n",
        "          # loss is mean squared error over a batch \n",
        "          loss = loss_func_full(output, target)/n/m\n",
        "          loss.backward()\n",
        "          cum_loss+=loss.item()\n",
        "      # gamma = 1/(2L), L is normalized with bs but this is a full grad update  \n",
        "      optParam['lr'] =stepsize(model,None)#/2 #TODO stepsize is computed for one batch! \n",
        "      lr_mean+= optParam['lr']\n",
        "      optimizer.step()\n",
        "      if \"prox\" in group:\n",
        "          prox = group[\"prox\"]\n",
        "          prox(optParam['params'][0].data,group[\"lambda\"],optParam['lr'], alpha=alpha) \n",
        "          lambda_mean += torch.mean(group[\"lambda\"])\n",
        "      if batch_idx % 2 +1 == 0:\n",
        "          print('Train Full Grad Batch:\\t {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "  if epoch % 3 == 0:\n",
        "    print('==Train Full Grad Epoch:\\t {} \\tLoss: {:.6f}\\t lambda: {:.3e}\\t lr: {:.3f}'.format(\n",
        "        epoch, cum_loss/3,lambda_mean/2,lr_mean/3))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3U0TuUT4MtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        if cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        data, target = Variable(data), Variable(target)\n",
        "\n",
        "        output = model(data)\n",
        "        # sum up batch loss\n",
        "        test_loss += loss_func(output, target).item()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    print('\\nTest set: Average loss: {:.4f}\\n'.format(test_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRb6bddpbFiQ",
        "colab_type": "text"
      },
      "source": [
        "Can L be 0? If not, there might be an error! I've seen that L_C is zero"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wP2VSzKamWUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def is_binary(A):\n",
        "  return ((A<1) *(A>0)).sum().item() == 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO_xdwLWV_lb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_array = torch.zeros(n).double().to(dev)\n",
        "m_array = torch.zeros(m).double().to(dev)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QlF9qB6xWJ2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "84d62611-b0e3-4e06-d6b0-4376947110a4"
      },
      "source": [
        "model = MatrixFactorization(m, n, r=r)\n",
        "model.to(dev)\n",
        "model_prev = MatrixFactorization(m, n, r=r)\n",
        "model_prev.to(dev)\n",
        "optimizerY = SARAH([model.Y.weight],[model_prev.Y.weight], lr=0.1) # learning rate\n",
        "optimizerX = SARAH([model.X.weight],[model_prev.X.weight], lr=0.1) # learning rate\n",
        "optimizerC = SARAH([model.C.weight],[model_prev.C.weight], lr=0.1)\n",
        "lambdasY = torch.zeros_like(model.Y.weight) #TODO is prev_model also proxed?\n",
        "lambdasX = torch.zeros_like(model.X.weight)\n",
        "param_list = [{'optimizer': optimizerX, 'step': stepsizeX, 'prox':prox_binary_, 'lambda':lambdasX},\n",
        "              {'optimizer': optimizerY, 'step': stepsizeY, 'prox':prox_binary_, 'lambda':lambdasY},\n",
        "              {'optimizer': optimizerC, 'step': stepsizeC, 'prox':prox_pos_, 'lambda':torch.tensor([0.0])}]\n",
        "loss_func = torch.nn.MSELoss()\n",
        "loss_func_full = torch.nn.MSELoss(reduction='sum')\n",
        "epoch = 1\n",
        "test()\n",
        "alpha=-1e-8\n",
        "#alpha = -1/n/m/len(train_loader)/10\n",
        "thresh=0.1\n",
        "full_grad_prob=0.3\n",
        "#while not is_binary(model.Y.fact.weight.data) or not is_binary(model.X.fact.weight.data):\n",
        "while not is_binary(model.Y.weight.data) or not is_binary(model.X.weight.data):\n",
        "    full_batch_grad = np.random.binomial(1,full_grad_prob)\n",
        "    if full_batch_grad:\n",
        "      train_full_grad(epoch,alpha)\n",
        "    else:\n",
        "      train(epoch,alpha)\n",
        "    if epoch % 6 == 0:\n",
        "      #phiX, phiY = torch.mean(phi(model.X.fact.weight.data)), torch.mean(phi(model.Y.fact.weight.data))\n",
        "      phiX, phiY = torch.mean(phi(model.X.weight.data)), torch.mean(phi(model.Y.weight.data))\n",
        "      print('--\\t\\t\\tphi(X):\\t {:.3f} \\tphi(Y): {:.3f}'.format(phiX,phiY))\n",
        "      if max(phiX,phiY) < thresh:\n",
        "        alpha*=2\n",
        "        thresh/=2\n",
        "    epoch+=1\n",
        "    if epoch % 50 == 0:\n",
        "      test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 4.7010\n",
            "\n",
            "==Train Epoch:\t\t\t 3 \tLoss: 0.642183\t lambda: 7.969e-08\t lr: 10.663\n",
            "==Train Epoch:\t\t\t 6 \tLoss: 0.521559\t lambda: 1.447e-07\t lr: 8.140\n",
            "--\t\t\tphi(X):\t 0.272 \tphi(Y): 0.274\n",
            "==Train Epoch:\t\t\t 9 \tLoss: 0.467683\t lambda: 2.317e-07\t lr: 7.031\n",
            "==Train Epoch:\t\t\t 12 \tLoss: 0.444347\t lambda: 2.752e-07\t lr: 7.029\n",
            "--\t\t\tphi(X):\t 0.277 \tphi(Y): 0.273\n",
            "==Train Epoch:\t\t\t 15 \tLoss: 0.448108\t lambda: 3.620e-07\t lr: 6.492\n",
            "==Train Full Grad Epoch:\t 18 \tLoss: 0.452143\t lambda: 4.269e-07\t lr: 4.167\n",
            "--\t\t\tphi(X):\t 0.283 \tphi(Y): 0.277\n",
            "==Train Epoch:\t\t\t 21 \tLoss: 0.428145\t lambda: 4.917e-07\t lr: 6.701\n",
            "==Train Epoch:\t\t\t 24 \tLoss: 0.422547\t lambda: 5.564e-07\t lr: 6.409\n",
            "--\t\t\tphi(X):\t 0.284 \tphi(Y): 0.279\n",
            "==Train Epoch:\t\t\t 27 \tLoss: 0.421124\t lambda: 6.426e-07\t lr: 5.883\n",
            "==Train Full Grad Epoch:\t 30 \tLoss: 0.421714\t lambda: 6.857e-07\t lr: 3.788\n",
            "--\t\t\tphi(X):\t 0.284 \tphi(Y): 0.279\n",
            "==Train Epoch:\t\t\t 33 \tLoss: 0.416888\t lambda: 7.504e-07\t lr: 5.998\n",
            "==Train Full Grad Epoch:\t 36 \tLoss: 0.415303\t lambda: 7.720e-07\t lr: 3.715\n",
            "--\t\t\tphi(X):\t 0.283 \tphi(Y): 0.279\n",
            "==Train Epoch:\t\t\t 39 \tLoss: 0.408446\t lambda: 8.367e-07\t lr: 6.400\n",
            "==Train Epoch:\t\t\t 42 \tLoss: 0.419194\t lambda: 9.230e-07\t lr: 6.232\n",
            "--\t\t\tphi(X):\t 0.281 \tphi(Y): 0.280\n",
            "==Train Full Grad Epoch:\t 45 \tLoss: 0.427507\t lambda: 9.877e-07\t lr: 3.611\n",
            "==Train Epoch:\t\t\t 48 \tLoss: 0.408818\t lambda: 1.053e-06\t lr: 6.201\n",
            "--\t\t\tphi(X):\t 0.281 \tphi(Y): 0.279\n",
            "\n",
            "Test set: Average loss: 0.4212\n",
            "\n",
            "==Train Epoch:\t\t\t 51 \tLoss: 0.403675\t lambda: 1.117e-06\t lr: 6.403\n",
            "==Train Full Grad Epoch:\t 54 \tLoss: 0.420527\t lambda: 1.182e-06\t lr: 3.561\n",
            "--\t\t\tphi(X):\t 0.281 \tphi(Y): 0.277\n",
            "==Train Epoch:\t\t\t 57 \tLoss: 0.404325\t lambda: 1.225e-06\t lr: 6.104\n",
            "==Train Epoch:\t\t\t 60 \tLoss: 0.400996\t lambda: 1.312e-06\t lr: 6.536\n",
            "--\t\t\tphi(X):\t 0.278 \tphi(Y): 0.276\n",
            "==Train Full Grad Epoch:\t 63 \tLoss: 0.409590\t lambda: 1.356e-06\t lr: 3.541\n",
            "==Train Full Grad Epoch:\t 66 \tLoss: 0.395958\t lambda: 1.421e-06\t lr: 3.480\n",
            "--\t\t\tphi(X):\t 0.272 \tphi(Y): 0.272\n",
            "==Train Full Grad Epoch:\t 69 \tLoss: 0.375492\t lambda: 1.487e-06\t lr: 3.435\n",
            "==Train Epoch:\t\t\t 72 \tLoss: 0.322636\t lambda: 1.575e-06\t lr: 5.892\n",
            "--\t\t\tphi(X):\t 0.254 \tphi(Y): 0.257\n",
            "==Train Epoch:\t\t\t 75 \tLoss: 0.253872\t lambda: 1.666e-06\t lr: 5.088\n",
            "==Train Full Grad Epoch:\t 78 \tLoss: 0.209341\t lambda: 1.735e-06\t lr: 3.233\n",
            "--\t\t\tphi(X):\t 0.220 \tphi(Y): 0.229\n",
            "==Train Epoch:\t\t\t 81 \tLoss: 0.182856\t lambda: 1.805e-06\t lr: 5.354\n",
            "==Train Epoch:\t\t\t 84 \tLoss: 0.170983\t lambda: 1.900e-06\t lr: 4.812\n",
            "--\t\t\tphi(X):\t 0.205 \tphi(Y): 0.217\n",
            "==Train Epoch:\t\t\t 87 \tLoss: 0.165617\t lambda: 1.971e-06\t lr: 5.094\n",
            "==Train Epoch:\t\t\t 90 \tLoss: 0.161994\t lambda: 2.065e-06\t lr: 5.114\n",
            "--\t\t\tphi(X):\t 0.202 \tphi(Y): 0.215\n",
            "==Train Epoch:\t\t\t 93 \tLoss: 0.159570\t lambda: 2.137e-06\t lr: 5.330\n",
            "==Train Epoch:\t\t\t 96 \tLoss: 0.158910\t lambda: 2.232e-06\t lr: 5.220\n",
            "--\t\t\tphi(X):\t 0.201 \tphi(Y): 0.216\n",
            "==Train Full Grad Epoch:\t 99 \tLoss: 0.162708\t lambda: 2.303e-06\t lr: 3.185\n",
            "\n",
            "Test set: Average loss: 0.1502\n",
            "\n",
            "==Train Epoch:\t\t\t 102 \tLoss: 0.159641\t lambda: 2.398e-06\t lr: 5.304\n",
            "--\t\t\tphi(X):\t 0.205 \tphi(Y): 0.217\n",
            "==Train Full Grad Epoch:\t 105 \tLoss: 0.162506\t lambda: 2.469e-06\t lr: 3.165\n",
            "==Train Epoch:\t\t\t 108 \tLoss: 0.154175\t lambda: 2.516e-06\t lr: 5.197\n",
            "--\t\t\tphi(X):\t 0.206 \tphi(Y): 0.220\n",
            "==Train Epoch:\t\t\t 111 \tLoss: 0.155711\t lambda: 2.610e-06\t lr: 4.933\n",
            "==Train Full Grad Epoch:\t 114 \tLoss: 0.161046\t lambda: 2.681e-06\t lr: 3.159\n",
            "--\t\t\tphi(X):\t 0.204 \tphi(Y): 0.221\n",
            "==Train Epoch:\t\t\t 117 \tLoss: 0.154242\t lambda: 2.729e-06\t lr: 5.209\n",
            "==Train Full Grad Epoch:\t 120 \tLoss: 0.156240\t lambda: 2.752e-06\t lr: 3.147\n",
            "--\t\t\tphi(X):\t 0.204 \tphi(Y): 0.221\n",
            "==Train Epoch:\t\t\t 123 \tLoss: 0.156529\t lambda: 2.847e-06\t lr: 4.692\n",
            "==Train Full Grad Epoch:\t 126 \tLoss: 0.159864\t lambda: 2.894e-06\t lr: 3.116\n",
            "--\t\t\tphi(X):\t 0.204 \tphi(Y): 0.223\n",
            "==Train Epoch:\t\t\t 129 \tLoss: 0.154604\t lambda: 2.965e-06\t lr: 5.146\n",
            "==Train Full Grad Epoch:\t 132 \tLoss: 0.156014\t lambda: 3.012e-06\t lr: 3.127\n",
            "--\t\t\tphi(X):\t 0.204 \tphi(Y): 0.224\n",
            "==Train Epoch:\t\t\t 135 \tLoss: 0.153064\t lambda: 3.083e-06\t lr: 5.211\n",
            "==Train Full Grad Epoch:\t 138 \tLoss: 0.155744\t lambda: 3.130e-06\t lr: 3.122\n",
            "--\t\t\tphi(X):\t 0.204 \tphi(Y): 0.224\n",
            "==Train Epoch:\t\t\t 141 \tLoss: 0.154699\t lambda: 3.224e-06\t lr: 4.904\n",
            "==Train Epoch:\t\t\t 144 \tLoss: 0.155418\t lambda: 3.295e-06\t lr: 4.984\n",
            "--\t\t\tphi(X):\t 0.204 \tphi(Y): 0.225\n",
            "==Train Epoch:\t\t\t 147 \tLoss: 0.154996\t lambda: 3.389e-06\t lr: 4.814\n",
            "\n",
            "Test set: Average loss: 0.1497\n",
            "\n",
            "==Train Full Grad Epoch:\t 150 \tLoss: 0.157710\t lambda: 3.459e-06\t lr: 3.106\n",
            "--\t\t\tphi(X):\t 0.204 \tphi(Y): 0.227\n",
            "==Train Epoch:\t\t\t 153 \tLoss: 0.155123\t lambda: 3.530e-06\t lr: 5.037\n",
            "==Train Epoch:\t\t\t 156 \tLoss: 0.151430\t lambda: 3.601e-06\t lr: 5.067\n",
            "--\t\t\tphi(X):\t 0.205 \tphi(Y): 0.228\n",
            "==Train Epoch:\t\t\t 159 \tLoss: 0.152007\t lambda: 3.648e-06\t lr: 5.003\n",
            "==Train Epoch:\t\t\t 162 \tLoss: 0.153025\t lambda: 3.695e-06\t lr: 4.899\n",
            "--\t\t\tphi(X):\t 0.204 \tphi(Y): 0.228\n",
            "==Train Epoch:\t\t\t 165 \tLoss: 0.154588\t lambda: 3.789e-06\t lr: 5.097\n",
            "==Train Epoch:\t\t\t 168 \tLoss: 0.154725\t lambda: 3.883e-06\t lr: 4.815\n",
            "--\t\t\tphi(X):\t 0.204 \tphi(Y): 0.229\n",
            "==Train Full Grad Epoch:\t 171 \tLoss: 0.158536\t lambda: 3.930e-06\t lr: 3.073\n",
            "==Train Full Grad Epoch:\t 174 \tLoss: 0.157156\t lambda: 4.000e-06\t lr: 3.069\n",
            "--\t\t\tphi(X):\t 0.204 \tphi(Y): 0.229\n",
            "==Train Epoch:\t\t\t 177 \tLoss: 0.154384\t lambda: 4.094e-06\t lr: 4.710\n",
            "==Train Full Grad Epoch:\t 180 \tLoss: 0.158290\t lambda: 4.165e-06\t lr: 3.042\n",
            "--\t\t\tphi(X):\t 0.205 \tphi(Y): 0.229\n",
            "==Train Epoch:\t\t\t 183 \tLoss: 0.155080\t lambda: 4.259e-06\t lr: 4.773\n",
            "==Train Epoch:\t\t\t 186 \tLoss: 0.156464\t lambda: 4.329e-06\t lr: 5.079\n",
            "--\t\t\tphi(X):\t 0.203 \tphi(Y): 0.228\n",
            "==Train Epoch:\t\t\t 189 \tLoss: 0.154204\t lambda: 4.400e-06\t lr: 4.994\n",
            "==Train Epoch:\t\t\t 192 \tLoss: 0.154775\t lambda: 4.494e-06\t lr: 5.202\n",
            "--\t\t\tphi(X):\t 0.203 \tphi(Y): 0.229\n",
            "==Train Epoch:\t\t\t 195 \tLoss: 0.154935\t lambda: 4.588e-06\t lr: 4.869\n",
            "==Train Full Grad Epoch:\t 198 \tLoss: 0.158592\t lambda: 4.658e-06\t lr: 3.057\n",
            "--\t\t\tphi(X):\t 0.203 \tphi(Y): 0.230\n",
            "\n",
            "Test set: Average loss: 0.1507\n",
            "\n",
            "==Train Epoch:\t\t\t 201 \tLoss: 0.151106\t lambda: 4.729e-06\t lr: 5.124\n",
            "==Train Epoch:\t\t\t 204 \tLoss: 0.154094\t lambda: 4.799e-06\t lr: 4.919\n",
            "--\t\t\tphi(X):\t 0.203 \tphi(Y): 0.231\n",
            "==Train Epoch:\t\t\t 207 \tLoss: 0.152415\t lambda: 4.870e-06\t lr: 4.958\n",
            "==Train Epoch:\t\t\t 210 \tLoss: 0.153674\t lambda: 4.964e-06\t lr: 5.080\n",
            "--\t\t\tphi(X):\t 0.205 \tphi(Y): 0.231\n",
            "==Train Full Grad Epoch:\t 213 \tLoss: 0.157192\t lambda: 5.034e-06\t lr: 3.047\n",
            "==Train Epoch:\t\t\t 216 \tLoss: 0.154754\t lambda: 5.104e-06\t lr: 4.770\n",
            "--\t\t\tphi(X):\t 0.205 \tphi(Y): 0.229\n",
            "==Train Full Grad Epoch:\t 219 \tLoss: 0.157181\t lambda: 5.175e-06\t lr: 3.036\n",
            "==Train Epoch:\t\t\t 222 \tLoss: 0.152923\t lambda: 5.245e-06\t lr: 5.126\n",
            "--\t\t\tphi(X):\t 0.204 \tphi(Y): 0.230\n",
            "==Train Epoch:\t\t\t 225 \tLoss: 0.154986\t lambda: 5.339e-06\t lr: 4.933\n",
            "==Train Epoch:\t\t\t 228 \tLoss: 0.153646\t lambda: 5.433e-06\t lr: 5.291\n",
            "--\t\t\tphi(X):\t 0.205 \tphi(Y): 0.231\n",
            "==Train Epoch:\t\t\t 231 \tLoss: 0.153372\t lambda: 5.527e-06\t lr: 4.916\n",
            "==Train Epoch:\t\t\t 234 \tLoss: 0.153118\t lambda: 5.621e-06\t lr: 5.069\n",
            "--\t\t\tphi(X):\t 0.205 \tphi(Y): 0.230\n",
            "==Train Full Grad Epoch:\t 237 \tLoss: 0.157223\t lambda: 5.692e-06\t lr: 3.028\n",
            "==Train Epoch:\t\t\t 240 \tLoss: 0.152925\t lambda: 5.785e-06\t lr: 5.133\n",
            "--\t\t\tphi(X):\t 0.205 \tphi(Y): 0.229\n",
            "==Train Epoch:\t\t\t 243 \tLoss: 0.153683\t lambda: 5.856e-06\t lr: 5.084\n",
            "==Train Full Grad Epoch:\t 246 \tLoss: 0.154184\t lambda: 5.879e-06\t lr: 3.028\n",
            "--\t\t\tphi(X):\t 0.205 \tphi(Y): 0.230\n",
            "==Train Full Grad Epoch:\t 249 \tLoss: 0.153839\t lambda: 5.903e-06\t lr: 3.027\n",
            "\n",
            "Test set: Average loss: 0.1436\n",
            "\n",
            "==Train Full Grad Epoch:\t 252 \tLoss: 0.157154\t lambda: 5.950e-06\t lr: 3.028\n",
            "--\t\t\tphi(X):\t 0.204 \tphi(Y): 0.229\n",
            "==Train Full Grad Epoch:\t 255 \tLoss: 0.153882\t lambda: 5.973e-06\t lr: 3.027\n",
            "==Train Full Grad Epoch:\t 258 \tLoss: 0.156874\t lambda: 6.044e-06\t lr: 3.025\n",
            "--\t\t\tphi(X):\t 0.204 \tphi(Y): 0.230\n",
            "==Train Epoch:\t\t\t 261 \tLoss: 0.152634\t lambda: 6.138e-06\t lr: 5.096\n",
            "==Train Epoch:\t\t\t 264 \tLoss: 0.154591\t lambda: 6.232e-06\t lr: 5.154\n",
            "--\t\t\tphi(X):\t 0.206 \tphi(Y): 0.230\n",
            "==Train Full Grad Epoch:\t 267 \tLoss: 0.155124\t lambda: 6.278e-06\t lr: 3.025\n",
            "==Train Epoch:\t\t\t 270 \tLoss: 0.152098\t lambda: 6.349e-06\t lr: 5.489\n",
            "--\t\t\tphi(X):\t 0.206 \tphi(Y): 0.230\n",
            "==Train Full Grad Epoch:\t 273 \tLoss: 0.157518\t lambda: 6.396e-06\t lr: 3.018\n",
            "==Train Full Grad Epoch:\t 276 \tLoss: 0.154858\t lambda: 6.443e-06\t lr: 3.021\n",
            "--\t\t\tphi(X):\t 0.205 \tphi(Y): 0.229\n",
            "==Train Epoch:\t\t\t 279 \tLoss: 0.155969\t lambda: 6.537e-06\t lr: 5.113\n",
            "==Train Epoch:\t\t\t 282 \tLoss: 0.152869\t lambda: 6.631e-06\t lr: 5.076\n",
            "--\t\t\tphi(X):\t 0.205 \tphi(Y): 0.230\n",
            "==Train Epoch:\t\t\t 285 \tLoss: 0.153848\t lambda: 6.725e-06\t lr: 4.920\n",
            "==Train Epoch:\t\t\t 288 \tLoss: 0.154394\t lambda: 6.819e-06\t lr: 4.782\n",
            "--\t\t\tphi(X):\t 0.205 \tphi(Y): 0.229\n",
            "==Train Epoch:\t\t\t 291 \tLoss: 0.154734\t lambda: 6.913e-06\t lr: 4.781\n",
            "==Train Full Grad Epoch:\t 294 \tLoss: 0.154011\t lambda: 6.936e-06\t lr: 3.016\n",
            "--\t\t\tphi(X):\t 0.205 \tphi(Y): 0.229\n",
            "==Train Epoch:\t\t\t 297 \tLoss: 0.152812\t lambda: 7.030e-06\t lr: 4.951\n",
            "\n",
            "Test set: Average loss: 0.1497\n",
            "\n",
            "==Train Epoch:\t\t\t 300 \tLoss: 0.155014\t lambda: 7.124e-06\t lr: 4.949\n",
            "--\t\t\tphi(X):\t 0.205 \tphi(Y): 0.230\n",
            "==Train Epoch:\t\t\t 303 \tLoss: 0.152008\t lambda: 7.194e-06\t lr: 4.628\n",
            "==Train Full Grad Epoch:\t 306 \tLoss: 0.156749\t lambda: 7.265e-06\t lr: 3.020\n",
            "--\t\t\tphi(X):\t 0.204 \tphi(Y): 0.229\n",
            "==Train Epoch:\t\t\t 309 \tLoss: 0.154976\t lambda: 7.359e-06\t lr: 4.763\n",
            "==Train Epoch:\t\t\t 312 \tLoss: 0.154500\t lambda: 7.453e-06\t lr: 4.918\n",
            "--\t\t\tphi(X):\t 0.204 \tphi(Y): 0.229\n",
            "==Train Epoch:\t\t\t 315 \tLoss: 0.152935\t lambda: 7.523e-06\t lr: 4.706\n",
            "==Train Full Grad Epoch:\t 318 \tLoss: 0.156683\t lambda: 7.570e-06\t lr: 3.011\n",
            "--\t\t\tphi(X):\t 0.205 \tphi(Y): 0.228\n",
            "==Train Full Grad Epoch:\t 321 \tLoss: 0.158531\t lambda: 7.641e-06\t lr: 2.998\n",
            "==Train Full Grad Epoch:\t 324 \tLoss: 0.157096\t lambda: 7.712e-06\t lr: 3.003\n",
            "--\t\t\tphi(X):\t 0.203 \tphi(Y): 0.228\n",
            "==Train Full Grad Epoch:\t 327 \tLoss: 0.156938\t lambda: 7.782e-06\t lr: 3.001\n",
            "==Train Full Grad Epoch:\t 330 \tLoss: 0.156365\t lambda: 7.829e-06\t lr: 3.008\n",
            "--\t\t\tphi(X):\t 0.204 \tphi(Y): 0.228\n",
            "==Train Epoch:\t\t\t 333 \tLoss: 0.154041\t lambda: 7.900e-06\t lr: 4.629\n",
            "==Train Epoch:\t\t\t 336 \tLoss: 0.154256\t lambda: 7.994e-06\t lr: 5.057\n",
            "--\t\t\tphi(X):\t 0.204 \tphi(Y): 0.228\n",
            "==Train Full Grad Epoch:\t 339 \tLoss: 0.155860\t lambda: 8.041e-06\t lr: 3.005\n",
            "==Train Full Grad Epoch:\t 342 \tLoss: 0.158119\t lambda: 8.088e-06\t lr: 2.997\n",
            "--\t\t\tphi(X):\t 0.203 \tphi(Y): 0.228\n",
            "==Train Epoch:\t\t\t 345 \tLoss: 0.155488\t lambda: 8.158e-06\t lr: 4.952\n",
            "==Train Epoch:\t\t\t 348 \tLoss: 0.152443\t lambda: 8.229e-06\t lr: 4.704\n",
            "--\t\t\tphi(X):\t 0.205 \tphi(Y): 0.228\n",
            "\n",
            "Test set: Average loss: 0.1475\n",
            "\n",
            "==Train Full Grad Epoch:\t 351 \tLoss: 0.157143\t lambda: 8.300e-06\t lr: 2.991\n",
            "==Train Epoch:\t\t\t 354 \tLoss: 0.151395\t lambda: 8.370e-06\t lr: 5.010\n",
            "--\t\t\tphi(X):\t 0.202 \tphi(Y): 0.227\n",
            "==Train Full Grad Epoch:\t 357 \tLoss: 0.156428\t lambda: 8.417e-06\t lr: 2.987\n",
            "==Train Epoch:\t\t\t 360 \tLoss: 0.150782\t lambda: 8.488e-06\t lr: 4.987\n",
            "--\t\t\tphi(X):\t 0.204 \tphi(Y): 0.226\n",
            "==Train Full Grad Epoch:\t 363 \tLoss: 0.155338\t lambda: 8.535e-06\t lr: 2.984\n",
            "==Train Epoch:\t\t\t 366 \tLoss: 0.148759\t lambda: 8.606e-06\t lr: 4.622\n",
            "--\t\t\tphi(X):\t 0.204 \tphi(Y): 0.226\n",
            "==Train Full Grad Epoch:\t 369 \tLoss: 0.154034\t lambda: 8.677e-06\t lr: 2.983\n",
            "==Train Full Grad Epoch:\t 372 \tLoss: 0.151468\t lambda: 8.747e-06\t lr: 2.990\n",
            "--\t\t\tphi(X):\t 0.202 \tphi(Y): 0.225\n",
            "==Train Epoch:\t\t\t 375 \tLoss: 0.148321\t lambda: 8.842e-06\t lr: 4.945\n",
            "==Train Full Grad Epoch:\t 378 \tLoss: 0.149484\t lambda: 8.913e-06\t lr: 2.977\n",
            "--\t\t\tphi(X):\t 0.200 \tphi(Y): 0.223\n",
            "==Train Full Grad Epoch:\t 381 \tLoss: 0.145442\t lambda: 8.936e-06\t lr: 2.973\n",
            "==Train Epoch:\t\t\t 384 \tLoss: 0.144478\t lambda: 9.031e-06\t lr: 4.835\n",
            "--\t\t\tphi(X):\t 0.199 \tphi(Y): 0.222\n",
            "==Train Epoch:\t\t\t 387 \tLoss: 0.141701\t lambda: 9.102e-06\t lr: 4.816\n",
            "==Train Epoch:\t\t\t 390 \tLoss: 0.136753\t lambda: 9.197e-06\t lr: 4.902\n",
            "--\t\t\tphi(X):\t 0.194 \tphi(Y): 0.219\n",
            "==Train Full Grad Epoch:\t 393 \tLoss: 0.134865\t lambda: 9.269e-06\t lr: 2.946\n",
            "==Train Epoch:\t\t\t 396 \tLoss: 0.127732\t lambda: 9.364e-06\t lr: 5.012\n",
            "--\t\t\tphi(X):\t 0.190 \tphi(Y): 0.212\n",
            "==Train Epoch:\t\t\t 399 \tLoss: 0.117148\t lambda: 9.460e-06\t lr: 4.912\n",
            "\n",
            "Test set: Average loss: 0.1084\n",
            "\n",
            "==Train Epoch:\t\t\t 402 \tLoss: 0.105635\t lambda: 9.557e-06\t lr: 4.751\n",
            "--\t\t\tphi(X):\t 0.180 \tphi(Y): 0.201\n",
            "==Train Epoch:\t\t\t 405 \tLoss: 0.090576\t lambda: 9.655e-06\t lr: 4.830\n",
            "==Train Epoch:\t\t\t 408 \tLoss: 0.077893\t lambda: 9.729e-06\t lr: 4.250\n",
            "--\t\t\tphi(X):\t 0.161 \tphi(Y): 0.184\n",
            "==Train Epoch:\t\t\t 411 \tLoss: 0.065113\t lambda: 9.804e-06\t lr: 4.424\n",
            "==Train Full Grad Epoch:\t 414 \tLoss: 0.051932\t lambda: 9.880e-06\t lr: 2.775\n",
            "--\t\t\tphi(X):\t 0.139 \tphi(Y): 0.167\n",
            "==Train Epoch:\t\t\t 417 \tLoss: 0.045118\t lambda: 9.931e-06\t lr: 3.994\n",
            "==Train Epoch:\t\t\t 420 \tLoss: 0.032877\t lambda: 1.003e-05\t lr: 4.460\n",
            "--\t\t\tphi(X):\t 0.120 \tphi(Y): 0.152\n",
            "==Train Epoch:\t\t\t 423 \tLoss: 0.023995\t lambda: 1.014e-05\t lr: 4.251\n",
            "==Train Full Grad Epoch:\t 426 \tLoss: 0.020185\t lambda: 1.019e-05\t lr: 2.666\n",
            "--\t\t\tphi(X):\t 0.105 \tphi(Y): 0.140\n",
            "==Train Full Grad Epoch:\t 429 \tLoss: 0.017182\t lambda: 1.027e-05\t lr: 2.651\n",
            "==Train Epoch:\t\t\t 432 \tLoss: 0.015408\t lambda: 1.035e-05\t lr: 4.155\n",
            "--\t\t\tphi(X):\t 0.099 \tphi(Y): 0.134\n",
            "==Train Epoch:\t\t\t 435 \tLoss: 0.014104\t lambda: 1.043e-05\t lr: 4.111\n",
            "==Train Epoch:\t\t\t 438 \tLoss: 0.013072\t lambda: 1.051e-05\t lr: 4.022\n",
            "--\t\t\tphi(X):\t 0.095 \tphi(Y): 0.130\n",
            "==Train Epoch:\t\t\t 441 \tLoss: 0.012471\t lambda: 1.059e-05\t lr: 4.135\n",
            "==Train Epoch:\t\t\t 444 \tLoss: 0.011781\t lambda: 1.070e-05\t lr: 4.032\n",
            "--\t\t\tphi(X):\t 0.093 \tphi(Y): 0.126\n",
            "==Train Epoch:\t\t\t 447 \tLoss: 0.011342\t lambda: 1.078e-05\t lr: 4.424\n",
            "\n",
            "Test set: Average loss: 0.0013\n",
            "\n",
            "==Train Full Grad Epoch:\t 450 \tLoss: 0.011081\t lambda: 1.086e-05\t lr: 2.597\n",
            "--\t\t\tphi(X):\t 0.091 \tphi(Y): 0.124\n",
            "==Train Full Grad Epoch:\t 453 \tLoss: 0.010895\t lambda: 1.094e-05\t lr: 2.593\n",
            "==Train Full Grad Epoch:\t 456 \tLoss: 0.010717\t lambda: 1.102e-05\t lr: 2.589\n",
            "--\t\t\tphi(X):\t 0.090 \tphi(Y): 0.122\n",
            "==Train Epoch:\t\t\t 459 \tLoss: 0.010450\t lambda: 1.110e-05\t lr: 4.326\n",
            "==Train Epoch:\t\t\t 462 \tLoss: 0.010330\t lambda: 1.118e-05\t lr: 3.932\n",
            "--\t\t\tphi(X):\t 0.089 \tphi(Y): 0.121\n",
            "==Train Epoch:\t\t\t 465 \tLoss: 0.010299\t lambda: 1.126e-05\t lr: 4.132\n",
            "==Train Epoch:\t\t\t 468 \tLoss: 0.010214\t lambda: 1.134e-05\t lr: 4.013\n",
            "--\t\t\tphi(X):\t 0.089 \tphi(Y): 0.120\n",
            "==Train Epoch:\t\t\t 471 \tLoss: 0.010187\t lambda: 1.145e-05\t lr: 4.216\n",
            "==Train Epoch:\t\t\t 474 \tLoss: 0.010073\t lambda: 1.153e-05\t lr: 4.019\n",
            "--\t\t\tphi(X):\t 0.088 \tphi(Y): 0.119\n",
            "==Train Epoch:\t\t\t 477 \tLoss: 0.010045\t lambda: 1.161e-05\t lr: 4.171\n",
            "==Train Epoch:\t\t\t 480 \tLoss: 0.010071\t lambda: 1.169e-05\t lr: 3.965\n",
            "--\t\t\tphi(X):\t 0.088 \tphi(Y): 0.119\n",
            "==Train Full Grad Epoch:\t 483 \tLoss: 0.010099\t lambda: 1.177e-05\t lr: 2.573\n",
            "==Train Full Grad Epoch:\t 486 \tLoss: 0.010014\t lambda: 1.182e-05\t lr: 2.571\n",
            "--\t\t\tphi(X):\t 0.087 \tphi(Y): 0.118\n",
            "==Train Epoch:\t\t\t 489 \tLoss: 0.010006\t lambda: 1.190e-05\t lr: 4.010\n",
            "==Train Epoch:\t\t\t 492 \tLoss: 0.010019\t lambda: 1.201e-05\t lr: 4.091\n",
            "--\t\t\tphi(X):\t 0.088 \tphi(Y): 0.118\n",
            "==Train Epoch:\t\t\t 495 \tLoss: 0.009930\t lambda: 1.209e-05\t lr: 4.083\n",
            "==Train Epoch:\t\t\t 498 \tLoss: 0.009961\t lambda: 1.220e-05\t lr: 4.057\n",
            "--\t\t\tphi(X):\t 0.087 \tphi(Y): 0.117\n",
            "\n",
            "Test set: Average loss: 0.0002\n",
            "\n",
            "==Train Epoch:\t\t\t 501 \tLoss: 0.009906\t lambda: 1.231e-05\t lr: 4.157\n",
            "==Train Epoch:\t\t\t 504 \tLoss: 0.009944\t lambda: 1.242e-05\t lr: 4.136\n",
            "--\t\t\tphi(X):\t 0.087 \tphi(Y): 0.117\n",
            "==Train Epoch:\t\t\t 507 \tLoss: 0.009946\t lambda: 1.250e-05\t lr: 4.154\n",
            "==Train Epoch:\t\t\t 510 \tLoss: 0.009912\t lambda: 1.260e-05\t lr: 4.054\n",
            "--\t\t\tphi(X):\t 0.087 \tphi(Y): 0.116\n",
            "==Train Epoch:\t\t\t 513 \tLoss: 0.009920\t lambda: 1.269e-05\t lr: 3.852\n",
            "==Train Epoch:\t\t\t 516 \tLoss: 0.009926\t lambda: 1.277e-05\t lr: 3.837\n",
            "--\t\t\tphi(X):\t 0.087 \tphi(Y): 0.116\n",
            "==Train Full Grad Epoch:\t 519 \tLoss: 0.010021\t lambda: 1.285e-05\t lr: 2.562\n",
            "==Train Epoch:\t\t\t 522 \tLoss: 0.009943\t lambda: 1.296e-05\t lr: 4.094\n",
            "--\t\t\tphi(X):\t 0.087 \tphi(Y): 0.116\n",
            "==Train Epoch:\t\t\t 525 \tLoss: 0.009915\t lambda: 1.304e-05\t lr: 4.001\n",
            "==Train Epoch:\t\t\t 528 \tLoss: 0.009942\t lambda: 1.314e-05\t lr: 3.915\n",
            "--\t\t\tphi(X):\t 0.086 \tphi(Y): 0.116\n",
            "==Train Epoch:\t\t\t 531 \tLoss: 0.009898\t lambda: 1.322e-05\t lr: 4.003\n",
            "==Train Full Grad Epoch:\t 534 \tLoss: 0.009990\t lambda: 1.331e-05\t lr: 2.560\n",
            "--\t\t\tphi(X):\t 0.086 \tphi(Y): 0.115\n",
            "==Train Full Grad Epoch:\t 537 \tLoss: 0.010011\t lambda: 1.336e-05\t lr: 2.559\n",
            "==Train Epoch:\t\t\t 540 \tLoss: 0.009957\t lambda: 1.347e-05\t lr: 3.828\n",
            "--\t\t\tphi(X):\t 0.086 \tphi(Y): 0.115\n",
            "==Train Full Grad Epoch:\t 543 \tLoss: 0.009990\t lambda: 1.355e-05\t lr: 2.558\n",
            "==Train Epoch:\t\t\t 546 \tLoss: 0.009909\t lambda: 1.363e-05\t lr: 3.971\n",
            "--\t\t\tphi(X):\t 0.086 \tphi(Y): 0.115\n",
            "==Train Full Grad Epoch:\t 549 \tLoss: 0.009999\t lambda: 1.368e-05\t lr: 2.558\n",
            "\n",
            "Test set: Average loss: 0.0002\n",
            "\n",
            "==Train Epoch:\t\t\t 552 \tLoss: 0.009986\t lambda: 1.376e-05\t lr: 4.284\n",
            "--\t\t\tphi(X):\t 0.086 \tphi(Y): 0.115\n",
            "==Train Epoch:\t\t\t 555 \tLoss: 0.009911\t lambda: 1.387e-05\t lr: 3.826\n",
            "==Train Epoch:\t\t\t 558 \tLoss: 0.009918\t lambda: 1.398e-05\t lr: 4.243\n",
            "--\t\t\tphi(X):\t 0.086 \tphi(Y): 0.115\n",
            "==Train Epoch:\t\t\t 561 \tLoss: 0.009919\t lambda: 1.409e-05\t lr: 4.201\n",
            "==Train Full Grad Epoch:\t 564 \tLoss: 0.009987\t lambda: 1.414e-05\t lr: 2.555\n",
            "--\t\t\tphi(X):\t 0.085 \tphi(Y): 0.114\n",
            "==Train Full Grad Epoch:\t 567 \tLoss: 0.009894\t lambda: 1.417e-05\t lr: 2.555\n",
            "==Train Epoch:\t\t\t 570 \tLoss: 0.009904\t lambda: 1.425e-05\t lr: 4.105\n",
            "--\t\t\tphi(X):\t 0.086 \tphi(Y): 0.114\n",
            "==Train Epoch:\t\t\t 573 \tLoss: 0.009870\t lambda: 1.433e-05\t lr: 4.042\n",
            "==Train Epoch:\t\t\t 576 \tLoss: 0.009914\t lambda: 1.441e-05\t lr: 3.977\n",
            "--\t\t\tphi(X):\t 0.085 \tphi(Y): 0.114\n",
            "==Train Epoch:\t\t\t 579 \tLoss: 0.009910\t lambda: 1.452e-05\t lr: 3.772\n",
            "==Train Full Grad Epoch:\t 582 \tLoss: 0.009919\t lambda: 1.457e-05\t lr: 2.553\n",
            "--\t\t\tphi(X):\t 0.085 \tphi(Y): 0.114\n",
            "==Train Epoch:\t\t\t 585 \tLoss: 0.009911\t lambda: 1.468e-05\t lr: 4.128\n",
            "==Train Full Grad Epoch:\t 588 \tLoss: 0.009967\t lambda: 1.476e-05\t lr: 2.552\n",
            "--\t\t\tphi(X):\t 0.085 \tphi(Y): 0.114\n",
            "==Train Epoch:\t\t\t 591 \tLoss: 0.009847\t lambda: 1.487e-05\t lr: 4.005\n",
            "==Train Full Grad Epoch:\t 594 \tLoss: 0.009982\t lambda: 1.495e-05\t lr: 2.552\n",
            "--\t\t\tphi(X):\t 0.085 \tphi(Y): 0.113\n",
            "==Train Epoch:\t\t\t 597 \tLoss: 0.009934\t lambda: 1.506e-05\t lr: 4.048\n",
            "\n",
            "Test set: Average loss: 0.0001\n",
            "\n",
            "==Train Epoch:\t\t\t 600 \tLoss: 0.009862\t lambda: 1.512e-05\t lr: 4.101\n",
            "--\t\t\tphi(X):\t 0.085 \tphi(Y): 0.114\n",
            "==Train Epoch:\t\t\t 603 \tLoss: 0.009863\t lambda: 1.517e-05\t lr: 4.087\n",
            "==Train Epoch:\t\t\t 606 \tLoss: 0.009819\t lambda: 1.525e-05\t lr: 4.196\n",
            "--\t\t\tphi(X):\t 0.084 \tphi(Y): 0.114\n",
            "==Train Epoch:\t\t\t 609 \tLoss: 0.009928\t lambda: 1.536e-05\t lr: 4.066\n",
            "==Train Epoch:\t\t\t 612 \tLoss: 0.009868\t lambda: 1.547e-05\t lr: 3.929\n",
            "--\t\t\tphi(X):\t 0.084 \tphi(Y): 0.113\n",
            "==Train Epoch:\t\t\t 615 \tLoss: 0.009847\t lambda: 1.552e-05\t lr: 3.764\n",
            "==Train Full Grad Epoch:\t 618 \tLoss: 0.010008\t lambda: 1.560e-05\t lr: 2.549\n",
            "--\t\t\tphi(X):\t 0.084 \tphi(Y): 0.113\n",
            "==Train Full Grad Epoch:\t 621 \tLoss: 0.009987\t lambda: 1.566e-05\t lr: 2.548\n",
            "==Train Epoch:\t\t\t 624 \tLoss: 0.009835\t lambda: 1.574e-05\t lr: 3.892\n",
            "--\t\t\tphi(X):\t 0.084 \tphi(Y): 0.113\n",
            "==Train Epoch:\t\t\t 627 \tLoss: 0.009869\t lambda: 1.585e-05\t lr: 3.984\n",
            "==Train Full Grad Epoch:\t 630 \tLoss: 0.009999\t lambda: 1.593e-05\t lr: 2.547\n",
            "--\t\t\tphi(X):\t 0.083 \tphi(Y): 0.113\n",
            "==Train Epoch:\t\t\t 633 \tLoss: 0.009897\t lambda: 1.603e-05\t lr: 4.128\n",
            "==Train Epoch:\t\t\t 636 \tLoss: 0.009916\t lambda: 1.614e-05\t lr: 3.970\n",
            "--\t\t\tphi(X):\t 0.083 \tphi(Y): 0.113\n",
            "==Train Full Grad Epoch:\t 639 \tLoss: 0.009920\t lambda: 1.620e-05\t lr: 2.546\n",
            "==Train Epoch:\t\t\t 642 \tLoss: 0.009941\t lambda: 1.631e-05\t lr: 4.163\n",
            "--\t\t\tphi(X):\t 0.083 \tphi(Y): 0.113\n",
            "==Train Epoch:\t\t\t 645 \tLoss: 0.009866\t lambda: 1.639e-05\t lr: 4.113\n",
            "==Train Epoch:\t\t\t 648 \tLoss: 0.009929\t lambda: 1.649e-05\t lr: 4.041\n",
            "--\t\t\tphi(X):\t 0.083 \tphi(Y): 0.112\n",
            "\n",
            "Test set: Average loss: 0.0002\n",
            "\n",
            "==Train Epoch:\t\t\t 651 \tLoss: 0.009947\t lambda: 1.658e-05\t lr: 3.986\n",
            "==Train Full Grad Epoch:\t 654 \tLoss: 0.009927\t lambda: 1.663e-05\t lr: 2.544\n",
            "--\t\t\tphi(X):\t 0.083 \tphi(Y): 0.112\n",
            "==Train Epoch:\t\t\t 657 \tLoss: 0.009931\t lambda: 1.674e-05\t lr: 4.082\n",
            "==Train Full Grad Epoch:\t 660 \tLoss: 0.010012\t lambda: 1.682e-05\t lr: 2.542\n",
            "--\t\t\tphi(X):\t 0.083 \tphi(Y): 0.112\n",
            "==Train Epoch:\t\t\t 663 \tLoss: 0.009838\t lambda: 1.687e-05\t lr: 4.031\n",
            "==Train Full Grad Epoch:\t 666 \tLoss: 0.009895\t lambda: 1.690e-05\t lr: 2.542\n",
            "--\t\t\tphi(X):\t 0.082 \tphi(Y): 0.111\n",
            "==Train Epoch:\t\t\t 669 \tLoss: 0.009807\t lambda: 1.698e-05\t lr: 3.997\n",
            "==Train Epoch:\t\t\t 672 \tLoss: 0.009878\t lambda: 1.709e-05\t lr: 4.130\n",
            "--\t\t\tphi(X):\t 0.082 \tphi(Y): 0.112\n",
            "==Train Epoch:\t\t\t 675 \tLoss: 0.009890\t lambda: 1.720e-05\t lr: 3.894\n",
            "==Train Epoch:\t\t\t 678 \tLoss: 0.009880\t lambda: 1.731e-05\t lr: 3.958\n",
            "--\t\t\tphi(X):\t 0.082 \tphi(Y): 0.111\n",
            "==Train Epoch:\t\t\t 681 \tLoss: 0.009873\t lambda: 1.739e-05\t lr: 3.848\n",
            "==Train Epoch:\t\t\t 684 \tLoss: 0.009884\t lambda: 1.747e-05\t lr: 4.116\n",
            "--\t\t\tphi(X):\t 0.082 \tphi(Y): 0.111\n",
            "==Train Full Grad Epoch:\t 687 \tLoss: 0.010013\t lambda: 1.755e-05\t lr: 2.538\n",
            "==Train Epoch:\t\t\t 690 \tLoss: 0.009837\t lambda: 1.766e-05\t lr: 3.775\n",
            "--\t\t\tphi(X):\t 0.081 \tphi(Y): 0.111\n",
            "==Train Epoch:\t\t\t 693 \tLoss: 0.009896\t lambda: 1.777e-05\t lr: 4.000\n",
            "==Train Full Grad Epoch:\t 696 \tLoss: 0.009974\t lambda: 1.785e-05\t lr: 2.537\n",
            "--\t\t\tphi(X):\t 0.081 \tphi(Y): 0.110\n",
            "==Train Epoch:\t\t\t 699 \tLoss: 0.009877\t lambda: 1.793e-05\t lr: 3.809\n",
            "\n",
            "Test set: Average loss: 0.0003\n",
            "\n",
            "==Train Epoch:\t\t\t 702 \tLoss: 0.009894\t lambda: 1.801e-05\t lr: 3.848\n",
            "--\t\t\tphi(X):\t 0.081 \tphi(Y): 0.110\n",
            "==Train Epoch:\t\t\t 705 \tLoss: 0.009800\t lambda: 1.807e-05\t lr: 4.369\n",
            "==Train Full Grad Epoch:\t 708 \tLoss: 0.009984\t lambda: 1.812e-05\t lr: 2.536\n",
            "--\t\t\tphi(X):\t 0.081 \tphi(Y): 0.110\n",
            "==Train Full Grad Epoch:\t 711 \tLoss: 0.009892\t lambda: 1.815e-05\t lr: 2.535\n",
            "==Train Epoch:\t\t\t 714 \tLoss: 0.009924\t lambda: 1.826e-05\t lr: 4.104\n",
            "--\t\t\tphi(X):\t 0.081 \tphi(Y): 0.110\n",
            "==Train Epoch:\t\t\t 717 \tLoss: 0.009869\t lambda: 1.837e-05\t lr: 3.963\n",
            "==Train Epoch:\t\t\t 720 \tLoss: 0.009951\t lambda: 1.845e-05\t lr: 3.752\n",
            "--\t\t\tphi(X):\t 0.080 \tphi(Y): 0.110\n",
            "==Train Epoch:\t\t\t 723 \tLoss: 0.009889\t lambda: 1.856e-05\t lr: 3.985\n",
            "==Train Full Grad Epoch:\t 726 \tLoss: 0.009992\t lambda: 1.864e-05\t lr: 2.532\n",
            "--\t\t\tphi(X):\t 0.080 \tphi(Y): 0.109\n",
            "==Train Epoch:\t\t\t 729 \tLoss: 0.009904\t lambda: 1.872e-05\t lr: 4.007\n",
            "==Train Full Grad Epoch:\t 732 \tLoss: 0.009992\t lambda: 1.877e-05\t lr: 2.531\n",
            "--\t\t\tphi(X):\t 0.080 \tphi(Y): 0.109\n",
            "==Train Epoch:\t\t\t 735 \tLoss: 0.009833\t lambda: 1.885e-05\t lr: 4.138\n",
            "==Train Full Grad Epoch:\t 738 \tLoss: 0.009978\t lambda: 1.891e-05\t lr: 2.530\n",
            "--\t\t\tphi(X):\t 0.079 \tphi(Y): 0.109\n",
            "==Train Epoch:\t\t\t 741 \tLoss: 0.009892\t lambda: 1.899e-05\t lr: 4.060\n",
            "==Train Epoch:\t\t\t 744 \tLoss: 0.009823\t lambda: 1.907e-05\t lr: 3.865\n",
            "--\t\t\tphi(X):\t 0.080 \tphi(Y): 0.109\n",
            "==Train Epoch:\t\t\t 747 \tLoss: 0.009833\t lambda: 1.915e-05\t lr: 4.098\n",
            "\n",
            "Test set: Average loss: 0.0003\n",
            "\n",
            "==Train Full Grad Epoch:\t 750 \tLoss: 0.009984\t lambda: 1.923e-05\t lr: 2.529\n",
            "--\t\t\tphi(X):\t 0.079 \tphi(Y): 0.108\n",
            "==Train Epoch:\t\t\t 753 \tLoss: 0.009915\t lambda: 1.934e-05\t lr: 3.714\n",
            "==Train Epoch:\t\t\t 756 \tLoss: 0.009941\t lambda: 1.945e-05\t lr: 4.011\n",
            "--\t\t\tphi(X):\t 0.079 \tphi(Y): 0.108\n",
            "==Train Epoch:\t\t\t 759 \tLoss: 0.009842\t lambda: 1.951e-05\t lr: 4.034\n",
            "==Train Epoch:\t\t\t 762 \tLoss: 0.009895\t lambda: 1.962e-05\t lr: 4.043\n",
            "--\t\t\tphi(X):\t 0.079 \tphi(Y): 0.108\n",
            "==Train Epoch:\t\t\t 765 \tLoss: 0.009890\t lambda: 1.972e-05\t lr: 4.059\n",
            "==Train Epoch:\t\t\t 768 \tLoss: 0.009886\t lambda: 1.983e-05\t lr: 3.916\n",
            "--\t\t\tphi(X):\t 0.078 \tphi(Y): 0.108\n",
            "==Train Epoch:\t\t\t 771 \tLoss: 0.009922\t lambda: 1.994e-05\t lr: 3.684\n",
            "==Train Full Grad Epoch:\t 774 \tLoss: 0.009921\t lambda: 2.000e-05\t lr: 2.524\n",
            "--\t\t\tphi(X):\t 0.078 \tphi(Y): 0.107\n",
            "==Train Epoch:\t\t\t 777 \tLoss: 0.009928\t lambda: 2.011e-05\t lr: 4.122\n",
            "==Train Epoch:\t\t\t 780 \tLoss: 0.009904\t lambda: 2.019e-05\t lr: 3.806\n",
            "--\t\t\tphi(X):\t 0.078 \tphi(Y): 0.107\n",
            "==Train Epoch:\t\t\t 783 \tLoss: 0.009900\t lambda: 2.027e-05\t lr: 3.978\n",
            "==Train Full Grad Epoch:\t 786 \tLoss: 0.009900\t lambda: 2.030e-05\t lr: 2.522\n",
            "--\t\t\tphi(X):\t 0.077 \tphi(Y): 0.107\n",
            "==Train Epoch:\t\t\t 789 \tLoss: 0.009927\t lambda: 2.040e-05\t lr: 3.977\n",
            "==Train Epoch:\t\t\t 792 \tLoss: 0.009900\t lambda: 2.049e-05\t lr: 4.010\n",
            "--\t\t\tphi(X):\t 0.077 \tphi(Y): 0.107\n",
            "==Train Epoch:\t\t\t 795 \tLoss: 0.009881\t lambda: 2.060e-05\t lr: 4.055\n",
            "==Train Epoch:\t\t\t 798 \tLoss: 0.009888\t lambda: 2.068e-05\t lr: 3.899\n",
            "--\t\t\tphi(X):\t 0.077 \tphi(Y): 0.107\n",
            "\n",
            "Test set: Average loss: 0.0003\n",
            "\n",
            "==Train Epoch:\t\t\t 801 \tLoss: 0.009880\t lambda: 2.079e-05\t lr: 4.082\n",
            "==Train Epoch:\t\t\t 804 \tLoss: 0.009891\t lambda: 2.087e-05\t lr: 3.807\n",
            "--\t\t\tphi(X):\t 0.077 \tphi(Y): 0.106\n",
            "==Train Epoch:\t\t\t 807 \tLoss: 0.009899\t lambda: 2.095e-05\t lr: 4.017\n",
            "==Train Full Grad Epoch:\t 810 \tLoss: 0.009986\t lambda: 2.103e-05\t lr: 2.516\n",
            "--\t\t\tphi(X):\t 0.076 \tphi(Y): 0.106\n",
            "==Train Epoch:\t\t\t 813 \tLoss: 0.009928\t lambda: 2.114e-05\t lr: 3.988\n",
            "==Train Epoch:\t\t\t 816 \tLoss: 0.009840\t lambda: 2.122e-05\t lr: 4.047\n",
            "--\t\t\tphi(X):\t 0.076 \tphi(Y): 0.106\n",
            "==Train Epoch:\t\t\t 819 \tLoss: 0.009902\t lambda: 2.130e-05\t lr: 3.803\n",
            "==Train Epoch:\t\t\t 822 \tLoss: 0.009873\t lambda: 2.141e-05\t lr: 3.658\n",
            "--\t\t\tphi(X):\t 0.076 \tphi(Y): 0.105\n",
            "==Train Epoch:\t\t\t 825 \tLoss: 0.009933\t lambda: 2.150e-05\t lr: 3.851\n",
            "==Train Epoch:\t\t\t 828 \tLoss: 0.009935\t lambda: 2.160e-05\t lr: 3.841\n",
            "--\t\t\tphi(X):\t 0.076 \tphi(Y): 0.105\n",
            "==Train Epoch:\t\t\t 831 \tLoss: 0.009914\t lambda: 2.171e-05\t lr: 4.155\n",
            "==Train Full Grad Epoch:\t 834 \tLoss: 0.009923\t lambda: 2.177e-05\t lr: 2.512\n",
            "--\t\t\tphi(X):\t 0.075 \tphi(Y): 0.104\n",
            "==Train Epoch:\t\t\t 837 \tLoss: 0.009906\t lambda: 2.185e-05\t lr: 4.135\n",
            "==Train Epoch:\t\t\t 840 \tLoss: 0.009854\t lambda: 2.193e-05\t lr: 4.044\n",
            "--\t\t\tphi(X):\t 0.075 \tphi(Y): 0.104\n",
            "==Train Epoch:\t\t\t 843 \tLoss: 0.009911\t lambda: 2.204e-05\t lr: 3.650\n",
            "==Train Epoch:\t\t\t 846 \tLoss: 0.009854\t lambda: 2.212e-05\t lr: 4.028\n",
            "--\t\t\tphi(X):\t 0.075 \tphi(Y): 0.104\n",
            "==Train Full Grad Epoch:\t 849 \tLoss: 0.010000\t lambda: 2.218e-05\t lr: 2.509\n",
            "\n",
            "Test set: Average loss: 0.0002\n",
            "\n",
            "==Train Full Grad Epoch:\t 852 \tLoss: 0.009978\t lambda: 2.226e-05\t lr: 2.508\n",
            "--\t\t\tphi(X):\t 0.074 \tphi(Y): 0.104\n",
            "==Train Epoch:\t\t\t 855 \tLoss: 0.009933\t lambda: 2.237e-05\t lr: 4.023\n",
            "==Train Full Grad Epoch:\t 858 \tLoss: 0.009988\t lambda: 2.242e-05\t lr: 2.506\n",
            "--\t\t\tphi(X):\t 0.074 \tphi(Y): 0.103\n",
            "==Train Epoch:\t\t\t 861 \tLoss: 0.009895\t lambda: 2.253e-05\t lr: 4.011\n",
            "==Train Epoch:\t\t\t 864 \tLoss: 0.009900\t lambda: 2.264e-05\t lr: 3.905\n",
            "--\t\t\tphi(X):\t 0.074 \tphi(Y): 0.103\n",
            "==Train Epoch:\t\t\t 867 \tLoss: 0.009845\t lambda: 2.272e-05\t lr: 3.951\n",
            "==Train Full Grad Epoch:\t 870 \tLoss: 0.010012\t lambda: 2.281e-05\t lr: 2.503\n",
            "--\t\t\tphi(X):\t 0.073 \tphi(Y): 0.103\n",
            "==Train Full Grad Epoch:\t 873 \tLoss: 0.009982\t lambda: 2.289e-05\t lr: 2.503\n",
            "==Train Full Grad Epoch:\t 876 \tLoss: 0.009899\t lambda: 2.292e-05\t lr: 2.503\n",
            "--\t\t\tphi(X):\t 0.073 \tphi(Y): 0.102\n",
            "==Train Epoch:\t\t\t 879 \tLoss: 0.009939\t lambda: 2.303e-05\t lr: 4.250\n",
            "==Train Epoch:\t\t\t 882 \tLoss: 0.009899\t lambda: 2.314e-05\t lr: 3.786\n",
            "--\t\t\tphi(X):\t 0.073 \tphi(Y): 0.102\n",
            "==Train Epoch:\t\t\t 885 \tLoss: 0.009932\t lambda: 2.324e-05\t lr: 3.836\n",
            "==Train Full Grad Epoch:\t 888 \tLoss: 0.009905\t lambda: 2.327e-05\t lr: 2.500\n",
            "--\t\t\tphi(X):\t 0.072 \tphi(Y): 0.101\n",
            "==Train Full Grad Epoch:\t 891 \tLoss: 0.009896\t lambda: 2.330e-05\t lr: 2.500\n",
            "==Train Epoch:\t\t\t 894 \tLoss: 0.009904\t lambda: 2.341e-05\t lr: 3.906\n",
            "--\t\t\tphi(X):\t 0.073 \tphi(Y): 0.102\n",
            "==Train Full Grad Epoch:\t 897 \tLoss: 0.010000\t lambda: 2.349e-05\t lr: 2.499\n",
            "\n",
            "Test set: Average loss: 0.0003\n",
            "\n",
            "==Train Epoch:\t\t\t 900 \tLoss: 0.009903\t lambda: 2.360e-05\t lr: 3.989\n",
            "--\t\t\tphi(X):\t 0.072 \tphi(Y): 0.101\n",
            "==Train Epoch:\t\t\t 903 \tLoss: 0.009861\t lambda: 2.371e-05\t lr: 3.998\n",
            "==Train Epoch:\t\t\t 906 \tLoss: 0.009898\t lambda: 2.382e-05\t lr: 3.667\n",
            "--\t\t\tphi(X):\t 0.072 \tphi(Y): 0.101\n",
            "==Train Epoch:\t\t\t 909 \tLoss: 0.009865\t lambda: 2.390e-05\t lr: 3.586\n",
            "==Train Full Grad Epoch:\t 912 \tLoss: 0.009975\t lambda: 2.396e-05\t lr: 2.495\n",
            "--\t\t\tphi(X):\t 0.071 \tphi(Y): 0.100\n",
            "==Train Full Grad Epoch:\t 915 \tLoss: 0.009983\t lambda: 2.404e-05\t lr: 2.495\n",
            "==Train Epoch:\t\t\t 918 \tLoss: 0.009922\t lambda: 2.415e-05\t lr: 3.980\n",
            "--\t\t\tphi(X):\t 0.071 \tphi(Y): 0.100\n",
            "==Train Epoch:\t\t\t 921 \tLoss: 0.009835\t lambda: 2.426e-05\t lr: 3.873\n",
            "==Train Epoch:\t\t\t 924 \tLoss: 0.009909\t lambda: 2.434e-05\t lr: 3.891\n",
            "--\t\t\tphi(X):\t 0.071 \tphi(Y): 0.100\n",
            "==Train Epoch:\t\t\t 927 \tLoss: 0.009872\t lambda: 2.456e-05\t lr: 3.921\n",
            "==Train Epoch:\t\t\t 930 \tLoss: 0.009833\t lambda: 2.473e-05\t lr: 3.768\n",
            "--\t\t\tphi(X):\t 0.070 \tphi(Y): 0.100\n",
            "==Train Epoch:\t\t\t 933 \tLoss: 0.009860\t lambda: 2.489e-05\t lr: 4.076\n",
            "==Train Epoch:\t\t\t 936 \tLoss: 0.009866\t lambda: 2.506e-05\t lr: 3.757\n",
            "--\t\t\tphi(X):\t 0.070 \tphi(Y): 0.099\n",
            "==Train Epoch:\t\t\t 939 \tLoss: 0.009864\t lambda: 2.522e-05\t lr: 3.829\n",
            "==Train Epoch:\t\t\t 942 \tLoss: 0.009973\t lambda: 2.544e-05\t lr: 4.122\n",
            "--\t\t\tphi(X):\t 0.069 \tphi(Y): 0.099\n",
            "==Train Full Grad Epoch:\t 945 \tLoss: 0.009986\t lambda: 2.555e-05\t lr: 2.486\n",
            "==Train Full Grad Epoch:\t 948 \tLoss: 0.009985\t lambda: 2.566e-05\t lr: 2.486\n",
            "--\t\t\tphi(X):\t 0.069 \tphi(Y): 0.098\n",
            "\n",
            "Test set: Average loss: 0.0003\n",
            "\n",
            "==Train Full Grad Epoch:\t 951 \tLoss: 0.009993\t lambda: 2.582e-05\t lr: 2.484\n",
            "==Train Full Grad Epoch:\t 954 \tLoss: 0.010005\t lambda: 2.599e-05\t lr: 2.484\n",
            "--\t\t\tphi(X):\t 0.069 \tphi(Y): 0.098\n",
            "==Train Epoch:\t\t\t 957 \tLoss: 0.009856\t lambda: 2.610e-05\t lr: 4.028\n",
            "==Train Full Grad Epoch:\t 960 \tLoss: 0.009925\t lambda: 2.621e-05\t lr: 2.483\n",
            "--\t\t\tphi(X):\t 0.068 \tphi(Y): 0.097\n",
            "==Train Full Grad Epoch:\t 963 \tLoss: 0.009975\t lambda: 2.637e-05\t lr: 2.482\n",
            "==Train Epoch:\t\t\t 966 \tLoss: 0.009885\t lambda: 2.654e-05\t lr: 3.803\n",
            "--\t\t\tphi(X):\t 0.068 \tphi(Y): 0.097\n",
            "==Train Epoch:\t\t\t 969 \tLoss: 0.009896\t lambda: 2.676e-05\t lr: 3.792\n",
            "==Train Epoch:\t\t\t 972 \tLoss: 0.009851\t lambda: 2.693e-05\t lr: 3.631\n",
            "--\t\t\tphi(X):\t 0.068 \tphi(Y): 0.097\n",
            "==Train Epoch:\t\t\t 975 \tLoss: 0.009915\t lambda: 2.715e-05\t lr: 3.955\n",
            "==Train Epoch:\t\t\t 978 \tLoss: 0.009907\t lambda: 2.737e-05\t lr: 4.028\n",
            "--\t\t\tphi(X):\t 0.068 \tphi(Y): 0.096\n",
            "==Train Epoch:\t\t\t 981 \tLoss: 0.009911\t lambda: 2.753e-05\t lr: 3.863\n",
            "==Train Epoch:\t\t\t 984 \tLoss: 0.009884\t lambda: 2.775e-05\t lr: 3.570\n",
            "--\t\t\tphi(X):\t 0.067 \tphi(Y): 0.096\n",
            "==Train Epoch:\t\t\t 987 \tLoss: 0.009927\t lambda: 2.797e-05\t lr: 3.584\n",
            "==Train Epoch:\t\t\t 990 \tLoss: 0.009898\t lambda: 2.819e-05\t lr: 3.811\n",
            "--\t\t\tphi(X):\t 0.067 \tphi(Y): 0.095\n",
            "==Train Epoch:\t\t\t 993 \tLoss: 0.009904\t lambda: 2.836e-05\t lr: 3.730\n",
            "==Train Full Grad Epoch:\t 996 \tLoss: 0.009926\t lambda: 2.847e-05\t lr: 2.472\n",
            "--\t\t\tphi(X):\t 0.066 \tphi(Y): 0.094\n",
            "==Train Epoch:\t\t\t 999 \tLoss: 0.009896\t lambda: 2.869e-05\t lr: 3.825\n",
            "\n",
            "Test set: Average loss: 0.0003\n",
            "\n",
            "==Train Full Grad Epoch:\t 1002 \tLoss: 0.009909\t lambda: 2.874e-05\t lr: 2.470\n",
            "--\t\t\tphi(X):\t 0.066 \tphi(Y): 0.094\n",
            "==Train Epoch:\t\t\t 1005 \tLoss: 0.009894\t lambda: 2.891e-05\t lr: 3.987\n",
            "==Train Epoch:\t\t\t 1008 \tLoss: 0.009972\t lambda: 2.913e-05\t lr: 3.750\n",
            "--\t\t\tphi(X):\t 0.066 \tphi(Y): 0.094\n",
            "==Train Full Grad Epoch:\t 1011 \tLoss: 0.009924\t lambda: 2.924e-05\t lr: 2.467\n",
            "==Train Epoch:\t\t\t 1014 \tLoss: 0.009869\t lambda: 2.941e-05\t lr: 3.827\n",
            "--\t\t\tphi(X):\t 0.065 \tphi(Y): 0.094\n",
            "==Train Full Grad Epoch:\t 1017 \tLoss: 0.010000\t lambda: 2.957e-05\t lr: 2.466\n",
            "==Train Epoch:\t\t\t 1020 \tLoss: 0.009893\t lambda: 2.979e-05\t lr: 3.741\n",
            "--\t\t\tphi(X):\t 0.065 \tphi(Y): 0.093\n",
            "==Train Epoch:\t\t\t 1023 \tLoss: 0.009869\t lambda: 2.996e-05\t lr: 3.686\n",
            "==Train Epoch:\t\t\t 1026 \tLoss: 0.009900\t lambda: 3.013e-05\t lr: 3.786\n",
            "--\t\t\tphi(X):\t 0.064 \tphi(Y): 0.093\n",
            "==Train Epoch:\t\t\t 1029 \tLoss: 0.009920\t lambda: 3.029e-05\t lr: 3.800\n",
            "==Train Full Grad Epoch:\t 1032 \tLoss: 0.009984\t lambda: 3.040e-05\t lr: 2.462\n",
            "--\t\t\tphi(X):\t 0.064 \tphi(Y): 0.092\n",
            "==Train Epoch:\t\t\t 1035 \tLoss: 0.009949\t lambda: 3.062e-05\t lr: 3.797\n",
            "==Train Epoch:\t\t\t 1038 \tLoss: 0.010006\t lambda: 3.084e-05\t lr: 3.753\n",
            "--\t\t\tphi(X):\t 0.063 \tphi(Y): 0.092\n",
            "==Train Epoch:\t\t\t 1041 \tLoss: 0.009888\t lambda: 3.107e-05\t lr: 4.100\n",
            "==Train Full Grad Epoch:\t 1044 \tLoss: 0.009916\t lambda: 3.112e-05\t lr: 2.458\n",
            "--\t\t\tphi(X):\t 0.063 \tphi(Y): 0.091\n",
            "==Train Epoch:\t\t\t 1047 \tLoss: 0.009895\t lambda: 3.134e-05\t lr: 3.840\n",
            "\n",
            "Test set: Average loss: 0.0002\n",
            "\n",
            "==Train Epoch:\t\t\t 1050 \tLoss: 0.009905\t lambda: 3.151e-05\t lr: 3.815\n",
            "--\t\t\tphi(X):\t 0.063 \tphi(Y): 0.090\n",
            "==Train Full Grad Epoch:\t 1053 \tLoss: 0.010029\t lambda: 3.162e-05\t lr: 2.455\n",
            "==Train Full Grad Epoch:\t 1056 \tLoss: 0.010008\t lambda: 3.179e-05\t lr: 2.455\n",
            "--\t\t\tphi(X):\t 0.062 \tphi(Y): 0.090\n",
            "==Train Epoch:\t\t\t 1059 \tLoss: 0.009881\t lambda: 3.195e-05\t lr: 3.903\n",
            "==Train Full Grad Epoch:\t 1062 \tLoss: 0.009937\t lambda: 3.206e-05\t lr: 2.452\n",
            "--\t\t\tphi(X):\t 0.062 \tphi(Y): 0.089\n",
            "==Train Full Grad Epoch:\t 1065 \tLoss: 0.009924\t lambda: 3.217e-05\t lr: 2.452\n",
            "==Train Epoch:\t\t\t 1068 \tLoss: 0.009874\t lambda: 3.240e-05\t lr: 3.726\n",
            "--\t\t\tphi(X):\t 0.062 \tphi(Y): 0.089\n",
            "==Train Epoch:\t\t\t 1071 \tLoss: 0.009853\t lambda: 3.256e-05\t lr: 3.673\n",
            "==Train Full Grad Epoch:\t 1074 \tLoss: 0.009996\t lambda: 3.267e-05\t lr: 2.448\n",
            "--\t\t\tphi(X):\t 0.061 \tphi(Y): 0.088\n",
            "==Train Epoch:\t\t\t 1077 \tLoss: 0.009967\t lambda: 3.284e-05\t lr: 3.712\n",
            "==Train Epoch:\t\t\t 1080 \tLoss: 0.009879\t lambda: 3.306e-05\t lr: 3.687\n",
            "--\t\t\tphi(X):\t 0.061 \tphi(Y): 0.088\n",
            "==Train Epoch:\t\t\t 1083 \tLoss: 0.009848\t lambda: 3.329e-05\t lr: 3.755\n",
            "==Train Epoch:\t\t\t 1086 \tLoss: 0.009893\t lambda: 3.345e-05\t lr: 3.731\n",
            "--\t\t\tphi(X):\t 0.060 \tphi(Y): 0.087\n",
            "==Train Full Grad Epoch:\t 1089 \tLoss: 0.009984\t lambda: 3.362e-05\t lr: 2.444\n",
            "==Train Full Grad Epoch:\t 1092 \tLoss: 0.010010\t lambda: 3.379e-05\t lr: 2.442\n",
            "--\t\t\tphi(X):\t 0.059 \tphi(Y): 0.086\n",
            "==Train Epoch:\t\t\t 1095 \tLoss: 0.009832\t lambda: 3.390e-05\t lr: 3.447\n",
            "==Train Epoch:\t\t\t 1098 \tLoss: 0.009921\t lambda: 3.412e-05\t lr: 3.822\n",
            "--\t\t\tphi(X):\t 0.059 \tphi(Y): 0.086\n",
            "\n",
            "Test set: Average loss: 0.0003\n",
            "\n",
            "==Train Epoch:\t\t\t 1101 \tLoss: 0.009905\t lambda: 3.434e-05\t lr: 3.741\n",
            "==Train Epoch:\t\t\t 1104 \tLoss: 0.009892\t lambda: 3.451e-05\t lr: 3.627\n",
            "--\t\t\tphi(X):\t 0.059 \tphi(Y): 0.086\n",
            "==Train Epoch:\t\t\t 1107 \tLoss: 0.009939\t lambda: 3.473e-05\t lr: 3.588\n",
            "==Train Epoch:\t\t\t 1110 \tLoss: 0.009985\t lambda: 3.495e-05\t lr: 3.814\n",
            "--\t\t\tphi(X):\t 0.058 \tphi(Y): 0.085\n",
            "==Train Epoch:\t\t\t 1113 \tLoss: 0.009887\t lambda: 3.518e-05\t lr: 3.659\n",
            "==Train Full Grad Epoch:\t 1116 \tLoss: 0.009940\t lambda: 3.529e-05\t lr: 2.433\n",
            "--\t\t\tphi(X):\t 0.058 \tphi(Y): 0.084\n",
            "==Train Epoch:\t\t\t 1119 \tLoss: 0.009888\t lambda: 3.546e-05\t lr: 3.569\n",
            "==Train Epoch:\t\t\t 1122 \tLoss: 0.009925\t lambda: 3.568e-05\t lr: 3.987\n",
            "--\t\t\tphi(X):\t 0.057 \tphi(Y): 0.083\n",
            "==Train Full Grad Epoch:\t 1125 \tLoss: 0.009989\t lambda: 3.585e-05\t lr: 2.429\n",
            "==Train Epoch:\t\t\t 1128 \tLoss: 0.009911\t lambda: 3.601e-05\t lr: 3.879\n",
            "--\t\t\tphi(X):\t 0.057 \tphi(Y): 0.083\n",
            "==Train Epoch:\t\t\t 1131 \tLoss: 0.009907\t lambda: 3.624e-05\t lr: 3.767\n",
            "==Train Epoch:\t\t\t 1134 \tLoss: 0.009909\t lambda: 3.646e-05\t lr: 3.497\n",
            "--\t\t\tphi(X):\t 0.057 \tphi(Y): 0.082\n",
            "==Train Epoch:\t\t\t 1137 \tLoss: 0.009891\t lambda: 3.668e-05\t lr: 3.628\n",
            "==Train Full Grad Epoch:\t 1140 \tLoss: 0.010044\t lambda: 3.685e-05\t lr: 2.425\n",
            "--\t\t\tphi(X):\t 0.056 \tphi(Y): 0.081\n",
            "==Train Epoch:\t\t\t 1143 \tLoss: 0.009880\t lambda: 3.708e-05\t lr: 3.762\n",
            "==Train Epoch:\t\t\t 1146 \tLoss: 0.009859\t lambda: 3.719e-05\t lr: 3.677\n",
            "--\t\t\tphi(X):\t 0.056 \tphi(Y): 0.080\n",
            "==Train Epoch:\t\t\t 1149 \tLoss: 0.009924\t lambda: 3.736e-05\t lr: 3.998\n",
            "\n",
            "Test set: Average loss: 0.0003\n",
            "\n",
            "==Train Full Grad Epoch:\t 1152 \tLoss: 0.010003\t lambda: 3.752e-05\t lr: 2.420\n",
            "--\t\t\tphi(X):\t 0.055 \tphi(Y): 0.079\n",
            "==Train Epoch:\t\t\t 1155 \tLoss: 0.009913\t lambda: 3.775e-05\t lr: 3.882\n",
            "==Train Epoch:\t\t\t 1158 \tLoss: 0.009927\t lambda: 3.792e-05\t lr: 3.916\n",
            "--\t\t\tphi(X):\t 0.055 \tphi(Y): 0.079\n",
            "==Train Full Grad Epoch:\t 1161 \tLoss: 0.009923\t lambda: 3.797e-05\t lr: 2.417\n",
            "==Train Epoch:\t\t\t 1164 \tLoss: 0.009900\t lambda: 3.820e-05\t lr: 3.808\n",
            "--\t\t\tphi(X):\t 0.054 \tphi(Y): 0.078\n",
            "==Train Epoch:\t\t\t 1167 \tLoss: 0.009820\t lambda: 3.831e-05\t lr: 3.727\n",
            "==Train Epoch:\t\t\t 1170 \tLoss: 0.009898\t lambda: 3.848e-05\t lr: 3.589\n",
            "--\t\t\tphi(X):\t 0.054 \tphi(Y): 0.078\n",
            "==Train Epoch:\t\t\t 1173 \tLoss: 0.009922\t lambda: 3.870e-05\t lr: 3.677\n",
            "==Train Epoch:\t\t\t 1176 \tLoss: 0.009907\t lambda: 3.892e-05\t lr: 3.578\n",
            "--\t\t\tphi(X):\t 0.054 \tphi(Y): 0.077\n",
            "==Train Full Grad Epoch:\t 1179 \tLoss: 0.010002\t lambda: 3.904e-05\t lr: 2.410\n",
            "==Train Epoch:\t\t\t 1182 \tLoss: 0.009922\t lambda: 3.920e-05\t lr: 3.806\n",
            "--\t\t\tphi(X):\t 0.053 \tphi(Y): 0.076\n",
            "==Train Full Grad Epoch:\t 1185 \tLoss: 0.009925\t lambda: 3.926e-05\t lr: 2.409\n",
            "==Train Epoch:\t\t\t 1188 \tLoss: 0.009870\t lambda: 3.943e-05\t lr: 3.336\n",
            "--\t\t\tphi(X):\t 0.053 \tphi(Y): 0.075\n",
            "==Train Full Grad Epoch:\t 1191 \tLoss: 0.009997\t lambda: 3.960e-05\t lr: 2.406\n",
            "==Train Epoch:\t\t\t 1194 \tLoss: 0.009932\t lambda: 3.982e-05\t lr: 3.762\n",
            "--\t\t\tphi(X):\t 0.052 \tphi(Y): 0.075\n",
            "==Train Epoch:\t\t\t 1197 \tLoss: 0.009815\t lambda: 3.994e-05\t lr: 3.823\n",
            "\n",
            "Test set: Average loss: 0.0003\n",
            "\n",
            "==Train Full Grad Epoch:\t 1200 \tLoss: 0.010010\t lambda: 4.010e-05\t lr: 2.403\n",
            "--\t\t\tphi(X):\t 0.052 \tphi(Y): 0.073\n",
            "==Train Epoch:\t\t\t 1203 \tLoss: 0.009978\t lambda: 4.033e-05\t lr: 3.665\n",
            "==Train Epoch:\t\t\t 1206 \tLoss: 0.009832\t lambda: 4.044e-05\t lr: 3.752\n",
            "--\t\t\tphi(X):\t 0.052 \tphi(Y): 0.073\n",
            "==Train Full Grad Epoch:\t 1209 \tLoss: 0.010016\t lambda: 4.061e-05\t lr: 2.399\n",
            "==Train Epoch:\t\t\t 1212 \tLoss: 0.009938\t lambda: 4.084e-05\t lr: 3.658\n",
            "--\t\t\tphi(X):\t 0.051 \tphi(Y): 0.072\n",
            "==Train Full Grad Epoch:\t 1215 \tLoss: 0.009937\t lambda: 4.095e-05\t lr: 2.397\n",
            "==Train Epoch:\t\t\t 1218 \tLoss: 0.009983\t lambda: 4.117e-05\t lr: 3.821\n",
            "--\t\t\tphi(X):\t 0.051 \tphi(Y): 0.072\n",
            "==Train Full Grad Epoch:\t 1221 \tLoss: 0.009985\t lambda: 4.129e-05\t lr: 2.397\n",
            "==Train Epoch:\t\t\t 1224 \tLoss: 0.009909\t lambda: 4.146e-05\t lr: 3.527\n",
            "--\t\t\tphi(X):\t 0.050 \tphi(Y): 0.071\n",
            "==Train Epoch:\t\t\t 1227 \tLoss: 0.009991\t lambda: 4.168e-05\t lr: 3.826\n",
            "==Train Epoch:\t\t\t 1230 \tLoss: 0.009927\t lambda: 4.185e-05\t lr: 3.550\n",
            "--\t\t\tphi(X):\t 0.050 \tphi(Y): 0.070\n",
            "==Train Epoch:\t\t\t 1233 \tLoss: 0.009911\t lambda: 4.208e-05\t lr: 3.615\n",
            "==Train Epoch:\t\t\t 1236 \tLoss: 0.009923\t lambda: 4.225e-05\t lr: 3.745\n",
            "--\t\t\tphi(X):\t 0.049 \tphi(Y): 0.069\n",
            "==Train Epoch:\t\t\t 1239 \tLoss: 0.009923\t lambda: 4.241e-05\t lr: 3.528\n",
            "==Train Full Grad Epoch:\t 1242 \tLoss: 0.010004\t lambda: 4.258e-05\t lr: 2.387\n",
            "--\t\t\tphi(X):\t 0.048 \tphi(Y): 0.068\n",
            "==Train Epoch:\t\t\t 1245 \tLoss: 0.009924\t lambda: 4.275e-05\t lr: 3.503\n",
            "==Train Epoch:\t\t\t 1248 \tLoss: 0.009857\t lambda: 4.287e-05\t lr: 3.607\n",
            "--\t\t\tphi(X):\t 0.048 \tphi(Y): 0.067\n",
            "\n",
            "Test set: Average loss: 0.0002\n",
            "\n",
            "==Train Epoch:\t\t\t 1251 \tLoss: 0.009945\t lambda: 4.309e-05\t lr: 3.949\n",
            "==Train Full Grad Epoch:\t 1254 \tLoss: 0.009992\t lambda: 4.326e-05\t lr: 2.382\n",
            "--\t\t\tphi(X):\t 0.048 \tphi(Y): 0.066\n",
            "==Train Epoch:\t\t\t 1257 \tLoss: 0.009885\t lambda: 4.343e-05\t lr: 3.586\n",
            "==Train Epoch:\t\t\t 1260 \tLoss: 0.009895\t lambda: 4.360e-05\t lr: 3.736\n",
            "--\t\t\tphi(X):\t 0.047 \tphi(Y): 0.065\n",
            "==Train Full Grad Epoch:\t 1263 \tLoss: 0.009945\t lambda: 4.372e-05\t lr: 2.378\n",
            "==Train Full Grad Epoch:\t 1266 \tLoss: 0.010008\t lambda: 4.383e-05\t lr: 2.377\n",
            "--\t\t\tphi(X):\t 0.047 \tphi(Y): 0.064\n",
            "==Train Epoch:\t\t\t 1269 \tLoss: 0.009892\t lambda: 4.406e-05\t lr: 3.617\n",
            "==Train Epoch:\t\t\t 1272 \tLoss: 0.009871\t lambda: 4.417e-05\t lr: 3.460\n",
            "--\t\t\tphi(X):\t 0.046 \tphi(Y): 0.064\n",
            "==Train Full Grad Epoch:\t 1275 \tLoss: 0.009998\t lambda: 4.434e-05\t lr: 2.374\n",
            "==Train Full Grad Epoch:\t 1278 \tLoss: 0.009943\t lambda: 4.445e-05\t lr: 2.372\n",
            "--\t\t\tphi(X):\t 0.046 \tphi(Y): 0.063\n",
            "==Train Epoch:\t\t\t 1281 \tLoss: 0.009910\t lambda: 4.468e-05\t lr: 3.702\n",
            "==Train Epoch:\t\t\t 1284 \tLoss: 0.009933\t lambda: 4.491e-05\t lr: 3.863\n",
            "--\t\t\tphi(X):\t 0.046 \tphi(Y): 0.062\n",
            "==Train Epoch:\t\t\t 1287 \tLoss: 0.009883\t lambda: 4.513e-05\t lr: 3.657\n",
            "==Train Full Grad Epoch:\t 1290 \tLoss: 0.010008\t lambda: 4.530e-05\t lr: 2.366\n",
            "--\t\t\tphi(X):\t 0.045 \tphi(Y): 0.060\n",
            "==Train Epoch:\t\t\t 1293 \tLoss: 0.009849\t lambda: 4.542e-05\t lr: 3.404\n",
            "==Train Epoch:\t\t\t 1296 \tLoss: 0.009954\t lambda: 4.565e-05\t lr: 3.607\n",
            "--\t\t\tphi(X):\t 0.045 \tphi(Y): 0.060\n",
            "==Train Epoch:\t\t\t 1299 \tLoss: 0.009853\t lambda: 4.587e-05\t lr: 3.339\n",
            "\n",
            "Test set: Average loss: 0.0002\n",
            "\n",
            "==Train Epoch:\t\t\t 1302 \tLoss: 0.009900\t lambda: 4.604e-05\t lr: 3.733\n",
            "--\t\t\tphi(X):\t 0.044 \tphi(Y): 0.059\n",
            "==Train Epoch:\t\t\t 1305 \tLoss: 0.009973\t lambda: 4.627e-05\t lr: 3.649\n",
            "==Train Epoch:\t\t\t 1308 \tLoss: 0.009919\t lambda: 4.650e-05\t lr: 3.726\n",
            "--\t\t\tphi(X):\t 0.043 \tphi(Y): 0.057\n",
            "==Train Epoch:\t\t\t 1311 \tLoss: 0.009924\t lambda: 4.673e-05\t lr: 3.514\n",
            "==Train Full Grad Epoch:\t 1314 \tLoss: 0.010030\t lambda: 4.690e-05\t lr: 2.355\n",
            "--\t\t\tphi(X):\t 0.043 \tphi(Y): 0.056\n",
            "==Train Epoch:\t\t\t 1317 \tLoss: 0.009867\t lambda: 4.707e-05\t lr: 3.571\n",
            "==Train Epoch:\t\t\t 1320 \tLoss: 0.009935\t lambda: 4.730e-05\t lr: 3.856\n",
            "--\t\t\tphi(X):\t 0.043 \tphi(Y): 0.055\n",
            "==Train Full Grad Epoch:\t 1323 \tLoss: 0.010000\t lambda: 4.747e-05\t lr: 2.350\n",
            "==Train Epoch:\t\t\t 1326 \tLoss: 0.009949\t lambda: 4.770e-05\t lr: 3.542\n",
            "--\t\t\tphi(X):\t 0.042 \tphi(Y): 0.054\n",
            "==Train Full Grad Epoch:\t 1329 \tLoss: 0.010020\t lambda: 4.787e-05\t lr: 2.347\n",
            "==Train Epoch:\t\t\t 1332 \tLoss: 0.009919\t lambda: 4.804e-05\t lr: 3.479\n",
            "--\t\t\tphi(X):\t 0.042 \tphi(Y): 0.053\n",
            "==Train Full Grad Epoch:\t 1335 \tLoss: 0.010015\t lambda: 4.821e-05\t lr: 2.345\n",
            "==Train Epoch:\t\t\t 1338 \tLoss: 0.009910\t lambda: 4.844e-05\t lr: 3.638\n",
            "--\t\t\tphi(X):\t 0.041 \tphi(Y): 0.052\n",
            "==Train Epoch:\t\t\t 1341 \tLoss: 0.009878\t lambda: 4.867e-05\t lr: 3.655\n",
            "==Train Full Grad Epoch:\t 1344 \tLoss: 0.010015\t lambda: 4.884e-05\t lr: 2.342\n",
            "--\t\t\tphi(X):\t 0.041 \tphi(Y): 0.051\n",
            "==Train Epoch:\t\t\t 1347 \tLoss: 0.009894\t lambda: 4.907e-05\t lr: 3.576\n",
            "\n",
            "Test set: Average loss: 0.0002\n",
            "\n",
            "==Train Epoch:\t\t\t 1350 \tLoss: 0.009953\t lambda: 4.930e-05\t lr: 3.718\n",
            "--\t\t\tphi(X):\t 0.040 \tphi(Y): 0.051\n",
            "==Train Epoch:\t\t\t 1353 \tLoss: 0.009890\t lambda: 4.953e-05\t lr: 3.630\n",
            "==Train Epoch:\t\t\t 1356 \tLoss: 0.009959\t lambda: 4.970e-05\t lr: 3.600\n",
            "--\t\t\tphi(X):\t 0.040 \tphi(Y): 0.050\n",
            "==Train Epoch:\t\t\t 1359 \tLoss: 0.009924\t lambda: 4.982e-05\t lr: 3.611\n",
            "==Train Full Grad Epoch:\t 1362 \tLoss: 0.010011\t lambda: 4.999e-05\t lr: 2.341\n",
            "--\t\t\tphi(X):\t 0.040 \tphi(Y): 0.049\n",
            "==Train Epoch:\t\t\t 1365 \tLoss: 0.009945\t lambda: 5.045e-05\t lr: 3.566\n",
            "==Train Epoch:\t\t\t 1368 \tLoss: 0.009919\t lambda: 5.079e-05\t lr: 3.607\n",
            "--\t\t\tphi(X):\t 0.040 \tphi(Y): 0.049\n",
            "==Train Epoch:\t\t\t 1371 \tLoss: 0.009901\t lambda: 5.125e-05\t lr: 3.387\n",
            "==Train Epoch:\t\t\t 1374 \tLoss: 0.009934\t lambda: 5.159e-05\t lr: 3.492\n",
            "--\t\t\tphi(X):\t 0.039 \tphi(Y): 0.049\n",
            "==Train Epoch:\t\t\t 1377 \tLoss: 0.009985\t lambda: 5.205e-05\t lr: 3.636\n",
            "==Train Epoch:\t\t\t 1380 \tLoss: 0.009920\t lambda: 5.251e-05\t lr: 3.443\n",
            "--\t\t\tphi(X):\t 0.039 \tphi(Y): 0.048\n",
            "==Train Epoch:\t\t\t 1383 \tLoss: 0.009915\t lambda: 5.286e-05\t lr: 3.615\n",
            "==Train Epoch:\t\t\t 1386 \tLoss: 0.009906\t lambda: 5.320e-05\t lr: 3.702\n",
            "--\t\t\tphi(X):\t 0.039 \tphi(Y): 0.048\n",
            "==Train Full Grad Epoch:\t 1389 \tLoss: 0.010026\t lambda: 5.355e-05\t lr: 2.342\n",
            "==Train Full Grad Epoch:\t 1392 \tLoss: 0.010021\t lambda: 5.389e-05\t lr: 2.340\n",
            "--\t\t\tphi(X):\t 0.038 \tphi(Y): 0.047\n",
            "==Train Epoch:\t\t\t 1395 \tLoss: 0.009902\t lambda: 5.435e-05\t lr: 3.713\n",
            "==Train Full Grad Epoch:\t 1398 \tLoss: 0.010016\t lambda: 5.469e-05\t lr: 2.341\n",
            "--\t\t\tphi(X):\t 0.038 \tphi(Y): 0.047\n",
            "\n",
            "Test set: Average loss: 0.0002\n",
            "\n",
            "==Train Epoch:\t\t\t 1401 \tLoss: 0.009925\t lambda: 5.515e-05\t lr: 3.279\n",
            "==Train Full Grad Epoch:\t 1404 \tLoss: 0.010007\t lambda: 5.550e-05\t lr: 2.341\n",
            "--\t\t\tphi(X):\t 0.037 \tphi(Y): 0.046\n",
            "==Train Epoch:\t\t\t 1407 \tLoss: 0.009900\t lambda: 5.584e-05\t lr: 3.585\n",
            "==Train Full Grad Epoch:\t 1410 \tLoss: 0.010018\t lambda: 5.619e-05\t lr: 2.341\n",
            "--\t\t\tphi(X):\t 0.037 \tphi(Y): 0.046\n",
            "==Train Epoch:\t\t\t 1413 \tLoss: 0.009916\t lambda: 5.653e-05\t lr: 3.536\n",
            "==Train Epoch:\t\t\t 1416 \tLoss: 0.009914\t lambda: 5.688e-05\t lr: 3.259\n",
            "--\t\t\tphi(X):\t 0.037 \tphi(Y): 0.046\n",
            "==Train Full Grad Epoch:\t 1419 \tLoss: 0.010025\t lambda: 5.722e-05\t lr: 2.341\n",
            "==Train Full Grad Epoch:\t 1422 \tLoss: 0.010016\t lambda: 5.757e-05\t lr: 2.341\n",
            "--\t\t\tphi(X):\t 0.036 \tphi(Y): 0.045\n",
            "==Train Epoch:\t\t\t 1425 \tLoss: 0.009896\t lambda: 5.780e-05\t lr: 3.639\n",
            "==Train Epoch:\t\t\t 1428 \tLoss: 0.009946\t lambda: 5.826e-05\t lr: 3.398\n",
            "--\t\t\tphi(X):\t 0.037 \tphi(Y): 0.045\n",
            "==Train Epoch:\t\t\t 1431 \tLoss: 0.009930\t lambda: 5.872e-05\t lr: 3.733\n",
            "==Train Epoch:\t\t\t 1434 \tLoss: 0.009948\t lambda: 5.918e-05\t lr: 3.783\n",
            "--\t\t\tphi(X):\t 0.036 \tphi(Y): 0.044\n",
            "==Train Epoch:\t\t\t 1437 \tLoss: 0.009936\t lambda: 5.964e-05\t lr: 3.425\n",
            "==Train Epoch:\t\t\t 1440 \tLoss: 0.009941\t lambda: 5.987e-05\t lr: 3.570\n",
            "--\t\t\tphi(X):\t 0.036 \tphi(Y): 0.044\n",
            "==Train Epoch:\t\t\t 1443 \tLoss: 0.009952\t lambda: 6.033e-05\t lr: 3.496\n",
            "==Train Epoch:\t\t\t 1446 \tLoss: 0.009917\t lambda: 6.068e-05\t lr: 3.772\n",
            "--\t\t\tphi(X):\t 0.036 \tphi(Y): 0.043\n",
            "==Train Epoch:\t\t\t 1449 \tLoss: 0.009911\t lambda: 6.114e-05\t lr: 3.416\n",
            "\n",
            "Test set: Average loss: 0.0002\n",
            "\n",
            "==Train Full Grad Epoch:\t 1452 \tLoss: 0.010042\t lambda: 6.149e-05\t lr: 2.342\n",
            "--\t\t\tphi(X):\t 0.035 \tphi(Y): 0.042\n",
            "==Train Epoch:\t\t\t 1455 \tLoss: 0.009954\t lambda: 6.172e-05\t lr: 3.614\n",
            "==Train Epoch:\t\t\t 1458 \tLoss: 0.009928\t lambda: 6.206e-05\t lr: 3.517\n",
            "--\t\t\tphi(X):\t 0.035 \tphi(Y): 0.042\n",
            "==Train Epoch:\t\t\t 1461 \tLoss: 0.009931\t lambda: 6.241e-05\t lr: 3.584\n",
            "==Train Epoch:\t\t\t 1464 \tLoss: 0.009934\t lambda: 6.287e-05\t lr: 3.422\n",
            "--\t\t\tphi(X):\t 0.034 \tphi(Y): 0.042\n",
            "==Train Epoch:\t\t\t 1467 \tLoss: 0.009959\t lambda: 6.333e-05\t lr: 3.408\n",
            "==Train Full Grad Epoch:\t 1470 \tLoss: 0.010033\t lambda: 6.356e-05\t lr: 2.343\n",
            "--\t\t\tphi(X):\t 0.034 \tphi(Y): 0.041\n",
            "==Train Epoch:\t\t\t 1473 \tLoss: 0.009891\t lambda: 6.391e-05\t lr: 3.359\n",
            "==Train Epoch:\t\t\t 1476 \tLoss: 0.009978\t lambda: 6.437e-05\t lr: 3.312\n",
            "--\t\t\tphi(X):\t 0.034 \tphi(Y): 0.041\n",
            "==Train Epoch:\t\t\t 1479 \tLoss: 0.009983\t lambda: 6.460e-05\t lr: 3.251\n",
            "==Train Epoch:\t\t\t 1482 \tLoss: 0.010006\t lambda: 6.507e-05\t lr: 3.743\n",
            "--\t\t\tphi(X):\t 0.034 \tphi(Y): 0.040\n",
            "==Train Epoch:\t\t\t 1485 \tLoss: 0.009909\t lambda: 6.530e-05\t lr: 3.649\n",
            "==Train Epoch:\t\t\t 1488 \tLoss: 0.009945\t lambda: 6.576e-05\t lr: 3.546\n",
            "--\t\t\tphi(X):\t 0.033 \tphi(Y): 0.040\n",
            "==Train Full Grad Epoch:\t 1491 \tLoss: 0.010025\t lambda: 6.611e-05\t lr: 2.344\n",
            "==Train Full Grad Epoch:\t 1494 \tLoss: 0.009977\t lambda: 6.634e-05\t lr: 2.344\n",
            "--\t\t\tphi(X):\t 0.032 \tphi(Y): 0.039\n",
            "==Train Epoch:\t\t\t 1497 \tLoss: 0.009955\t lambda: 6.669e-05\t lr: 3.313\n",
            "\n",
            "Test set: Average loss: 0.0002\n",
            "\n",
            "==Train Full Grad Epoch:\t 1500 \tLoss: 0.010036\t lambda: 6.703e-05\t lr: 2.346\n",
            "--\t\t\tphi(X):\t 0.032 \tphi(Y): 0.038\n",
            "==Train Epoch:\t\t\t 1503 \tLoss: 0.009902\t lambda: 6.738e-05\t lr: 3.489\n",
            "==Train Epoch:\t\t\t 1506 \tLoss: 0.009915\t lambda: 6.773e-05\t lr: 3.410\n",
            "--\t\t\tphi(X):\t 0.032 \tphi(Y): 0.038\n",
            "==Train Epoch:\t\t\t 1509 \tLoss: 0.009962\t lambda: 6.819e-05\t lr: 3.115\n",
            "==Train Full Grad Epoch:\t 1512 \tLoss: 0.010037\t lambda: 6.854e-05\t lr: 2.346\n",
            "--\t\t\tphi(X):\t 0.032 \tphi(Y): 0.037\n",
            "==Train Epoch:\t\t\t 1515 \tLoss: 0.009996\t lambda: 6.900e-05\t lr: 3.594\n",
            "==Train Epoch:\t\t\t 1518 \tLoss: 0.010021\t lambda: 6.947e-05\t lr: 3.816\n",
            "--\t\t\tphi(X):\t 0.032 \tphi(Y): 0.037\n",
            "==Train Epoch:\t\t\t 1521 \tLoss: 0.009941\t lambda: 6.993e-05\t lr: 3.339\n",
            "==Train Epoch:\t\t\t 1524 \tLoss: 0.009938\t lambda: 7.039e-05\t lr: 3.285\n",
            "--\t\t\tphi(X):\t 0.031 \tphi(Y): 0.036\n",
            "==Train Full Grad Epoch:\t 1527 \tLoss: 0.010032\t lambda: 7.074e-05\t lr: 2.344\n",
            "==Train Epoch:\t\t\t 1530 \tLoss: 0.009929\t lambda: 7.109e-05\t lr: 3.538\n",
            "--\t\t\tphi(X):\t 0.030 \tphi(Y): 0.035\n",
            "==Train Full Grad Epoch:\t 1533 \tLoss: 0.009975\t lambda: 7.121e-05\t lr: 2.346\n",
            "==Train Epoch:\t\t\t 1536 \tLoss: 0.010016\t lambda: 7.156e-05\t lr: 3.588\n",
            "--\t\t\tphi(X):\t 0.030 \tphi(Y): 0.035\n",
            "==Train Epoch:\t\t\t 1539 \tLoss: 0.009988\t lambda: 7.190e-05\t lr: 3.574\n",
            "==Train Full Grad Epoch:\t 1542 \tLoss: 0.010028\t lambda: 7.225e-05\t lr: 2.348\n",
            "--\t\t\tphi(X):\t 0.030 \tphi(Y): 0.034\n",
            "==Train Epoch:\t\t\t 1545 \tLoss: 0.009936\t lambda: 7.272e-05\t lr: 3.313\n",
            "==Train Full Grad Epoch:\t 1548 \tLoss: 0.009974\t lambda: 7.283e-05\t lr: 2.348\n",
            "--\t\t\tphi(X):\t 0.029 \tphi(Y): 0.034\n",
            "\n",
            "Test set: Average loss: 0.0001\n",
            "\n",
            "==Train Full Grad Epoch:\t 1551 \tLoss: 0.010028\t lambda: 7.318e-05\t lr: 2.350\n",
            "==Train Epoch:\t\t\t 1554 \tLoss: 0.009953\t lambda: 7.365e-05\t lr: 3.657\n",
            "--\t\t\tphi(X):\t 0.029 \tphi(Y): 0.034\n",
            "==Train Full Grad Epoch:\t 1557 \tLoss: 0.010027\t lambda: 7.399e-05\t lr: 2.350\n",
            "==Train Full Grad Epoch:\t 1560 \tLoss: 0.009978\t lambda: 7.423e-05\t lr: 2.351\n",
            "--\t\t\tphi(X):\t 0.028 \tphi(Y): 0.034\n",
            "==Train Epoch:\t\t\t 1563 \tLoss: 0.009966\t lambda: 7.469e-05\t lr: 3.802\n",
            "==Train Full Grad Epoch:\t 1566 \tLoss: 0.010012\t lambda: 7.504e-05\t lr: 2.353\n",
            "--\t\t\tphi(X):\t 0.028 \tphi(Y): 0.034\n",
            "==Train Epoch:\t\t\t 1569 \tLoss: 0.009926\t lambda: 7.539e-05\t lr: 3.507\n",
            "==Train Full Grad Epoch:\t 1572 \tLoss: 0.010019\t lambda: 7.574e-05\t lr: 2.353\n",
            "--\t\t\tphi(X):\t 0.028 \tphi(Y): 0.034\n",
            "==Train Epoch:\t\t\t 1575 \tLoss: 0.009934\t lambda: 7.620e-05\t lr: 3.568\n",
            "==Train Epoch:\t\t\t 1578 \tLoss: 0.009982\t lambda: 7.655e-05\t lr: 3.503\n",
            "--\t\t\tphi(X):\t 0.027 \tphi(Y): 0.034\n",
            "==Train Full Grad Epoch:\t 1581 \tLoss: 0.010031\t lambda: 7.690e-05\t lr: 2.355\n",
            "==Train Full Grad Epoch:\t 1584 \tLoss: 0.009982\t lambda: 7.714e-05\t lr: 2.355\n",
            "--\t\t\tphi(X):\t 0.026 \tphi(Y): 0.034\n",
            "==Train Epoch:\t\t\t 1587 \tLoss: 0.009930\t lambda: 7.760e-05\t lr: 3.326\n",
            "==Train Epoch:\t\t\t 1590 \tLoss: 0.009916\t lambda: 7.783e-05\t lr: 3.433\n",
            "--\t\t\tphi(X):\t 0.027 \tphi(Y): 0.034\n",
            "==Train Epoch:\t\t\t 1593 \tLoss: 0.009953\t lambda: 7.818e-05\t lr: 3.430\n",
            "==Train Epoch:\t\t\t 1596 \tLoss: 0.009929\t lambda: 7.865e-05\t lr: 3.560\n",
            "--\t\t\tphi(X):\t 0.026 \tphi(Y): 0.034\n",
            "==Train Epoch:\t\t\t 1599 \tLoss: 0.009978\t lambda: 7.911e-05\t lr: 3.214\n",
            "\n",
            "Test set: Average loss: 0.0001\n",
            "\n",
            "==Train Epoch:\t\t\t 1602 \tLoss: 0.009921\t lambda: 7.946e-05\t lr: 3.598\n",
            "--\t\t\tphi(X):\t 0.026 \tphi(Y): 0.034\n",
            "==Train Epoch:\t\t\t 1605 \tLoss: 0.010015\t lambda: 7.981e-05\t lr: 3.180\n",
            "==Train Epoch:\t\t\t 1608 \tLoss: 0.009936\t lambda: 8.028e-05\t lr: 3.619\n",
            "--\t\t\tphi(X):\t 0.025 \tphi(Y): 0.034\n",
            "==Train Full Grad Epoch:\t 1611 \tLoss: 0.010028\t lambda: 8.051e-05\t lr: 2.362\n",
            "==Train Full Grad Epoch:\t 1614 \tLoss: 0.010017\t lambda: 8.074e-05\t lr: 2.362\n",
            "--\t\t\tphi(X):\t 0.024 \tphi(Y): 0.033\n",
            "==Train Epoch:\t\t\t 1617 \tLoss: 0.009943\t lambda: 8.109e-05\t lr: 3.257\n",
            "==Train Epoch:\t\t\t 1620 \tLoss: 0.009973\t lambda: 8.156e-05\t lr: 3.452\n",
            "--\t\t\tphi(X):\t 0.024 \tphi(Y): 0.034\n",
            "==Train Full Grad Epoch:\t 1623 \tLoss: 0.009988\t lambda: 8.179e-05\t lr: 2.364\n",
            "==Train Epoch:\t\t\t 1626 \tLoss: 0.009928\t lambda: 8.203e-05\t lr: 3.436\n",
            "--\t\t\tphi(X):\t 0.024 \tphi(Y): 0.034\n",
            "==Train Epoch:\t\t\t 1629 \tLoss: 0.009940\t lambda: 8.238e-05\t lr: 3.661\n",
            "==Train Epoch:\t\t\t 1632 \tLoss: 0.009944\t lambda: 8.284e-05\t lr: 3.384\n",
            "--\t\t\tphi(X):\t 0.023 \tphi(Y): 0.034\n",
            "==Train Full Grad Epoch:\t 1635 \tLoss: 0.010022\t lambda: 8.319e-05\t lr: 2.365\n",
            "==Train Epoch:\t\t\t 1638 \tLoss: 0.010025\t lambda: 8.354e-05\t lr: 3.478\n",
            "--\t\t\tphi(X):\t 0.023 \tphi(Y): 0.034\n",
            "==Train Full Grad Epoch:\t 1641 \tLoss: 0.010021\t lambda: 8.389e-05\t lr: 2.369\n",
            "==Train Epoch:\t\t\t 1644 \tLoss: 0.009935\t lambda: 8.424e-05\t lr: 3.689\n",
            "--\t\t\tphi(X):\t 0.022 \tphi(Y): 0.034\n",
            "==Train Epoch:\t\t\t 1647 \tLoss: 0.009939\t lambda: 8.459e-05\t lr: 3.420\n",
            "\n",
            "Test set: Average loss: 0.0001\n",
            "\n",
            "==Train Epoch:\t\t\t 1650 \tLoss: 0.009971\t lambda: 8.506e-05\t lr: 3.575\n",
            "--\t\t\tphi(X):\t 0.022 \tphi(Y): 0.034\n",
            "==Train Epoch:\t\t\t 1653 \tLoss: 0.009978\t lambda: 8.541e-05\t lr: 3.178\n",
            "==Train Full Grad Epoch:\t 1656 \tLoss: 0.009989\t lambda: 8.564e-05\t lr: 2.372\n",
            "--\t\t\tphi(X):\t 0.021 \tphi(Y): 0.033\n",
            "==Train Full Grad Epoch:\t 1659 \tLoss: 0.010011\t lambda: 8.599e-05\t lr: 2.374\n",
            "==Train Epoch:\t\t\t 1662 \tLoss: 0.010004\t lambda: 8.646e-05\t lr: 3.551\n",
            "--\t\t\tphi(X):\t 0.020 \tphi(Y): 0.034\n",
            "==Train Full Grad Epoch:\t 1665 \tLoss: 0.010022\t lambda: 8.681e-05\t lr: 2.374\n",
            "==Train Epoch:\t\t\t 1668 \tLoss: 0.009961\t lambda: 8.728e-05\t lr: 3.504\n",
            "--\t\t\tphi(X):\t 0.020 \tphi(Y): 0.034\n",
            "==Train Epoch:\t\t\t 1671 \tLoss: 0.009916\t lambda: 8.775e-05\t lr: 3.532\n",
            "==Train Epoch:\t\t\t 1674 \tLoss: 0.009942\t lambda: 8.810e-05\t lr: 3.759\n",
            "--\t\t\tphi(X):\t 0.019 \tphi(Y): 0.034\n",
            "==Train Epoch:\t\t\t 1677 \tLoss: 0.009959\t lambda: 8.856e-05\t lr: 3.452\n",
            "==Train Epoch:\t\t\t 1680 \tLoss: 0.009978\t lambda: 8.903e-05\t lr: 3.581\n",
            "--\t\t\tphi(X):\t 0.018 \tphi(Y): 0.034\n",
            "==Train Epoch:\t\t\t 1683 \tLoss: 0.009935\t lambda: 8.950e-05\t lr: 3.622\n",
            "==Train Epoch:\t\t\t 1686 \tLoss: 0.009956\t lambda: 8.997e-05\t lr: 3.414\n",
            "--\t\t\tphi(X):\t 0.018 \tphi(Y): 0.034\n",
            "==Train Full Grad Epoch:\t 1689 \tLoss: 0.010020\t lambda: 9.032e-05\t lr: 2.384\n",
            "==Train Epoch:\t\t\t 1692 \tLoss: 0.009914\t lambda: 9.079e-05\t lr: 3.431\n",
            "--\t\t\tphi(X):\t 0.017 \tphi(Y): 0.033\n",
            "==Train Full Grad Epoch:\t 1695 \tLoss: 0.009991\t lambda: 9.102e-05\t lr: 2.384\n",
            "==Train Epoch:\t\t\t 1698 \tLoss: 0.009921\t lambda: 9.149e-05\t lr: 3.418\n",
            "--\t\t\tphi(X):\t 0.016 \tphi(Y): 0.033\n",
            "\n",
            "Test set: Average loss: 0.0001\n",
            "\n",
            "==Train Full Grad Epoch:\t 1701 \tLoss: 0.010025\t lambda: 9.184e-05\t lr: 2.387\n",
            "==Train Epoch:\t\t\t 1704 \tLoss: 0.009975\t lambda: 9.219e-05\t lr: 3.484\n",
            "--\t\t\tphi(X):\t 0.016 \tphi(Y): 0.033\n",
            "==Train Epoch:\t\t\t 1707 \tLoss: 0.009946\t lambda: 9.266e-05\t lr: 3.565\n",
            "==Train Epoch:\t\t\t 1710 \tLoss: 0.009967\t lambda: 9.313e-05\t lr: 3.485\n",
            "--\t\t\tphi(X):\t 0.015 \tphi(Y): 0.033\n",
            "==Train Epoch:\t\t\t 1713 \tLoss: 0.009990\t lambda: 9.360e-05\t lr: 3.405\n",
            "==Train Epoch:\t\t\t 1716 \tLoss: 0.009910\t lambda: 9.395e-05\t lr: 3.491\n",
            "--\t\t\tphi(X):\t 0.014 \tphi(Y): 0.033\n",
            "==Train Epoch:\t\t\t 1719 \tLoss: 0.009914\t lambda: 9.418e-05\t lr: 3.645\n",
            "==Train Epoch:\t\t\t 1722 \tLoss: 0.010023\t lambda: 9.453e-05\t lr: 3.918\n",
            "--\t\t\tphi(X):\t 0.014 \tphi(Y): 0.033\n",
            "==Train Epoch:\t\t\t 1725 \tLoss: 0.009937\t lambda: 9.488e-05\t lr: 3.212\n",
            "==Train Full Grad Epoch:\t 1728 \tLoss: 0.010021\t lambda: 9.524e-05\t lr: 2.394\n",
            "--\t\t\tphi(X):\t 0.013 \tphi(Y): 0.033\n",
            "==Train Full Grad Epoch:\t 1731 \tLoss: 0.010023\t lambda: 9.547e-05\t lr: 2.395\n",
            "==Train Epoch:\t\t\t 1734 \tLoss: 0.009933\t lambda: 9.594e-05\t lr: 3.334\n",
            "--\t\t\tphi(X):\t 0.013 \tphi(Y): 0.033\n",
            "==Train Epoch:\t\t\t 1737 \tLoss: 0.009989\t lambda: 9.629e-05\t lr: 3.568\n",
            "==Train Epoch:\t\t\t 1740 \tLoss: 0.009936\t lambda: 9.664e-05\t lr: 3.141\n",
            "--\t\t\tphi(X):\t 0.012 \tphi(Y): 0.033\n",
            "==Train Epoch:\t\t\t 1743 \tLoss: 0.009963\t lambda: 9.700e-05\t lr: 3.716\n",
            "==Train Full Grad Epoch:\t 1746 \tLoss: 0.009994\t lambda: 9.723e-05\t lr: 2.400\n",
            "--\t\t\tphi(X):\t 0.011 \tphi(Y): 0.032\n",
            "==Train Epoch:\t\t\t 1749 \tLoss: 0.009905\t lambda: 9.758e-05\t lr: 3.408\n",
            "\n",
            "Test set: Average loss: 0.0001\n",
            "\n",
            "==Train Epoch:\t\t\t 1752 \tLoss: 0.009978\t lambda: 9.805e-05\t lr: 3.498\n",
            "--\t\t\tphi(X):\t 0.011 \tphi(Y): 0.033\n",
            "==Train Epoch:\t\t\t 1755 \tLoss: 0.009925\t lambda: 9.841e-05\t lr: 3.525\n",
            "==Train Epoch:\t\t\t 1758 \tLoss: 0.009982\t lambda: 9.876e-05\t lr: 3.251\n",
            "--\t\t\tphi(X):\t 0.010 \tphi(Y): 0.033\n",
            "==Train Full Grad Epoch:\t 1761 \tLoss: 0.010026\t lambda: 9.911e-05\t lr: 2.405\n",
            "==Train Epoch:\t\t\t 1764 \tLoss: 0.009965\t lambda: 9.958e-05\t lr: 3.543\n",
            "--\t\t\tphi(X):\t 0.009 \tphi(Y): 0.033\n",
            "==Train Epoch:\t\t\t 1767 \tLoss: 0.009953\t lambda: 1.000e-04\t lr: 3.522\n",
            "==Train Epoch:\t\t\t 1770 \tLoss: 0.009934\t lambda: 1.004e-04\t lr: 3.523\n",
            "--\t\t\tphi(X):\t 0.008 \tphi(Y): 0.033\n",
            "==Train Epoch:\t\t\t 1773 \tLoss: 0.009966\t lambda: 1.009e-04\t lr: 3.575\n",
            "==Train Full Grad Epoch:\t 1776 \tLoss: 0.010019\t lambda: 1.012e-04\t lr: 2.411\n",
            "--\t\t\tphi(X):\t 0.007 \tphi(Y): 0.032\n",
            "==Train Epoch:\t\t\t 1779 \tLoss: 0.010015\t lambda: 1.016e-04\t lr: 3.334\n",
            "==Train Epoch:\t\t\t 1782 \tLoss: 0.009972\t lambda: 1.019e-04\t lr: 3.727\n",
            "--\t\t\tphi(X):\t 0.007 \tphi(Y): 0.033\n",
            "==Train Epoch:\t\t\t 1785 \tLoss: 0.009989\t lambda: 1.022e-04\t lr: 3.454\n",
            "==Train Epoch:\t\t\t 1788 \tLoss: 0.009918\t lambda: 1.026e-04\t lr: 3.747\n",
            "--\t\t\tphi(X):\t 0.006 \tphi(Y): 0.033\n",
            "==Train Epoch:\t\t\t 1791 \tLoss: 0.009972\t lambda: 1.031e-04\t lr: 3.327\n",
            "==Train Full Grad Epoch:\t 1794 \tLoss: 0.009996\t lambda: 1.032e-04\t lr: 2.417\n",
            "--\t\t\tphi(X):\t 0.005 \tphi(Y): 0.032\n",
            "==Train Full Grad Epoch:\t 1797 \tLoss: 0.009999\t lambda: 1.035e-04\t lr: 2.418\n",
            "\n",
            "Test set: Average loss: 0.0001\n",
            "\n",
            "==Train Epoch:\t\t\t 1800 \tLoss: 0.009990\t lambda: 1.039e-04\t lr: 3.362\n",
            "--\t\t\tphi(X):\t 0.005 \tphi(Y): 0.033\n",
            "==Train Full Grad Epoch:\t 1803 \tLoss: 0.010033\t lambda: 1.042e-04\t lr: 2.419\n",
            "==Train Epoch:\t\t\t 1806 \tLoss: 0.009960\t lambda: 1.046e-04\t lr: 3.531\n",
            "--\t\t\tphi(X):\t 0.005 \tphi(Y): 0.033\n",
            "==Train Full Grad Epoch:\t 1809 \tLoss: 0.010000\t lambda: 1.049e-04\t lr: 2.423\n",
            "==Train Epoch:\t\t\t 1812 \tLoss: 0.009916\t lambda: 1.053e-04\t lr: 3.436\n",
            "--\t\t\tphi(X):\t 0.005 \tphi(Y): 0.032\n",
            "==Train Full Grad Epoch:\t 1815 \tLoss: 0.010026\t lambda: 1.056e-04\t lr: 2.426\n",
            "==Train Epoch:\t\t\t 1818 \tLoss: 0.010008\t lambda: 1.061e-04\t lr: 3.474\n",
            "--\t\t\tphi(X):\t 0.005 \tphi(Y): 0.032\n",
            "==Train Epoch:\t\t\t 1821 \tLoss: 0.009921\t lambda: 1.064e-04\t lr: 3.360\n",
            "==Train Epoch:\t\t\t 1824 \tLoss: 0.009961\t lambda: 1.068e-04\t lr: 3.458\n",
            "--\t\t\tphi(X):\t 0.004 \tphi(Y): 0.032\n",
            "==Train Epoch:\t\t\t 1827 \tLoss: 0.009921\t lambda: 1.072e-04\t lr: 3.514\n",
            "==Train Epoch:\t\t\t 1830 \tLoss: 0.009958\t lambda: 1.075e-04\t lr: 3.544\n",
            "--\t\t\tphi(X):\t 0.004 \tphi(Y): 0.032\n",
            "==Train Epoch:\t\t\t 1833 \tLoss: 0.009977\t lambda: 1.079e-04\t lr: 3.523\n",
            "==Train Full Grad Epoch:\t 1836 \tLoss: 0.010029\t lambda: 1.083e-04\t lr: 2.437\n",
            "--\t\t\tphi(X):\t 0.003 \tphi(Y): 0.032\n",
            "==Train Epoch:\t\t\t 1839 \tLoss: 0.009973\t lambda: 1.086e-04\t lr: 3.503\n",
            "==Train Epoch:\t\t\t 1842 \tLoss: 0.009955\t lambda: 1.091e-04\t lr: 3.485\n",
            "--\t\t\tphi(X):\t 0.003 \tphi(Y): 0.032\n",
            "==Train Full Grad Epoch:\t 1845 \tLoss: 0.010003\t lambda: 1.094e-04\t lr: 2.441\n",
            "==Train Epoch:\t\t\t 1848 \tLoss: 0.009975\t lambda: 1.097e-04\t lr: 3.283\n",
            "--\t\t\tphi(X):\t 0.003 \tphi(Y): 0.032\n",
            "\n",
            "Test set: Average loss: 0.0001\n",
            "\n",
            "==Train Epoch:\t\t\t 1851 \tLoss: 0.009999\t lambda: 1.102e-04\t lr: 3.365\n",
            "==Train Epoch:\t\t\t 1854 \tLoss: 0.009953\t lambda: 1.104e-04\t lr: 3.734\n",
            "--\t\t\tphi(X):\t 0.003 \tphi(Y): 0.032\n",
            "==Train Epoch:\t\t\t 1857 \tLoss: 0.009982\t lambda: 1.108e-04\t lr: 3.520\n",
            "==Train Epoch:\t\t\t 1860 \tLoss: 0.009938\t lambda: 1.112e-04\t lr: 3.473\n",
            "--\t\t\tphi(X):\t 0.002 \tphi(Y): 0.031\n",
            "==Train Epoch:\t\t\t 1863 \tLoss: 0.009957\t lambda: 1.116e-04\t lr: 3.526\n",
            "==Train Epoch:\t\t\t 1866 \tLoss: 0.009981\t lambda: 1.119e-04\t lr: 3.631\n",
            "--\t\t\tphi(X):\t 0.002 \tphi(Y): 0.031\n",
            "==Train Epoch:\t\t\t 1869 \tLoss: 0.009969\t lambda: 1.124e-04\t lr: 3.732\n",
            "==Train Epoch:\t\t\t 1872 \tLoss: 0.009988\t lambda: 1.128e-04\t lr: 3.501\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.031\n",
            "==Train Full Grad Epoch:\t 1875 \tLoss: 0.010002\t lambda: 1.130e-04\t lr: 2.457\n",
            "==Train Full Grad Epoch:\t 1878 \tLoss: 0.010020\t lambda: 1.132e-04\t lr: 2.457\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.031\n",
            "==Train Epoch:\t\t\t 1881 \tLoss: 0.009922\t lambda: 1.136e-04\t lr: 3.465\n",
            "==Train Epoch:\t\t\t 1884 \tLoss: 0.010024\t lambda: 1.141e-04\t lr: 3.278\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.031\n",
            "==Train Full Grad Epoch:\t 1887 \tLoss: 0.010016\t lambda: 1.143e-04\t lr: 2.462\n",
            "==Train Full Grad Epoch:\t 1890 \tLoss: 0.010021\t lambda: 1.147e-04\t lr: 2.464\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.030\n",
            "==Train Full Grad Epoch:\t 1893 \tLoss: 0.010005\t lambda: 1.148e-04\t lr: 2.464\n",
            "==Train Epoch:\t\t\t 1896 \tLoss: 0.009980\t lambda: 1.151e-04\t lr: 3.517\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.031\n",
            "==Train Epoch:\t\t\t 1899 \tLoss: 0.009970\t lambda: 1.156e-04\t lr: 3.754\n",
            "\n",
            "Test set: Average loss: 0.0001\n",
            "\n",
            "==Train Epoch:\t\t\t 1902 \tLoss: 0.010012\t lambda: 1.161e-04\t lr: 3.564\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.031\n",
            "==Train Epoch:\t\t\t 1905 \tLoss: 0.009942\t lambda: 1.164e-04\t lr: 3.619\n",
            "==Train Epoch:\t\t\t 1908 \tLoss: 0.010026\t lambda: 1.169e-04\t lr: 3.736\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.030\n",
            "==Train Epoch:\t\t\t 1911 \tLoss: 0.009960\t lambda: 1.174e-04\t lr: 3.508\n",
            "==Train Full Grad Epoch:\t 1914 \tLoss: 0.010004\t lambda: 1.176e-04\t lr: 2.470\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.030\n",
            "==Train Epoch:\t\t\t 1917 \tLoss: 0.010007\t lambda: 1.180e-04\t lr: 3.555\n",
            "==Train Epoch:\t\t\t 1920 \tLoss: 0.009960\t lambda: 1.183e-04\t lr: 3.394\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.030\n",
            "==Train Full Grad Epoch:\t 1923 \tLoss: 0.010025\t lambda: 1.187e-04\t lr: 2.472\n",
            "==Train Epoch:\t\t\t 1926 \tLoss: 0.009993\t lambda: 1.190e-04\t lr: 3.637\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.030\n",
            "==Train Epoch:\t\t\t 1929 \tLoss: 0.010000\t lambda: 1.194e-04\t lr: 3.582\n",
            "==Train Epoch:\t\t\t 1932 \tLoss: 0.009963\t lambda: 1.199e-04\t lr: 3.697\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.030\n",
            "==Train Epoch:\t\t\t 1935 \tLoss: 0.009964\t lambda: 1.203e-04\t lr: 3.601\n",
            "==Train Epoch:\t\t\t 1938 \tLoss: 0.009989\t lambda: 1.208e-04\t lr: 3.605\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.030\n",
            "==Train Epoch:\t\t\t 1941 \tLoss: 0.009944\t lambda: 1.212e-04\t lr: 3.411\n",
            "==Train Epoch:\t\t\t 1944 \tLoss: 0.010019\t lambda: 1.215e-04\t lr: 3.523\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.030\n",
            "==Train Full Grad Epoch:\t 1947 \tLoss: 0.010007\t lambda: 1.218e-04\t lr: 2.479\n",
            "\n",
            "Test set: Average loss: 0.0001\n",
            "\n",
            "==Train Epoch:\t\t\t 1950 \tLoss: 0.009981\t lambda: 1.221e-04\t lr: 3.325\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.029\n",
            "==Train Epoch:\t\t\t 1953 \tLoss: 0.009996\t lambda: 1.226e-04\t lr: 3.536\n",
            "==Train Full Grad Epoch:\t 1956 \tLoss: 0.010025\t lambda: 1.229e-04\t lr: 2.479\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.029\n",
            "==Train Epoch:\t\t\t 1959 \tLoss: 0.009975\t lambda: 1.234e-04\t lr: 3.295\n",
            "==Train Epoch:\t\t\t 1962 \tLoss: 0.009986\t lambda: 1.239e-04\t lr: 3.530\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.029\n",
            "==Train Epoch:\t\t\t 1965 \tLoss: 0.009964\t lambda: 1.242e-04\t lr: 3.659\n",
            "==Train Full Grad Epoch:\t 1968 \tLoss: 0.010025\t lambda: 1.246e-04\t lr: 2.483\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.029\n",
            "==Train Epoch:\t\t\t 1971 \tLoss: 0.009970\t lambda: 1.251e-04\t lr: 3.871\n",
            "==Train Epoch:\t\t\t 1974 \tLoss: 0.009985\t lambda: 1.255e-04\t lr: 3.518\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.029\n",
            "==Train Epoch:\t\t\t 1977 \tLoss: 0.009929\t lambda: 1.259e-04\t lr: 3.468\n",
            "==Train Epoch:\t\t\t 1980 \tLoss: 0.009955\t lambda: 1.261e-04\t lr: 3.687\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.029\n",
            "==Train Epoch:\t\t\t 1983 \tLoss: 0.009961\t lambda: 1.266e-04\t lr: 3.531\n",
            "==Train Full Grad Epoch:\t 1986 \tLoss: 0.010012\t lambda: 1.268e-04\t lr: 2.488\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.028\n",
            "==Train Epoch:\t\t\t 1989 \tLoss: 0.010034\t lambda: 1.273e-04\t lr: 3.666\n",
            "==Train Epoch:\t\t\t 1992 \tLoss: 0.009986\t lambda: 1.277e-04\t lr: 3.661\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.028\n",
            "==Train Full Grad Epoch:\t 1995 \tLoss: 0.010007\t lambda: 1.279e-04\t lr: 2.490\n",
            "==Train Full Grad Epoch:\t 1998 \tLoss: 0.010013\t lambda: 1.280e-04\t lr: 2.490\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.028\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Epoch:\t\t\t 2001 \tLoss: 0.009974\t lambda: 1.284e-04\t lr: 3.463\n",
            "==Train Epoch:\t\t\t 2004 \tLoss: 0.009979\t lambda: 1.287e-04\t lr: 3.262\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.028\n",
            "==Train Full Grad Epoch:\t 2007 \tLoss: 0.010027\t lambda: 1.290e-04\t lr: 2.491\n",
            "==Train Epoch:\t\t\t 2010 \tLoss: 0.009974\t lambda: 1.292e-04\t lr: 3.721\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.028\n",
            "==Train Full Grad Epoch:\t 2013 \tLoss: 0.010022\t lambda: 1.294e-04\t lr: 2.493\n",
            "==Train Epoch:\t\t\t 2016 \tLoss: 0.010001\t lambda: 1.298e-04\t lr: 3.673\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.028\n",
            "==Train Epoch:\t\t\t 2019 \tLoss: 0.009983\t lambda: 1.302e-04\t lr: 3.468\n",
            "==Train Epoch:\t\t\t 2022 \tLoss: 0.009963\t lambda: 1.306e-04\t lr: 3.563\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.028\n",
            "==Train Full Grad Epoch:\t 2025 \tLoss: 0.010022\t lambda: 1.309e-04\t lr: 2.497\n",
            "==Train Epoch:\t\t\t 2028 \tLoss: 0.010008\t lambda: 1.312e-04\t lr: 3.479\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.027\n",
            "==Train Epoch:\t\t\t 2031 \tLoss: 0.009947\t lambda: 1.316e-04\t lr: 3.650\n",
            "==Train Epoch:\t\t\t 2034 \tLoss: 0.010004\t lambda: 1.318e-04\t lr: 3.683\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.027\n",
            "==Train Epoch:\t\t\t 2037 \tLoss: 0.010016\t lambda: 1.322e-04\t lr: 3.504\n",
            "==Train Full Grad Epoch:\t 2040 \tLoss: 0.010024\t lambda: 1.325e-04\t lr: 2.500\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.027\n",
            "==Train Epoch:\t\t\t 2043 \tLoss: 0.009970\t lambda: 1.330e-04\t lr: 3.726\n",
            "==Train Full Grad Epoch:\t 2046 \tLoss: 0.010015\t lambda: 1.332e-04\t lr: 2.501\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.027\n",
            "==Train Epoch:\t\t\t 2049 \tLoss: 0.009890\t lambda: 1.336e-04\t lr: 3.325\n",
            "\n",
            "Test set: Average loss: 0.0001\n",
            "\n",
            "==Train Epoch:\t\t\t 2052 \tLoss: 0.009938\t lambda: 1.339e-04\t lr: 3.363\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.027\n",
            "==Train Epoch:\t\t\t 2055 \tLoss: 0.010008\t lambda: 1.344e-04\t lr: 3.584\n",
            "==Train Epoch:\t\t\t 2058 \tLoss: 0.009937\t lambda: 1.349e-04\t lr: 3.500\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.027\n",
            "==Train Full Grad Epoch:\t 2061 \tLoss: 0.010019\t lambda: 1.351e-04\t lr: 2.506\n",
            "==Train Full Grad Epoch:\t 2064 \tLoss: 0.010015\t lambda: 1.352e-04\t lr: 2.505\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.026\n",
            "==Train Epoch:\t\t\t 2067 \tLoss: 0.009983\t lambda: 1.356e-04\t lr: 3.289\n",
            "==Train Epoch:\t\t\t 2070 \tLoss: 0.009999\t lambda: 1.360e-04\t lr: 3.652\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.026\n",
            "==Train Full Grad Epoch:\t 2073 \tLoss: 0.010009\t lambda: 1.362e-04\t lr: 2.508\n",
            "==Train Full Grad Epoch:\t 2076 \tLoss: 0.010026\t lambda: 1.365e-04\t lr: 2.508\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.026\n",
            "==Train Epoch:\t\t\t 2079 \tLoss: 0.010010\t lambda: 1.369e-04\t lr: 3.517\n",
            "==Train Epoch:\t\t\t 2082 \tLoss: 0.009976\t lambda: 1.373e-04\t lr: 3.697\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.026\n",
            "==Train Epoch:\t\t\t 2085 \tLoss: 0.009987\t lambda: 1.377e-04\t lr: 3.438\n",
            "==Train Epoch:\t\t\t 2088 \tLoss: 0.010021\t lambda: 1.382e-04\t lr: 3.511\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.026\n",
            "==Train Epoch:\t\t\t 2091 \tLoss: 0.009975\t lambda: 1.386e-04\t lr: 3.757\n",
            "==Train Full Grad Epoch:\t 2094 \tLoss: 0.010024\t lambda: 1.389e-04\t lr: 2.513\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.025\n",
            "==Train Epoch:\t\t\t 2097 \tLoss: 0.009953\t lambda: 1.393e-04\t lr: 3.446\n",
            "\n",
            "Test set: Average loss: 0.0001\n",
            "\n",
            "==Train Epoch:\t\t\t 2100 \tLoss: 0.009986\t lambda: 1.397e-04\t lr: 3.798\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.025\n",
            "==Train Epoch:\t\t\t 2103 \tLoss: 0.009967\t lambda: 1.401e-04\t lr: 3.430\n",
            "==Train Epoch:\t\t\t 2106 \tLoss: 0.009953\t lambda: 1.405e-04\t lr: 3.715\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.025\n",
            "==Train Epoch:\t\t\t 2109 \tLoss: 0.009984\t lambda: 1.408e-04\t lr: 3.535\n",
            "==Train Epoch:\t\t\t 2112 \tLoss: 0.009927\t lambda: 1.412e-04\t lr: 3.714\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.025\n",
            "==Train Epoch:\t\t\t 2115 \tLoss: 0.009965\t lambda: 1.419e-04\t lr: 3.624\n",
            "==Train Full Grad Epoch:\t 2118 \tLoss: 0.010022\t lambda: 1.424e-04\t lr: 2.519\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.024\n",
            "==Train Epoch:\t\t\t 2121 \tLoss: 0.010033\t lambda: 1.431e-04\t lr: 3.754\n",
            "==Train Epoch:\t\t\t 2124 \tLoss: 0.009998\t lambda: 1.438e-04\t lr: 3.740\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.025\n",
            "==Train Full Grad Epoch:\t 2127 \tLoss: 0.010027\t lambda: 1.445e-04\t lr: 2.523\n",
            "==Train Full Grad Epoch:\t 2130 \tLoss: 0.010020\t lambda: 1.447e-04\t lr: 2.523\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.024\n",
            "==Train Epoch:\t\t\t 2133 \tLoss: 0.009938\t lambda: 1.457e-04\t lr: 3.709\n",
            "==Train Epoch:\t\t\t 2136 \tLoss: 0.010051\t lambda: 1.461e-04\t lr: 3.541\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.024\n",
            "==Train Epoch:\t\t\t 2139 \tLoss: 0.010022\t lambda: 1.471e-04\t lr: 3.921\n",
            "==Train Full Grad Epoch:\t 2142 \tLoss: 0.010027\t lambda: 1.478e-04\t lr: 2.527\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.024\n",
            "==Train Epoch:\t\t\t 2145 \tLoss: 0.010000\t lambda: 1.483e-04\t lr: 3.599\n",
            "==Train Epoch:\t\t\t 2148 \tLoss: 0.009983\t lambda: 1.492e-04\t lr: 3.671\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.024\n",
            "\n",
            "Test set: Average loss: 0.0001\n",
            "\n",
            "==Train Full Grad Epoch:\t 2151 \tLoss: 0.010030\t lambda: 1.499e-04\t lr: 2.530\n",
            "==Train Epoch:\t\t\t 2154 \tLoss: 0.010005\t lambda: 1.506e-04\t lr: 3.675\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.024\n",
            "==Train Epoch:\t\t\t 2157 \tLoss: 0.009952\t lambda: 1.511e-04\t lr: 3.302\n",
            "==Train Epoch:\t\t\t 2160 \tLoss: 0.009954\t lambda: 1.521e-04\t lr: 3.617\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.023\n",
            "==Train Epoch:\t\t\t 2163 \tLoss: 0.010059\t lambda: 1.530e-04\t lr: 3.932\n",
            "==Train Epoch:\t\t\t 2166 \tLoss: 0.009965\t lambda: 1.537e-04\t lr: 3.628\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.023\n",
            "==Train Epoch:\t\t\t 2169 \tLoss: 0.009962\t lambda: 1.547e-04\t lr: 3.649\n",
            "==Train Epoch:\t\t\t 2172 \tLoss: 0.010003\t lambda: 1.554e-04\t lr: 3.501\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.023\n",
            "==Train Epoch:\t\t\t 2175 \tLoss: 0.010030\t lambda: 1.563e-04\t lr: 3.208\n",
            "==Train Epoch:\t\t\t 2178 \tLoss: 0.010020\t lambda: 1.571e-04\t lr: 3.829\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.022\n",
            "==Train Epoch:\t\t\t 2181 \tLoss: 0.009964\t lambda: 1.580e-04\t lr: 3.644\n",
            "==Train Full Grad Epoch:\t 2184 \tLoss: 0.010026\t lambda: 1.587e-04\t lr: 2.541\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.022\n",
            "==Train Epoch:\t\t\t 2187 \tLoss: 0.009976\t lambda: 1.597e-04\t lr: 3.816\n",
            "==Train Epoch:\t\t\t 2190 \tLoss: 0.009994\t lambda: 1.606e-04\t lr: 3.582\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.022\n",
            "==Train Epoch:\t\t\t 2193 \tLoss: 0.010001\t lambda: 1.611e-04\t lr: 3.591\n",
            "==Train Epoch:\t\t\t 2196 \tLoss: 0.010056\t lambda: 1.616e-04\t lr: 3.581\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.022\n",
            "==Train Epoch:\t\t\t 2199 \tLoss: 0.010004\t lambda: 1.625e-04\t lr: 3.671\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Epoch:\t\t\t 2202 \tLoss: 0.009981\t lambda: 1.632e-04\t lr: 3.510\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.021\n",
            "==Train Epoch:\t\t\t 2205 \tLoss: 0.009974\t lambda: 1.642e-04\t lr: 3.513\n",
            "==Train Epoch:\t\t\t 2208 \tLoss: 0.010042\t lambda: 1.651e-04\t lr: 3.591\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.021\n",
            "==Train Epoch:\t\t\t 2211 \tLoss: 0.009984\t lambda: 1.658e-04\t lr: 3.740\n",
            "==Train Epoch:\t\t\t 2214 \tLoss: 0.009975\t lambda: 1.666e-04\t lr: 3.333\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.021\n",
            "==Train Epoch:\t\t\t 2217 \tLoss: 0.010032\t lambda: 1.675e-04\t lr: 3.523\n",
            "==Train Epoch:\t\t\t 2220 \tLoss: 0.010044\t lambda: 1.685e-04\t lr: 3.631\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.020\n",
            "==Train Full Grad Epoch:\t 2223 \tLoss: 0.010022\t lambda: 1.689e-04\t lr: 2.557\n",
            "==Train Epoch:\t\t\t 2226 \tLoss: 0.010003\t lambda: 1.696e-04\t lr: 3.466\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.020\n",
            "==Train Epoch:\t\t\t 2229 \tLoss: 0.010001\t lambda: 1.706e-04\t lr: 3.710\n",
            "==Train Full Grad Epoch:\t 2232 \tLoss: 0.010025\t lambda: 1.713e-04\t lr: 2.561\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.019\n",
            "==Train Full Grad Epoch:\t 2235 \tLoss: 0.010033\t lambda: 1.720e-04\t lr: 2.562\n",
            "==Train Epoch:\t\t\t 2238 \tLoss: 0.010007\t lambda: 1.730e-04\t lr: 3.658\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.019\n",
            "==Train Epoch:\t\t\t 2241 \tLoss: 0.010058\t lambda: 1.734e-04\t lr: 3.559\n",
            "==Train Epoch:\t\t\t 2244 \tLoss: 0.010003\t lambda: 1.744e-04\t lr: 3.772\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.019\n",
            "==Train Epoch:\t\t\t 2247 \tLoss: 0.010003\t lambda: 1.751e-04\t lr: 3.611\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Full Grad Epoch:\t 2250 \tLoss: 0.010027\t lambda: 1.756e-04\t lr: 2.567\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.018\n",
            "==Train Full Grad Epoch:\t 2253 \tLoss: 0.010026\t lambda: 1.763e-04\t lr: 2.568\n",
            "==Train Epoch:\t\t\t 2256 \tLoss: 0.009951\t lambda: 1.770e-04\t lr: 3.608\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.018\n",
            "==Train Epoch:\t\t\t 2259 \tLoss: 0.009971\t lambda: 1.780e-04\t lr: 3.620\n",
            "==Train Epoch:\t\t\t 2262 \tLoss: 0.009985\t lambda: 1.789e-04\t lr: 3.887\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.018\n",
            "==Train Full Grad Epoch:\t 2265 \tLoss: 0.010033\t lambda: 1.796e-04\t lr: 2.576\n",
            "==Train Full Grad Epoch:\t 2268 \tLoss: 0.010031\t lambda: 1.803e-04\t lr: 2.575\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.017\n",
            "==Train Epoch:\t\t\t 2271 \tLoss: 0.009984\t lambda: 1.811e-04\t lr: 3.568\n",
            "==Train Epoch:\t\t\t 2274 \tLoss: 0.010034\t lambda: 1.818e-04\t lr: 3.492\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.017\n",
            "==Train Epoch:\t\t\t 2277 \tLoss: 0.010026\t lambda: 1.827e-04\t lr: 3.581\n",
            "==Train Full Grad Epoch:\t 2280 \tLoss: 0.010024\t lambda: 1.832e-04\t lr: 2.582\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.017\n",
            "==Train Full Grad Epoch:\t 2283 \tLoss: 0.010027\t lambda: 1.837e-04\t lr: 2.582\n",
            "==Train Epoch:\t\t\t 2286 \tLoss: 0.009983\t lambda: 1.846e-04\t lr: 3.641\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.017\n",
            "==Train Epoch:\t\t\t 2289 \tLoss: 0.009987\t lambda: 1.856e-04\t lr: 3.538\n",
            "==Train Full Grad Epoch:\t 2292 \tLoss: 0.010025\t lambda: 1.863e-04\t lr: 2.586\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.016\n",
            "==Train Epoch:\t\t\t 2295 \tLoss: 0.009952\t lambda: 1.872e-04\t lr: 3.880\n",
            "==Train Full Grad Epoch:\t 2298 \tLoss: 0.010033\t lambda: 1.877e-04\t lr: 2.589\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.015\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Epoch:\t\t\t 2301 \tLoss: 0.009977\t lambda: 1.884e-04\t lr: 3.620\n",
            "==Train Full Grad Epoch:\t 2304 \tLoss: 0.010031\t lambda: 1.891e-04\t lr: 2.592\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.015\n",
            "==Train Full Grad Epoch:\t 2307 \tLoss: 0.010031\t lambda: 1.894e-04\t lr: 2.591\n",
            "==Train Epoch:\t\t\t 2310 \tLoss: 0.009975\t lambda: 1.899e-04\t lr: 3.825\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.015\n",
            "==Train Epoch:\t\t\t 2313 \tLoss: 0.010045\t lambda: 1.908e-04\t lr: 3.817\n",
            "==Train Full Grad Epoch:\t 2316 \tLoss: 0.010029\t lambda: 1.915e-04\t lr: 2.596\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.014\n",
            "==Train Epoch:\t\t\t 2319 \tLoss: 0.010048\t lambda: 1.922e-04\t lr: 3.548\n",
            "==Train Full Grad Epoch:\t 2322 \tLoss: 0.010030\t lambda: 1.930e-04\t lr: 2.599\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.014\n",
            "==Train Epoch:\t\t\t 2325 \tLoss: 0.009996\t lambda: 1.937e-04\t lr: 3.513\n",
            "==Train Epoch:\t\t\t 2328 \tLoss: 0.010038\t lambda: 1.946e-04\t lr: 3.418\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.014\n",
            "==Train Epoch:\t\t\t 2331 \tLoss: 0.009990\t lambda: 1.951e-04\t lr: 3.499\n",
            "==Train Epoch:\t\t\t 2334 \tLoss: 0.010024\t lambda: 1.961e-04\t lr: 3.741\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.013\n",
            "==Train Epoch:\t\t\t 2337 \tLoss: 0.010012\t lambda: 1.968e-04\t lr: 3.637\n",
            "==Train Epoch:\t\t\t 2340 \tLoss: 0.010033\t lambda: 1.975e-04\t lr: 3.488\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.013\n",
            "==Train Full Grad Epoch:\t 2343 \tLoss: 0.010034\t lambda: 1.982e-04\t lr: 2.610\n",
            "==Train Epoch:\t\t\t 2346 \tLoss: 0.010002\t lambda: 1.987e-04\t lr: 3.690\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.013\n",
            "==Train Full Grad Epoch:\t 2349 \tLoss: 0.010029\t lambda: 1.992e-04\t lr: 2.611\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Epoch:\t\t\t 2352 \tLoss: 0.010022\t lambda: 2.001e-04\t lr: 3.638\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.012\n",
            "==Train Epoch:\t\t\t 2355 \tLoss: 0.010002\t lambda: 2.015e-04\t lr: 3.573\n",
            "==Train Full Grad Epoch:\t 2358 \tLoss: 0.010033\t lambda: 2.025e-04\t lr: 2.613\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.012\n",
            "==Train Full Grad Epoch:\t 2361 \tLoss: 0.010032\t lambda: 2.034e-04\t lr: 2.616\n",
            "==Train Full Grad Epoch:\t 2364 \tLoss: 0.010031\t lambda: 2.049e-04\t lr: 2.617\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.011\n",
            "==Train Epoch:\t\t\t 2367 \tLoss: 0.010018\t lambda: 2.063e-04\t lr: 3.503\n",
            "==Train Full Grad Epoch:\t 2370 \tLoss: 0.010032\t lambda: 2.073e-04\t lr: 2.619\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.011\n",
            "==Train Epoch:\t\t\t 2373 \tLoss: 0.010014\t lambda: 2.092e-04\t lr: 3.524\n",
            "==Train Epoch:\t\t\t 2376 \tLoss: 0.010039\t lambda: 2.106e-04\t lr: 3.995\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.010\n",
            "==Train Full Grad Epoch:\t 2379 \tLoss: 0.010037\t lambda: 2.111e-04\t lr: 2.623\n",
            "==Train Epoch:\t\t\t 2382 \tLoss: 0.010017\t lambda: 2.125e-04\t lr: 3.817\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.010\n",
            "==Train Epoch:\t\t\t 2385 \tLoss: 0.010039\t lambda: 2.140e-04\t lr: 3.873\n",
            "==Train Epoch:\t\t\t 2388 \tLoss: 0.010009\t lambda: 2.154e-04\t lr: 4.071\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.009\n",
            "==Train Epoch:\t\t\t 2391 \tLoss: 0.010043\t lambda: 2.163e-04\t lr: 3.781\n",
            "==Train Epoch:\t\t\t 2394 \tLoss: 0.010035\t lambda: 2.183e-04\t lr: 3.315\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.009\n",
            "==Train Epoch:\t\t\t 2397 \tLoss: 0.010023\t lambda: 2.197e-04\t lr: 3.463\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Full Grad Epoch:\t 2400 \tLoss: 0.010039\t lambda: 2.211e-04\t lr: 2.634\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.008\n",
            "==Train Epoch:\t\t\t 2403 \tLoss: 0.009989\t lambda: 2.226e-04\t lr: 3.706\n",
            "==Train Epoch:\t\t\t 2406 \tLoss: 0.010040\t lambda: 2.235e-04\t lr: 3.883\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.008\n",
            "==Train Full Grad Epoch:\t 2409 \tLoss: 0.010030\t lambda: 2.249e-04\t lr: 2.640\n",
            "==Train Epoch:\t\t\t 2412 \tLoss: 0.010071\t lambda: 2.264e-04\t lr: 3.713\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.007\n",
            "==Train Epoch:\t\t\t 2415 \tLoss: 0.010040\t lambda: 2.283e-04\t lr: 3.829\n",
            "==Train Epoch:\t\t\t 2418 \tLoss: 0.010004\t lambda: 2.302e-04\t lr: 3.564\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.006\n",
            "==Train Epoch:\t\t\t 2421 \tLoss: 0.009969\t lambda: 2.321e-04\t lr: 3.702\n",
            "==Train Epoch:\t\t\t 2424 \tLoss: 0.010038\t lambda: 2.336e-04\t lr: 3.598\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.006\n",
            "==Train Full Grad Epoch:\t 2427 \tLoss: 0.010037\t lambda: 2.355e-04\t lr: 2.651\n",
            "==Train Epoch:\t\t\t 2430 \tLoss: 0.010018\t lambda: 2.393e-04\t lr: 4.046\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.005\n",
            "==Train Full Grad Epoch:\t 2433 \tLoss: 0.010038\t lambda: 2.422e-04\t lr: 2.656\n",
            "==Train Epoch:\t\t\t 2436 \tLoss: 0.010065\t lambda: 2.450e-04\t lr: 3.633\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.004\n",
            "==Train Epoch:\t\t\t 2439 \tLoss: 0.010078\t lambda: 2.479e-04\t lr: 3.713\n",
            "==Train Epoch:\t\t\t 2442 \tLoss: 0.010047\t lambda: 2.508e-04\t lr: 3.746\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.003\n",
            "==Train Epoch:\t\t\t 2445 \tLoss: 0.010038\t lambda: 2.546e-04\t lr: 3.158\n",
            "==Train Full Grad Epoch:\t 2448 \tLoss: 0.010048\t lambda: 2.565e-04\t lr: 2.664\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.002\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Epoch:\t\t\t 2451 \tLoss: 0.009997\t lambda: 2.642e-04\t lr: 3.813\n",
            "==Train Epoch:\t\t\t 2454 \tLoss: 0.010031\t lambda: 2.719e-04\t lr: 3.689\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.001\n",
            "==Train Epoch:\t\t\t 2457 \tLoss: 0.010080\t lambda: 2.872e-04\t lr: 3.751\n",
            "==Train Epoch:\t\t\t 2460 \tLoss: 0.010024\t lambda: 2.988e-04\t lr: 3.451\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.000\n",
            "==Train Epoch:\t\t\t 2463 \tLoss: 0.010026\t lambda: 3.295e-04\t lr: 3.560\n",
            "==Train Epoch:\t\t\t 2466 \tLoss: 0.010008\t lambda: 3.602e-04\t lr: 3.824\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.000\n",
            "==Train Full Grad Epoch:\t 2469 \tLoss: 0.010029\t lambda: 3.909e-04\t lr: 2.678\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2yLD7u4SRiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch = next(iter(train_loader))[0]\n",
        "batch = batch.cuda()\n",
        "batch_Y = model.Y(batch[:,0]).float().to(dev)\n",
        "batch_i = batch[:,1]\n",
        "unique_i, inverse_i, count_i = torch.unique(batch_i, return_inverse=True, return_counts=True)\n",
        "I1hot =I_b[inverse_i].to(dev)\n",
        "W = torch.einsum('js,jt,ju->stu', batch_Y.float() , batch_Y.float(), I1hot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DelYfOhFQuzC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34314153-ad91-4c38-ed81-6915a9a8d5b0"
      },
      "source": [
        "n_array = torch.zeros(n).float().to(dev)\n",
        "n_array.index_add_(0,batch_i,(batch_Y**2).sum(1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([54., 50., 51., 52., 45., 43., 58., 38., 42., 64., 49., 42., 50., 46.,\n",
              "        35., 61., 50., 40., 46., 46., 57., 62., 51., 52., 51., 47., 61., 54.,\n",
              "        69., 45., 48., 49., 51., 39., 47., 63., 54., 50., 52., 45., 47., 48.,\n",
              "        56., 48., 49., 57., 49., 43., 55., 48., 49., 57., 55., 55., 34., 58.,\n",
              "        53., 45., 64., 50., 44., 38., 53., 54., 53., 47., 59., 45., 45., 38.,\n",
              "        43., 43., 32., 48., 50., 52., 43., 49., 59., 64., 48., 54., 58., 54.,\n",
              "        46., 67., 56., 53., 51., 61., 46., 61., 50., 56., 52., 62., 41., 56.,\n",
              "        48., 51., 51., 49., 50., 46., 56., 52., 45., 61., 37., 49., 39., 53.,\n",
              "        48., 53., 51., 47., 54., 56., 60., 46., 47., 49., 48., 48., 52., 43.,\n",
              "        42., 45., 49., 45., 46., 50., 47., 42., 50., 53., 57., 48., 53., 51.,\n",
              "        44., 43., 52., 42., 37., 36., 50., 64., 43., 30., 43., 39., 61., 46.,\n",
              "        45., 43., 41., 49., 46., 56., 47., 42., 41., 62., 43., 56., 55., 51.,\n",
              "        46., 43., 48., 44., 48., 38., 51., 48., 46., 62., 51., 54., 47., 54.,\n",
              "        57., 59., 52., 53., 65., 46., 48., 52., 45., 51., 43., 46., 39., 60.,\n",
              "        55., 48., 48., 37.], device='cuda:0', grad_fn=<IndexAddBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAgZFx7sRG0j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "919b6ca3-54af-4f56-d46b-79e1e9ee69be"
      },
      "source": [
        "n_array.max().item()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "69.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QMv5TG_RPvO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fe77181-136b-4889-af76-fee5767da33a"
      },
      "source": [
        "batch[:,1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([39, 38, 62,  ..., 67, 95, 32], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPUYNQ_3fzGA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9908e5fb-2fc4-486c-e832-d4e9e1825412"
      },
      "source": [
        "Y[0:10,:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa8WVQpKeqQC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07beaf08-d635-4eea-a128-88a71663131b"
      },
      "source": [
        "model.Y.weight[0:10,:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 1.],\n",
              "        [0., 0., 1.],\n",
              "        [0., 0., 1.],\n",
              "        [1., 0., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [0., 1., 0.],\n",
              "        [0., 1., 0.],\n",
              "        [0., 1., 0.],\n",
              "        [0., 0., 0.]], device='cuda:0', dtype=torch.float64,\n",
              "       grad_fn=<SliceBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06HoX-V6l5zP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acc04acf-d18e-4768-a28f-349fb2279445"
      },
      "source": [
        "model.Y.weight.sum(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([50., 59., 54.], device='cuda:0', dtype=torch.float64,\n",
              "       grad_fn=<SumBackward1>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKc2Ic9Il-WN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fef8f8bb-511b-433d-f7d7-8e4b4119d7c8"
      },
      "source": [
        "Y.sum(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([54., 50., 59.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Y_IRtgZkW88",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c104c189-5c57-4133-f784-3555d3b6c30f"
      },
      "source": [
        "C"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4.21, 4.23, 0.5 ],\n",
              "       [1.01, 3.82, 2.7 ],\n",
              "       [4.78, 1.05, 2.86]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHFNjYCugO2r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bab93533-5b89-4821-8923-0af438c3c5f3"
      },
      "source": [
        "model.C.weight"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[1.0055, 3.8218, 2.7007],\n",
              "        [4.7808, 1.0474, 2.8617],\n",
              "        [4.2134, 4.2298, 0.4993]], device='cuda:0', dtype=torch.float64,\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRoY9NhakfAl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2eac1405-eddc-4578-cc33-11b8dcca9fa8"
      },
      "source": [
        "X[0:10,:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYs9hu03kZyp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "dbe24616-f301-4549-a298-77a8f2864605"
      },
      "source": [
        "model.X.fact.weight[0:10,:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleAttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-65ca680e71ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    770\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 772\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleAttributeError\u001b[0m: 'Embedding' object has no attribute 'fact'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5HuTuumXbir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optParam = optimizerX.param_groups[0]\n",
        "print((optParam['params'][0].grad !=0).sum())\n",
        "gradX = optParam['params'][0].grad\n",
        "batch_i = gradX.sum(1) != 0\n",
        "gradX[batch_i,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InSGk0-Xoihy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch = next(iter(train_loader))[0]\n",
        "#batch = batch.to(dev)\n",
        "batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfuQ14tDSAiZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(np.unique(batch[:,1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94Ig7_hMWR2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.round(torch.matmul(torch.matmul(model.Y.fact.weight,model.C.fact.weight),torch.transpose(model.X.fact.weight,0,1))*100)/100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huV92OMHsqKP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y@C@X.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba9mJzqb7Mm_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "D"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
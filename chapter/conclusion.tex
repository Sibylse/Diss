\chapter{Conclusions}\label{chap:Conclusions}
Making decisions is hard -- however, with respect to machine learning, we have seen that there are some tools to reach suitable binary solutions by means of theoretically sound and empirically robust methods. With this work, we contribute to the theory of making hard decisions in the scope of clustering. Our overview of state-of-the-art clustering methods shows that there is a variety of clustering tasks equivalent to matrix factorization problems involving binary constraints on one or two of the factor matrices. Summarizing clustering problems in the framework of matrix factorization has proven useful for deriving characteristics of solutions and comparing approaches. For instance, we lay out in which sense the famous $k$-means clustering task corresponds to learning orthogonal projections, eigenvectors, and nonnegative factorizations in Theorem~\ref{thm:kmeansobj}.  

The overview of clustering tasks as matrix factorization has additionally proven useful to provide deeper insights into the state of the art. As such, we lay out how the seemingly arbitrary step of discretization via $k$-means in spectral clustering can actually be made theoretically justified. Here, the formalization of the minimum cut objective as matrix factorization displays its relation to kernel $k$-means, yielding the equivalence of minimum cut and $k$-means clustering, given a symmetric decomposition of the data matrix such as the eigendecomposition. Based on this observation, we have developed the algorithm \textsc{SpectACl}, tackling two of the main issues in spectral clustering: robustness to noise and interpretability of the embedding. Looking at visualizations of the  embedding used in \textsc{SpectACl} (cf.\@ Figure~\ref{fig:binaryEmb}), we see how every projected eigenvector corresponds to one cluster component of a specific density. The exploration of density-based clustering in the framework of spectral clustering furthermore builds a bridge to the other main approach in nonconvex clustering, which is represented by the DBSCAN algorithm. In conclusion, the algorithm \textsc{SpectACl} and its normalized version is an efficient, robust, and versatile tool to derive nonconvex clusters.

Our comparison of popular clustering objectives shows that the majority of methods is designed to find partitioning clusters. Suitable adaptations of Lloyd's algorithm (cf.\@ Section~\ref{sec:ZS:Lloyds}) guarantee the convergence to a local optimum of the objective function subject to the partitioning and particularly binary constraints. This offers an undeniable advantage over other practical alternatives, requiring a relaxation of the binary constraint. The major drawback of relaxing approaches is the discretization step, in which theoretical guarantees, which might be provided for solutions of the relaxed objective, are usually lost. However, the partitioning constraint, enabling the alternating minimization according to Lloyd, is not feasible in some applications. Overlapping and nonexhaustive clusterings are more likely to represent the \emph{true} model when it comes among other things to the clustering of text or genomic data. In this case, the theoretical foundation regarding the efficient optimization of corresponding objectives is leaky.

In this work, we mainly focus on the optimization of Boolean matrix factorizations, which is explicitly designed to model overlapping clusters of binary data. The optimization of this clustering objective is provisional, since so far only greedy heuristics prevail. This results in a strong preference bias towards factorizations where the first tile covers the largest chunk of the data, and consecutive tiles cover less and less. By contrast, we propose the simultaneous optimization of all clusters via an efficient proximal gradient procedure, based on a nonnegative relaxation of the binary values. We specifically incorporate a penalty term to regulate the convergence towards approximately binary values. This way, we still can not guarantee that the discretized solutions are actual local minima of the objective function, but we can guarantee that the approximately binary result is a local minimum of the relaxed objective. The approach to add a penalty term for nonbinary values is not new (cf.\@ Section~\ref{sec:ZS:Penalty}). In particular, an optimization scheme exists that incorporates nonbinary penalization for binary matrix factorization, aiming once again for nonoverlapping clusters. However, this existing optimization scheme is based on multiplicative updates, which are slow in practice due to a very conservative choice of the stepsize. 
We propose the optimization of Boolean matrix factorization via PALM, a universal approach which is applicable to solving constrained matrix factorizations. The constraints are thereby required to be convertible to a simple but possibly nonsmooth regularization term whose proximal mapping is efficient to compute. We have derived a closed form for the proximal mapping of a nonbinary penalization term (cf.\@ Section~\ref{sec:PT:binprox}). Hence, we contribute to the theory of proximal optimization, extending its applications to problems involving binary or more general integer constraints.

The task of Boolean matrix factorization is moreover interesting due to its established methodology to estimate the rank of the factorization. In particular, the Minimum Description Length (MDL) principle is used for that purpose. Originating from the selection of interesting patterns in pattern mining, the MDL principle has been transferred to the more general notion of Boolean matrix factorization. In this work, we explore two directions in order to automatically select the rank of Boolean factorizations: the one is to minimize the description length via a suitable relaxation as a smooth \KL function, and the other is based on controlling the false discovery rate. We propose two instances of the first direction with the algorithms \textsc{Primp} and \textsc{Panpal}. Our experimental evaluation shows that the method \textsc{Primp}, employing a description length based on a compression by code tables, delivers correct rank estimations and is able to filter the haphazard from the formative structure. \textsc{Primp}, on the other hand, tends to overestimate the rank of the clustering under some circumstances. For such cases, we propose the estimation of the probability that particular tiles are generated by Bernoulli distributed noise (cf.\@ Section~\ref{sec:TP:rejectNullHyp}). We furthermore propose to incorporate the model selection by false discovery control into the optimization scheme, with the algorithm \textsc{TrustPal}. In the scope of controlling the false discovery rate in Boolean factorizations, we contribute towards the aspect of explainability. We derive characteristics of local minima of the Boolean factorization's residual sum of squares, and show that every item selected in a Boolean bi-cluster occurs in at least half of the data points assigned to that cluster; hence, giving first explanations of what makes a tile a tile.

Last but not least, we have explored the possibility to derive descriptions of clusters with regard to predefined classes. Identifying similarly expressed features together with class-discriminating features in one cluster helps the practitioner to explore a dataset in a supervised setting. A motivating example is the analysis of genomic mutations, and the development of genomic variations from normal to tumor cells and a possible relapse. The method \textsc{C-Salt} extends the tradition model of matrix factorization with an additional matrix, reflecting class-specific feature expressions for each cluster. Therewith, we find patterns in a database which cannot be derived with state-of-the-art methods. Due to the near-orthogonality of matrix factorizations (cf.\@ Section~\ref{sec:ZS:NMFClus}), deviations from the patterns which identify cluster membership are hardly recovered by general matrix factorization approaches. The model assumption of \textsc{C-Salt} is a fundamental change in the core of the matrix factorization scheme. It takes into account that class-defining characteristics may not hold over all instances. Instead, groups are identified together with their class-indicating features. With regard to our motivating example, the model assumption of \textsc{C-Salt} takes into account that there may not be one therapy for all, but multiple treatments, each one tailored to features of people belonging to one cluster.
%--------------------------------------
% Impact and Future Directions
%--------------------------------------
\section{Impact and Future Directions}
We have discussed and evaluated in which ways deriving Boolean matrix factorizations by advanced numerical optimization methods is preferable over greedy optimization. This poses only the beginning of a potentially versatile theory of numerical optimization for binary learning tasks. The theoretical progress in the field of nonconvex optimization provides new insights, which are now transferable to Boolean factorization (and perhaps beyond Boolean). Meanwhile, stochastic~\citep{davis2016sound}, accelerated~\citep{li2015accelerated}, and inertial~\citep{pock2016inertial} variants exist of the PALM optimization scheme. The exploration of stochastic gradient descent for matrix factorization is particularly promising with regard to its generalizing properties~\citep{hardt2016train,hoffer2017train}. Likewise, inertial methods are interesting since they can find \emph{good} local optima, even if the objective functions' landscape is rough~\citep{goh2017why}. Inertial methods integrate the direction of the previous gradient descent update into the current step, which is sometimes referred to as giving gradient descent a short-time memory. This is also the foundation of accelerating methods, which can improve the convergence rate. Considering that the framework \textsc{PAL-Tiling} is basically applicable to all discussed matrix factorizations from Chapter~\ref{chap:ZeroShades} (discarding the partitioning requirement), \textsc{PAL-Tiling} and possible extensions potentially provide a general optimization scheme for hard clusterings.   

If the matrix product is not approximated in another algebra (as in Boolean matrix factorization), then we can adapt the optimization scheme of \textsc{PAL-Tiling} such that the thresholding step at the end is no longer necessary. For example, consider the optimization of the objective of $k$-means by \textsc{PAL-Tiling}. One could derive binary solutions via \textsc{PAL-Tiling}, by increasing the weight of the nonbinary penalization such that only binary values in the cluster assignment matrix are possible. Although this optimization scheme comes with theoretical convergence guarantees to a local optimum, the result will however not be suitable due to the vast amount of local minima in $k$-means clustering. On the contrary, the method will be stuck at a local optimum close to the initialization matrix. This issue can be addressed by providing better initializations. Those initializations may themselves be derived by the application of \textsc{PAL-Tiling}, employing a lower scale nonbinary penalty term. Hence, we could also gradually increase the nonbinary penalty term, starting with an unconstrained matrix factorization. This is a promising approach to derive matrix factorizations with binary constraints under convergence guarantees.  

With regard to our evaluations of automatic determinations of the factorization rank, extending clustering from binary to real-valued data would be of interest. In particular, the experiments of \textsc{SpectACl} display potential to integrate model selection techniques into this nonconvex clustering task. Looking at the visualization of the clustering of the three blobs dataset in Figure~\ref{fig:synthViz}, we see that \textsc{SpectACl} is able to identify a smaller and dense cluster on top of a larger cluster. Imagine that we compare \textsc{SpectACl}'s clustering of two clusters with the one shown, assuming three clusters. It is reasonable to assume that \textsc{SpectACl} returns the two point clouds at both edges just as DBSCAN if the number of clusters is set to two. If we follow the approach of hypothesis testing from \textsc{TrustPal}, then a bound on the probability that the smaller, denser cluster appears due to noise effects from a larger cluster would deliver a theoretically founded way of determining whether the rank of this three blobs dataset is equal to two or three.
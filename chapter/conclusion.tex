\chapter{Conclusions}\label{chap:Conclusions}
Making decisions is hard -- but in machine learning there are tools to establish methods which are able to reach for suitable binary solutions by means of theoretically sound and empirically robust methods. With this work, we contribute to the theory and methods of making decisions in the scope of clustering. Our overview of state-of-the-art clustering methods shows that most well-known tasks to find the best set of assignments corresponds to a matrix factorization involving binary constraints on one or two of the factor matrices. Summarizing clustering problems in the standard framework of matrix factorization has proven useful to derive characteristics of solutions and to compare the approaches. For instance, we lay out in what sense the famous $k$-means clustering corresponds to learning orthogonal projections, eigenvectors and nonnegative factorizations (cf.\@ Theorem~\ref{thm:kmeansobj}).  

We particularly show that the formalization of clustering in the standard framework of matrix factorization helps to understand and develop more robust approaches based on common best practice in the field. The spectral relaxation used to derive minimum cut models employs a $k$-means clustering on embedded data points, which is described as a standard but exchangeable discretization step. The formalization of the minimum cut objective in terms of matrix factorization, the derived relation to kernel $k$-means and therewith also $k$-means shows why this discretization step is actually theoretically justified. By employing this theoretical background, we have developed an algorithm solving two of the main issues in spectral clustering; namely robustness to noise and interpretability of the embedding. Looking at visualizations of the projected eigenvectors (cf.\@ Figure~\ref{fig:binaryEmb}), we see how every projected eigenvector corresponds to one component of a specified density. The exploration of density based clustering in the framework of spectral clustering has furthermore been motivated by the more general formulation of minimum cut as a matrix factorization problem. The result, the algorithm \textsc{SpectACl} and its normalized version, is an efficient, robust and versatile tool to derive nonconvex clusters.

The comparison of popular clustering objectives shows that the majority of methods is created to find partitioning clusters. In this case, the clustering is approximable by (an adaptation of) Lloyd's algorithm (cf.\@ Section~\ref{sec:ZS:Lloyds}) which guarantees the convergence to a local optimum of the objective. This offers an undeniable advantage over other practical alternatives which require a relaxation of the binary constraint. The relaxed solution is  required to be discretized in the end, a step in which theoretical guarantees, provided for solutions of the relaxed objective are usually lost. On the other hand, in some applications, overlapping and non-exhaustive clusterings are more likely to represent the \emph{true} model. In this case, little theory is provided for the efficient optimization of corresponding objectives.

In this work, we mainly focus on the optimization of Boolean matrix factorizations, which is explicitly designed to model overlap between clusters of binary data. Since established methods for clustering do not allow for overlap between the clusters, greedy heuristics have been suggested to optimize Boolean matrix factorizations. This results in a strong preference bias towards factorizations where the first tile covers the largest chunk of the data and consecutive tiles less and less. By contrast, we propose the simultaneous optimization of clusters via an efficient proximal gradient procedure, based on a nonnegative relaxation of the binary values. We incorporate a penalty term to regulate the convergence towards approximately binary values. This way, we still can not guarantee that the discretized solutions are actually local minima of the objective function, but we can guarantee that the approximately binary result is a local minimum of the relaxed objective. The approach to add a penalty term for nonbinary values is not new (cf.\@ Section~\ref{sec:ZS:Penalty}). In particular, there exists an optimization scheme to incorporate nonbinary penalization for binary matrix factorization, aiming once again for nonoverlapping clusters. However, the suggested optimization employs multiplicative updates, which are slow in practice due to a very conservative choice of the step-size. 
We propose the optimization of Boolean matrix factorization via PALM, a universal approach which is applicable to solve matrix factorizations under constraints which are convertible to a simple but possibly nonsmooth regularization term for which the proximal mapping is efficiently computable. We have derived a closed form for the proximal mapping of a nonbinary penalization term. In so doing we contribute to the theory of proximal optimization, extending possible applications to problems involving binary or more general integer constraints.

The task of Boolean matrix factorization is moreover interesting because there exist well-researched methods to estimate the rank of the factorization. In particular, the minimum description length principle is used for that purpose, extending its application for the selection of interesting patterns to the more general notion of Boolean matrix factorization. In this work, we propose two approaches to perform an automatic rank selection: one incorporates the minimization of description lengths having a suitable relaxation as a smooth \KL function and one based on the control of the false discovery rate. We have empirically shown that the proposed method \textsc{Primp} employing a description length based on the compression by code tables, delivers particularly correct rank estimations, filtering the haphazard from the formative structure. Yet while \textsc{Primp} tends under some circumstances to overestimate the rank of the clustering, the method \textsc{TrustPal}, incorporating model selection by the false discovery control, may be used to prune the set of derived clusters to those which are significant, given a specified noise assumption. Here, we particularly contribute towards explainability of Boolean matrix factorization. We show that local minima of the approximation error of a Boolean factorization inhibit certain characteristics, showing for instance with respect to medical applications that every feature which is identified to be decisive for a regarded cluster is exhibited by at least half of the patients which are assigned to that cluster. Furthermore, by the method of false discovery rate control provided with \textsc{TrustPal}, the practitioner has a mechanism to validate the significance of derived clusters.

With regard to medical applications, we have additionally explored the possibility to derive descriptions of clusters by identifying those features which are similarly expressed in one cluster together with those features which partition the observations in one cluster into a set of predefined classes. A motivating example is here the analysis of genomic mutations, occuring from a normal cell to a tumor cell and a relapse. The method \textsc{C-Salt} extends the factorization by two matrices with a third matrix, reflecting class-specific feature expressions of instances in one cluster. Therewith, we are able to find patterns in a database which can not be derived with state-of-the art methods, relying on the traditional factorization approach. Due to the near-orthogonality of matrix factorizations (cf.\@ Section~\ref{sec:ZS:NMFClus}), deviations from the patterns which identify cluster membership are hardly recovered by general matrix factorization approaches. The model assumption of \textsc{C-Salt} is entirely new and takes into account that class defining characteristics may not hold over all instances. Instead, groups are identified together with class indicating features belonging to one group. With regard to our motivating example, the model assumption of \textsc{C-Salt} takes into account that there may not be one therapy for them all but multiple treatments, each one tailored to features of people belonging to one cluster.
%--------------------------------------
% Impact and Future Directions
%--------------------------------------
\subsection{Impact and Future Directions}
Formalizing Boolean matrix factorization in the scope of state-of-the art optimization methods has one big advantage: the progress which is made with respect to nonconvex optimization is transferable to Boolean factorizations. Meanwhile there exist stochastic variants~\citep{davis2016sound}, accelerated methods~\citep{li2015accelerated} and inertial methods~\citep{pock2016inertial}.


\thispagestyle{plain}
\bigskip
\begin{center}
	\Large
    \textbf{Abstract}
\end{center}
One of the first and most fundamental tasks in machine learning is to group observations within a dataset. Given a notion of similarity, finding those instances which are outstandingly similar to each other has manifold applications. Recommender systems and topic analysis in text data are examples which are most intuitive to grasp. The interpretation of the groups, called \emph{clusters}, is facilitated if the assignment of samples is definite. Especially in high-dimensional data, denoting a degree to which an observation belongs to a specified cluster requires a subsequent processing of the model to filter the most important information. We argue that a good summary of the data provides hard decisions on the following question: how many groups are there, and which observations belong to which clusters? In this work, we contribute to the theoretical and practical background of clustering tasks, addressing one or both aspects of this question. Our overview of state-of-the-art clustering approaches details the challenges of our ambition to provide hard decisions. Based on this overview, we develop new methodologies for two branches of clustering: the one concerns the derivation of nonconvex clusters, known as \emph{spectral clustering}; the other addresses the identification of \emph{bi-clusters}, a set of samples together with similarity defining features, via Boolean matrix factorization. 
One of the main challenges in both considered settings is the robustness to noise. Assuming that the issue of robustness is controllable by means of theoretical insights, we have a closer look at those aspects of established clustering methods which lack a theoretical foundation. In the scope of nonconvex clustering on real-valued data, this addresses the typically performed discretization step of relaxed solutions. We propose an upper bound on the objective function, such that specifically discretized relaxed solutions are actually local optima of the upper bound. With this new method, \textsc{SpectACl}, we explore the optimization of two of the main nonconvex clustering paradigms, implementing the maximum density or the minimum cut approach. 
In the scope of Boolean factorizations of binary data, we propose a versatile framework for the general optimization of matrix factorizations subject to binary constraints. Especially in Boolean matrix factorization, the established optimization scheme is based on intuitive methods, implementing greedy heuristics such that quality guarantees of obtained solutions are hardly provided. In contrast, we propose to apply recent advances in nonconvex optimization theory for the optimization of a product in Boolean algebra. Therewith, we provide convergence guarantees to local optima of a relaxed objective, where the factor matrices are only required to be approximately binary. By means of this optimization scheme, called \textsc{PAL-Tiling}, we propose two approaches to determine the number of clusters automatically. The one is based on the established application of the minimum description length, an information-theoretic approach balancing the model complexity against the fit to the data. The other proposes a novel application of false discovery rate control in the scope of unsupervised learning. In addition to the more traditional clustering tasks, we discuss the clustering by Boolean matrix factorization in a slightly different context. Given information about pre-defined classes, we consider the task to identify not only those features which indicate similarity of instances, but also those having discriminating properties over the classes. Here, we extend the traditional matrix factorization with another factor matrix, modeling class-specific alterations within a cluster. 
We evaluate the ability of all proposed approaches to filter the structure from the noise under various settings in synthetic and real-world data. The empirical evaluations complement our theoretical considerations, showing how our proposed methods combine theoretical soundness with their practical application. 

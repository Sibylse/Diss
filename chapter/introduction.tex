% einleitung.tex
\chapter{Introduction}
% \begin{chapquote}{LL Cool J, \textit{I need a Beat}}
% Basic Patterns, grouped and changed\\
% Sequence frequent, seek and gain\\
% Break, break for narration
% \end{chapquote}
%\emph{Es gibt kein richtiges Leben im Falschen} -- with this famous dictum, Theodor Adorno emphasizes the importance of being able to differentiate right from wrong. Although the statement originally addresses the possibilities of \emph{right} human behavior under difficult (\emph{wrong}) circumstances, we can adapt some of Adornos' thoughts in the scope of learning theory. Difficult circumstances might reflect If we make the wrong assumptions about the underlying model, then we will not be able to derive something genuine. 
Making decisions is hard -- generally in life, but for learning systems as well. Finding the optimal (sequence of) choices leading to the best outcome offers in most cases far too many possibilities. One may think of simple examples such as computing the optimal move in a chess game, deciding over the winner in a sports bet or finding the optimal gift for a special someone. With respect to machine learning, these examples are prototype applications of binary decision making in the three main fields: chess for reinforcement learning, placing a bet for supervised learning and giving recommendations for unsupervised learning. Across these fields, the requirement to make binary yes or no decisions is handled differently. Famous applications of reinforcement learning exploit the possibility to let a computer run simulations of its own, e.g., play a game against itself~\citep{silver2017mastering}. In this variant, the vast discrete search space is traversed during idle times. A compressed model maintaining the gained knowledge is then used to swiftly make a decision during a game. The compressed model is for instance based on a classification model. Classification is an instance of supervised learning. In this branch of machine learning, determining if an object belongs to one class or the other is usually based on a relaxed model, e.g., reflecting the probabilities of possible outcomes. With regard to our example, a bet should be made on the side which is most likely to win.
While reinforcement learning basically relies on knowledge gained by self-made experience (finding out the best strategy in a game by playing against itself again and again), clustering relies on the experience gathered by others. For instance, given the shopping history of a huge collection of costumers, a clustering model can be used to recommend a gift which is liked by people having a similar shopping history such as a specific costumer. Of those three examples, we focus on the latter in this work.  

As the name suggests, what exactly we are aiming for is unclear in unsupervised learning. It comprises tasks where the goal is to explore and to find characteristics and recurring patterns, setting some instances apart from others. We denote a set of observations which belongs together in some sense as a cluster. A set of clusters, the clustering, is supposed to represent the key concepts found in the data. Concept learning is an umbrella term for tasks which involve finding a set of objects which can usefully be grouped together~\citep{morik1993knowledge}. Being developed during the 1990s, concept learning has been formalized with respect to what was a hot topic back then: inductive logic programming~\citep{muggleton1994inductive}. In this framework, a theory has been established around the what and how of hypothesis selection. What has been valid then has become even more crucial now and thus, we take up some of the most important requirements for the definition of suitable clusterings. 
%--------------------------------
\paragraph{completeness and non-reducibility} Given a set of derived hypotheses (a model), completeness requires that all necessary information provided by the data is derivable from the model. Non-reducibility states that there is no redundancy among these hypotheses, that there is no smaller set of hypotheses satisfying completeness. These requirements are similar to those defining the basis of a vector space. Given a set of data points, the subspace spanned by those data points has a set of vectors called the basis. Basis vectors are complete in the sense that every data point lies in the span of the basis vectors. Thus, we can reconstruct every data point as a linear combination of the basis vectors. A basis is furthermore non-reducible due to the linear independence of its vectors. The concept of linear independence implies that none of the basis vectors is a linear combination of the others and thus, every point in the spanned space is uniquely represented as a linear combination of basis vectors. 
We focus here on cluster models which approximately implement the constraints of completeness and non-reducibility.  Completeness is not a strong requirement for cluster algorithms because an aggregation of data to groups does not need to pertain all the information provided in the data. Errors in measurements or generally noisy information which are not relevant for the task at hand should be filtered out. Therefore, we aim at finding a lower dimensional subspace than the space spanned by all data points, maintaining the most characteristic traits of the data. This mechanism is incorporated by matrix factorizations, which we employ as a standard framework of reference. It approximates a given data matrix by the product of two matrices. One factor represents the basis vectors of the derived subspace and the other factor returns the coefficients of the linear combination, derived by a projection of the data points onto the derived subspace. In that sense, matrix factorization derives complete and non-reducible cluster models as it finds a set of basis vectors of a (low-dimensional) subspace, approximating the space spanned by all data points.
%We discuss here for a particular instance of clustering how this trade-off can be managed in an information-theoretic and a probabilistic way.  
%------------------
\paragraph{interpretability} 
It goes without saying that especially in explorative data mining, where the model is supposed to provide insight into the data, we need to be able to interpret the information encoded in the model. This addresses one aspect of interpretability and this is the reason why we require that assignments of points to clusters are binary. There are plausible reasons to consider fuzzy memberships or probabilistic cluster assignments as well, the most prominent example of such methods is probably the latent Dirichlet allocation~\citep{blei2003latent}. However, understanding the implications of such models typically requires a post-processing step and doesn't come naturally. As an example, how are we supposed to interpret a clustering stating that a single instance belongs with probability $0.4$ to cluster one and with probability $0.6$ to cluster two. Let us recall our gift recommendation example, what could we learn from such an assignment? Should we buy a gift from cluster one or cluster two? Would maybe none of the clusters provide suitable gifts or both? We argue that a more helpful cluster indication provides the user with clear information, recommending to buy a gift from one of the clusters, both or none.  
%------------------
\paragraph{explainability} 
The inductive logic programming algorithm MYCIN is one of the prototypes of explainable artificial intelligence methods~\citep{fagan1980computer}. Given a set of hand-coded rules, this algorithm lays out its reasoning together with the suggested diagnosis. 
The demand for the ability to provide explanations for particular results acquired by an algorithm, gained momentum since the European Union urged the disclosure of any automated decision-making, made on a solely algorithmic basis since 2018 (cf.\@ \cite{doshi2017towards} and references therein). Explainability is related to and often also referred to as interpretability. We explicitly distinguish between these terms here and refer with explainability to the transparency of the method, whereas interpretability concerns the way information is presented to the practitioner. In short, while interpretability refers to understanding what is returned by an algorithm, explainability refers to understanding why the result is derived. We distinguish two ways to address the explainability of clustering model. The one is to provide characteristics of (local) optima of the objective function. Those characteristics could be necessary conditions which are formulated in an understandable way, e.g., as a rule. The other concerns a reasoning for the performed model selection. Model selection refers here particularly to the derived number of clusters. Being able to explain under which circumstances the number of derived clusters is suitable, trust towards algorithmic based decisions can be established in an unsupervised setting where no direct feedback is provided.
%-------------------------
\paragraph{preference bias} Bias in inductive logic programming generally addresses the influence a specific learning method has on the result. Preference bias more specifically hints at the bias emerging from the way the search space is traversed. That is among other things a bias which occurs when a method focuses on some hypotheses more than others or when there are possibilities to prune some of the hypotheses.  
This is a relevant aspect for clustering algorithms since clustering objectives are by at large nonconvex and have multiple local optima. Therefore, the applied optimization procedure has a big impact on the result. This is a general issue in optimization and there is a relatively new field emerging around the question how a particular setting of learning parameters 
%such as the step size or number of iterations 
in gradient descent procedures influences the probability to end up in local optima of a certain kind~\citep{henning2012quasi,henning2015probabilistic}. Although these efforts pose only the beginning in creating a characterization of results generated by numerical optimization methods, it also points out a long-term advantage of algorithms which follow generic optimization methods. Insights in optimization theory are transferable to particular instances of the optimization methods, enabling the development of even more robust and suitable algorithms by incorporating newer findings, e.g., adding noise to the gradient\citep{jin2017escape,hennig2013fast}.

Another aspect of bias addresses the problem of approximation in hard clustering tasks. Most of the popular clustering problems are not only NP-hard but also NP-hard to approximate within a given factor. As a result, efficient methods which are supposed to approximate solutions of the given problem return for some example data sets solutions which diverge from actual optima of the objective (unless NP=P). Since a guaranteed approximation of the objective is under this assumption not possible, the question is how to suitably relax the objective. Theoretical soundness and insight into solutions of the relaxing methods are therefore relevant. Whenever possible, the search space of the relaxed objective should be suitably restricted such that the learner is pointed into directions where suitable optima are more likely to be found. 

An important factor determining the suitablilty of clustering models is the number of expected clusters. We need to find a trade-off between the desired low-dimensionality of the subspace and its approximation to the space spanned by the data. If the model summarizes the data too coarsely, then only few if any knowledge can be gained. Otherwise, if the model does not generalize enough, then we run the risk of reflecting structure when there is none. A common way around this problem is to let the user decide over the number of clusters. This stands however in contrast to the explorative setting, where the user often knows little about the key characteristics of the data, not to mention the number of clusters. Therefore, finding ways to automatically determine the number of clusters is important.

\paragraph{}The requirements of completeness, non-reducibility and interpretability determine the overall framework of this thesis: matrix factorizations with binary constraints. Within this framework, we propose clustering methods which are explainable and for which we discuss and study the circumstances influencing the preference bias.  Popular instances of matrix factorization encompass principal component analysis, singular value decomposition and eigendecomposition. We will see that a whole branch of clustering, probably the most widely known and used, has an objective related to either of the popular eigendecompositions subject to binary constraints. These objectives have furthermore applications in fields which are not directly related to clustering, such as hashing and combinatorial optimization problems~\citep{ding2008nonnegative,mukherjee2015nmf}. Not only in this respect, being able to make hard decisions in the standard framework of matrix factorizations and the standard method of optimization has applications far outside the scope of clustering. The first step in creating a mathematical theory of making hard decisions.
%==========================
% Roadmap
%==========================
\section{Roadmap}
\paragraph{Background} The background material is covered in chapters~\ref{chap:ZeroShades} and \ref{chap:Algorithms}. In Chapter~\ref{chap:ZeroShades}, we discuss the broad spectrum of clustering tasks where the approximation error is minimized in Frobenius norm subject to binary constraints. We discuss and unveil under which circumstances this optimization task defines the objectives of $k$-means, spectral clustering and various subspace clustering objectives. A unified formalism of these objectives makes similarities and differences between the tasks apparent, obtaining a taxonomy of hard clustering optimization problems.
In Chapter~\ref{chap:Algorithms}, we review existing optimization approaches, generic to objective characteristics. Considering what approaches exist for the optimization of discussed algorithms, we distinguish between the relaxations relying either on singular value decomposition, eigendecomposition ororthonormal nonnegative decompositions and optimizations relying on a greedy approach, alternating minimization and a nonbinary penalization.  For every of these approaches there exists one prototype clustering associated therewith. We discuss in detail this particular clustering instance and how this approach is applicable to related problems.
\paragraph{the spectacl of nonconvex clustering} 
When it comes to clustering nonconvex shapes, two paradigms are used to find the most suitable clustering: minimum cut and maximum density. The most popular algorithms incorporating these paradigms are Spectral Clustering and DBSCAN. Both paradigms have their pros and cons. While minimum cut clusterings are sensitive to noise, density-based clusterings have trouble handling clusters with varying densities. Furthermore, spectral clustering involves a peculiar discretization step based on $k$-means clustering, a choice which apparently works well in practice but which lacks a theoretical or intuitive explanation. In contrast, DBSCAN is very sensitive to its parameter setting and the related preference bias is not well understood. In Chapter~\ref{chap:Spectacl}, we propose \textsc{SpectACl}~\citep{hess2019spectacl}: a method combining the advantages of both approaches, while solving the mentioned drawbacks. Our method is easy to implement, based on spectral clustering. However, unlike Spectral Clustering, we demonstrate the fundamental soundness of applying $k$-means for discretization in \textsc{SpectACl}: we show that the application of $k$-means in \textsc{SpectACl} results in an optimization of an upper bound of the objective function. Through experiments on synthetic and real-world data, we demonstrate that our approach provides robust and reliable clusterings. 
%-----------------------------------------
\paragraph{Proximal Alternating minimization for BMF} From Chapter~\ref{chap:PALMB} on, we focus on the clustering of binary data by Boolean matrix factorization. Boolean matrix factorization decomposes the data matrix into two binary factor matrices, whose product is performed in Boolean algebra. The restriction to only binary factor matrices enables the interpretation of cluster descriptions by binary vectors. Choosing the matrix multiplication to be performed in Boolean algebra enables a maximal free choice of cluster assignments. Since one plus one equals one in Boolean algebra, the optimization with respect to Boolean operations enables the derivation of non-exhaustive, overlapping clusters. However, due to the non-linearity of this product and the difficulties arising from optimizing with respect to binary constraints without employing further constraints, restricting to non-overlapping clusters, the optimization of Boolean matrix factorizations is particularly difficult. To this end, only heuristic and greedy approaches have been proposed for that matter, resulting in an undesired preference bias where the first cluster is prone to cover most of the data already. We propose in Chapter~\ref{chap:PALMB} a generic optimization procedure based on latest results in nonconvex, nonsmooth optimization~\cite{hess2017primping}. Our main contribution is here the derivation of a prox operator which embodies the penalization of nonbinary factor matrices. We provide therewith a universal solution to the matrix factorization problem with respect to binary constraints. \cite{piatkowski2018exponential} recently  extended this optimization scheme from binary to integer constraints in order to estimate integer exponential families. 
%----------------------------
\paragraph{Rank Selection by MDL} 
The parameter which influences the result most in Boolean matrix factorization is the number of expected clusters. Usually, on toy datasets even the greedy approaches work sufficently well if the number of clusters is correctly set. In Chapter~\ref{chap:RankMDL} we explore the direct minimization of description lengths of the resulting factorization. This approach is well known for model selection and data compression, but not for finding suitable factorizations via numerical optimization. 
We demonstrate the superior robustness of the new approach in the presence of several kinds of noise and types of underlying structure. 
Moreover, our general framework can work with any cost measure having a suitable real-valued relaxation. Thereby, no convexity assumptions have to be met. 
The experimental results on synthetic data and image data show that the new method identifies interpretable patterns 
which explain the data almost always better than the competing algorithms. 
%----------------------
\paragraph{Rank Selection by FDR}
Model selection by means of the minimum description length is information-theoretically founded and can be used to deliver empirically satisfactory results.
Yet, there are no guarantees that any of the returned clusters do not actually arise from noise, i.e., are false discoveries. In chapter~\ref{chap:RankFDR}, we propose and discuss the usage of the false discovery rate in the unsupervised Boolean matrix factorization setting~\citep{hess2018trustworthy}. We prove two bounds on the probability that a found cluster is constituted of random Bernoulli-distributed noise. Each bound exploits a specific property of the factorization which minimizes the approximation error---yielding new insights on the minimizers of Boolean matrix factorization. This does not only contribute to the explainability of Boolean matrix factorization results, but leads also to improved algorithms by replacing heuristic rank selection techniques with a theoretically well-based approach. Our empirical demonstration shows that both bounds deliver excellent results in various practical settings.
%--------------------------------------
\paragraph{mining class-specific alterations in binary data} In chapter~\ref{chap:CSalt}, we have a peek into applications of interpretable Boolean matrix factorizations in a supervised setting.
Given labeled data represented by a binary matrix, we consider the task to derive a Boolean matrix factorization which identifies commonalities and specifications among the classes. While existing works focus on clusters which are either specific to one or common over the classes, we propose the concept of class-specific alterations~\citep{hess2017csalt}. Therewith, we are able to derive clusters which are common to all classes together with its class-related deviations. Opposed to general classifical methods, we are less interesting in prediction than explainability of the results. The model is designed to derive the properties which create the uniformity of a group and which discriminate among the classes withing a group. Therewith, we broaden the applicability of our proposed method \textsc{C-Salt} to datasets whose class-dependencies have a more complex structure. On the basis of synthetic and real-world datasets, we show on the one hand that our method is  able to filter structure which corresponds to our model assumption, and on the other hand that our model assumption is justified in real-world application.

\paragraph{Conclusion} We finally conclude our findings in chapter~\ref{chap:Conclusions} and discuss further directions.
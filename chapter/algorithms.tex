\chapter{Algorithms}\label{chap:Algorithms}
So far, we have only roughly discussed which optimization schemes are used for the considered optimization problems. The attentive reader might have noticed that some approaches recur over various optimization tasks. In this section, we discuss the most important optimization paradigms: each approach is presented by means of a popular application. The chosen example often illustrates how and why the optimization scheme is suitable for the task at hand. The transfer to related problems is then trivial in most cases. Note that a mixture of the discussed approaches is often applied.
%---------------------------------
% Greedy Approach
%---------------------------------
\section{Greedy Approach}\label{sec:ZS:GreedyApproach}
We call the greedy approach a successive calculation of the clusters. The word \emph{greedy} refers to the optimization of the current cluster regardless of the possibility to improve the approximation by subsequent clusters. This approach is theoretically founded for the computation of truncated singular value decompositions. 
We recall that the SVD of a matrix $D=V\Sigma U^\top$ also provides the optimal approximation of rank $r$: let $\mathcal{S}=\{1,\ldots,r\}$ denote the set of all indices up to $r$, then we have
\begin{align}
     (V_{\cdot \mathcal{S}}\Sigma_{\mathcal{S}\mathcal{S}},U_{\cdot\mathcal{S}})&\in\argmin_{X,Y}\bigl\lVert D-YX^\top\bigr\rVert ^2 &\text{s.t. } X\in\mathbb{R}^{n\times r}, Y\in\mathbb{R}^{m\times r}.\label{eq:svd}
\end{align}
The derivation of this property and a more general discussion of SVD related optimization problems and their optimization can be found in~\citep{udell2016generalized}.
Due to the one-to-one relationship between the singular value decomposition of a matrix and its low-rank approximation, the calculation of the optimal rank-$r$ factorization is reducible to calculating an optimal rank-one factorization, given the optimal rank-$(r-1)$ factorization. Denoting with $\mathcal{T}=\{1,\ldots,r-1\}$ the outer product indices of a rank-$(r-1)$ factorization, then from Eq.~\eqref{eq:svd} follows that  
\begin{align*}
\bigl\lVert D-V_{\cdot \mathcal{S}}\Sigma_{\mathcal{S}\mathcal{S}}U_{\cdot\mathcal{S}}^\top\bigr\rVert ^2
     &= \min_{x,y} \bigl\lVert \bigr(D-V_{\cdot \mathcal{T}}\Sigma_{\mathcal{T}\mathcal{T}}U_{\cdot\mathcal{T}}^\top\bigl)-yx^\top\bigr\rVert ^2 &\text{s.t. } x\in\mathbb{R}^{n}, y\in\mathbb{R}^{m}\\
\Rightarrow(V_{\cdot r}\Sigma_{rr},U_{\cdot r})
     &\in \argmin_{x,y} \bigl\lVert (D-V_{\cdot \mathcal{T}}\Sigma_{\mathcal{T}\mathcal{T}}U_{\cdot\mathcal{T}}^\top)-yx^\top\bigr\rVert ^2 &\text{s.t. } x\in\mathbb{R}^{n}, y\in\mathbb{R}^{m}.
\end{align*}
This relationship motivates a greedy approach. Computing the factor matrices outer product by outer product leads to the optimal solution. This property does not hold if nonnegativity or binary constraints are introduced. Still, the greedy optimization scheme might lead to satisfying solutions if the optimal rank-one factorization is much more easily computed than the factorization of a higher rank. The drawback of the greedy approach is the lack of quality guarantees, where comparable numerical optimization methods assure the convergence to a local minimum of the objective at least.
\begin{algorithm}[t]
\caption{Computing a solution to the binary matrix factorization problem in Eq.~\eqref{eq:BinMF_YPartition} via the greedy approach.} 
\begin{algorithmic}[1]
  \Function{Proximus}{$D,r$}
    \For {$s\in\{1,\ldots,r\}$}
    	\State $\displaystyle(X_s,Y_s)\gets \argmin_{x,y} \bigl\lVert D-yx^\top\bigr\rVert ^2 $ \hfill s.t. $x\in\{0,1\}^n,y\in\{0,1\}^m$
    	\label{alg:greedy:opt}
    	\State $D\gets \diag\left(\thickbar{Y_{\cdot s}}\right)D$ \label{alg:greedy:reduce}
    \EndFor
    \State \Return Y
  \EndFunction
\end{algorithmic}
\label{alg:greedy}
\end{algorithm}

We exemplify the application of the greedy approach by means of the algorithm~\textsc{Proximus} depicted in Algorithm~\ref{alg:greedy}~\citep{koyuturk2003proximus}. Every bicluster $(X_{\cdot s},Y_{\cdot s})$ is determined as a solution to the rank-one problem~\ref{eq:BinMF} in step~\ref{alg:greedy:opt} and the data matrix is reduced in step~\ref{alg:greedy:reduce}; the rows which are assigned to the current cluster are set to zero. Here, the optimization of the rank-one factorization is achieved by alternating optimization, based on the results in Theorem~\ref{thm:BMFpartition}. \ref{eq:BinMF} factorizations of rank one trivially have orthogonal columns. Hence the optimal binary vector $x$ minimizing the optimization problem in step~\ref{alg:greedy:opt} is given as $x\in\Theta(D^\top y/\lvert y\rvert )$ when $y$ is fixed. Similarly, is the optimal $y$ is determined while fixing $x$. In a similar fashion are other two-sided clustering algorithms designed, computing plaid~\citep{cheng2000biclustering, lazzeroni2002plaid,turner2005improved} and Boolean matrix factorizations~\citep{miettinen2008discrete, geerts2004tiling}. The greedy optimization scheme is yet less suited for  one-sided clustering. Imagine that the cluster center $x$ in step~\ref{alg:greedy:opt} is allowed to have nonnegative values. Then the optimal rank-one factorization where $y$ is restricted to binary values always assigns the whole dataset to one cluster and $x$ reflects the centroid of the dataset. Optimizing the interplay between multiple clusters is particularly crucial for one-sided clusterings.
%------------------------------------
% Alternating Minimization
%------------------------------------
\section{Alternating Minimization}\label{sec:ZS:Lloyds}
Probably the best known clustering procedure is \emph{the} k-means algorithm, also called Lloyd's algorithm~\citep{lloyd1982least}. This method is often visually portrayed as the repetitive assignment of points to the nearest cluster followed by an update of the cluster centers, but it also performs a theoretically well-founded alternating minimization of the within point scatter~\eqref{eq:KM}.
%---------- Algorithm ALS ----------------
\begin{algorithm}[t]
\caption{Lloyd's Alternating Minimization for $k$-means} 
\begin{algorithmic}[1]
  \Function{kmeans}{$D,r$}
  \For {$k\in\{1,\ldots,K\}$}
    \State $\displaystyle X \gets D^\top Y\bigl(Y^\top Y\bigr)^{-1} \in  \argmin_{X}\bigl\lVert D-YX^\top\bigr\rVert ^2$ s.t. $X\in\mathbbm{R}_+^{n\times r}$
    \For {$j\in\{1,\ldots,m\}$}
        \State $\displaystyle s\gets \argmin_{t\in\{1,\ldots, r\}}\bigl\lVert D_{j\cdot}-X_{\cdot t}^\top\bigr\rVert ^2$ \label{alg:lloyd:ys}
        \State $Y_{j\cdot}\gets e_s^\top$ \Comment{$e_s$ is the $s$-th standard basis vector}\label{alg:lloyd:y}
    \EndFor
\EndFor
  \State \Return $Y$
  \EndFunction
\end{algorithmic}
\label{alg:lloyd}
\end{algorithm}
The theoretical aspects such as convergence are studied in the wider scope of alternating minimization. 

We outline Lloyds' minimization in Algorithm~\ref{alg:lloyd}. In every iteration, every factor matrix is updated with the optimal solution of the objective when the other factor matrix is fixed. The optimum with respect to $X$ is given by the equivalence of Eqs.~\eqref{eq:kmeansYX} and~\eqref{eq:kmeansYXmean}. Likewise, the optimal partition matrix minimizing the approximation error of problem~\eqref{eq:KM} is easily computed: we set in every row exactly that entry to one, which corresponds to the closest cluster center as stated in steps~\ref{alg:lloyd:ys} and~\ref{alg:lloyd:y}.

The ability to denote the solution of objective~\eqref{eq:KM} when one matrix is fixed in closed form makes alternating optimization very appealing. The generally hard combinatorial problem to find the best cluster partition is disassembled into the iterative solution of easier problems. This optimization scheme is also easily adapted for tri-factorizations (cf. \@Section~\ref{sec:ZS:TwoSided}). Cruicial is here that the binary cluster indicator matrix reflects a partition. Finding an optimal binary matrix minimizing the optimization error when the other matrix is fixed is itself a hard combinatorial problem for which no closed-form solution is known.
There are some attempts to transfer the elegant alternating minimization to non-partitioning clusters~\citep{chawla2013kmeans,whang2018non}. However, these methods require the specification of additional parameters which are difficult to set in advance, such as the amount of overlap or the number outliers in the data.

The drawback of alternating minimization is the tendency to get stuck in local, less desirable minima. Therefore, a suitable initialization of the matrices is particularly important. We refer the reader to \cite{celebi2013comparative} for an overview of initialization techniques. 
%-----------------------------
% Orthogonal Relaxation
%-----------------------------
\section{Orthogonal Nonnegative  Relaxation}\label{sec:ZS:OrthogonalRelaxation} 
We have mentioned in Section~\ref{sec:ZS:NMFClus} that the relation of nonnegative matrix factorization to clustering stems from the near-orthogonality of factor matrices solving objective~\eqref{eq:NMF}. If we further constrain the feasible set to nonnegative and orthogonal  matrices, then we obtain a new learning task called Orthogonal NMF (ONMF).
Various authors emphasize the relationship of orthogonal NMF and $k$-means, some even erroneously claim equivalence~\citep{ding2005equivalence, ding2006orthogonal, li2006relationships}. Let us have a closer look at this relationship and revisit the $k$-means objectives from Theorem~\ref{thm:kmeansobj}. The orthogonal relaxation is to disregard the special structure of the matrix $Y$ as a binary partition matrix, and to require only that $Y$ is an orthogonal nonnegative matrix. Using this relaxation, solutions to the $k$-means objective from Eq.~\eqref{eq:kmeansYX} are approximated by solutions to the orthogonal nonnegative matrix factorization given as
\begin{align}\label{eq:ONMF}
    \min_{Y,X} &\ \bigl\lVert D-ZX^\top\bigr\rVert ^2 &\text{s.t. } Z^\top Z=I,\  Z\in\mathbb{R}_+^{m\times n},\  X\in\mathbb{R}_+^{n\times r}.
\end{align}
Orthogonal nonnegative matrix factorization is an NP-hard problem, yet approximable in polynomial time~\citep{asteris2015orthogonal}.
Requiring that the columns of $Z$ are orthogonal and nonnegative implicates that every row of $Z$ has at most one nonzero entry. Thus, the relaxed matrix $Z$ also indicates a nonoverlapping clustering by its support: by its nonzero entries. The indicated clustering does however not necessarily yield a partition of the data points since some rows of $Z$ could be entirely zero. Hence, not all points are necessarily assigned to a cluster via orthogonal nonnegative matrix factorization.

\cite{pompili2014ONMF} show that  clusterings obtained via the orthogonal relaxation are related, albeit not equivalent to spherical $k$-means. Their evaluation shows that clusterings based on ONMF are determined by a Voronoi tesselation, where each cell is a covex cone. That is, the clusters are assigned according to the directions of the datapoints only. Opposed to spherical $k$-means, the norm of data points have here an influence on the found cluster directions: data points which have a larger distance to the origin have a larger influence on the found clusters.
\begin{algorithm}[t]
\caption{Clustering via orthogonal nonnegative matrix factorization} 
\begin{algorithmic}[1]
  \Function{ONMF}{$D,r,\epsilon$} \Comment{$\epsilon\gtrsim 0$}
  \State $\displaystyle (Z,X)\gets \argmin_{Z,X}\bigl\lVert D-ZX^\top\bigr\rVert ^2$ s.t. $Z^\top Z=I,\ Z\in\mathbbm{R}_+^{m\times r},\ X\in\mathbbm{R}_+^{n\times r}$\label{alg:ONMFOpt}
  \State $Y\gets \theta_\epsilon(Z)$\label{alg:ONMF_Y}
  \State \Return $Y$
  \EndFunction
\end{algorithmic}
\label{alg:onmf}
\end{algorithm}
We summarize the method associated with the orthogonal relaxation of the $k$-means problem in Algorithm~\ref{alg:onmf}. The calculation of the orthonormal factorization in step~\ref{alg:ONMFOpt} is left open and requires a survey on its own. We refer the reader here to the penalty methods using multiplicative updates~\citep{ding2006orthogonal, li2006relationships}, the optimization on the Stiefel manifold~\citep{yoo2010orthogonal} and the alternating minimization and augmented Lagrangian approaches~\citep{pompili2014ONMF}. The thresholding to binary cluster assignments in step~\ref{alg:ONMF_Y} employs the parameter $\epsilon$ to set all values smaller than zero to zero, countering numerical instabilities of the returned solution $Z$.
%-------------------------------
% Spectral Relaxation
%-------------------------------
\section{Spectral Relaxation}\label{sec:ZS:SpectralRelaxation}
The constraint to nonnegative factor matrices in ONMF morphs the polynomially solvable problem of SVD to an NP-hard problem. Thus, being able to derive a suitable clustering from an orthogonal, not necessarily nonnegative relaxation would speed up the computation.
An orthogonal relaxation of discussed one-sided cluster objectives results in an eigendecomposition, as stated by the Ky Fan theorem~\citep{fan1949theorem}. Let us have a look at the trace maximization objectives summarized for one-sided clustering objectives in Table~\ref{tbl:oneSided}. All these objectives have an orthogonal relaxation of the form 
\begin{align}
\sum_{s=1}^k\lambda_s = \max_Z&\ \tr\left(Z^\top W Z\right) & \text{s.t. } Z^\top Z = I,\  Z\in\mathbb{R}^{m\times r} \label{eq:kyFan} \tag{KyFan}
\end{align}
where $W\in \mathbb{R}^{m\times m}$ is a symmetric matrix. The Ky Fan theorem says that the minimizer of the trace maximization above is given by the eigenvectors to the $r$ largest eigenvalues of $W$. The maximum value is attained at the sum of the $r$ largest eigenvalues. 

\begin{algorithm}[t]
\caption{The orthogonal relaxation of spectral clustering.} 
\begin{algorithmic}[1]
  \Function{SC}{$L,r$} \Comment{$L$: graph Laplacian}
  \State $\displaystyle Z\gets \argmin_{Z}\tr\left(Z^\top LZ\right)$ s.t. $Z^\top Z=I, Z\in\mathbbm{R}^{m\times r}$ \Comment{eigenvectors}\label{alg:spectralClustering:eig}
  \State $\displaystyle (X,Y)\gets \argmin_{X,Y}\bigl\lVert Z-YX^\top\bigr\rVert ^2$ s.t. $Y\in\mathbbm{1}^{m\times r}, X\in\mathbbm{R}^{n\times r}$ \Comment{$k$-means}
  \State \Return $Y$
  \EndFunction
\end{algorithmic}
\label{alg:spectralClustering}
\end{algorithm}
The application of this relaxation is mainly known from the pipeline of methods called spectral clustering, which originates from the orthogonal relaxation of the minimum cut objective~\citep{Luxburg2007tutorial}. Note however, that there are some examples of graphs where the minimum rational cut and the solution obtained by spectral clustering diverge~\citep{guattery1998quality}.  We summarize the steps of Spectral Clustering (SC) in Algorithm~\ref{alg:spectralClustering}. The algorithm is specified with respect to the input graph Laplacian $L$ (cf.\@ Table~\ref{tbl:laplacians}) and the number of clusters $r$. We assume here that $L$ is a symmetric matrix (unlike the random walk Laplacian) which implies that the eigenvectors of $L$ are the solutions to problem~\eqref{eq:kyFan}. Usually $L$ is defined as the symmetric normalized Laplacian. In this setting, the smallest eigenvalues of the Laplacian are correspond to the largest eigenvalues of the normalized adjacency matrix, say $\tilde{W}=I_W^{-1/2}WI_W^{-1/2}$. The employed weighted adjacency matrix reflects local similarities of points and is to be computed in a preprocessing step. There are multiple options to do so. Usually, $W$ is defined via the $k$-nearest neighbour graph. Denoting with $A$ the adjacency matrix to the asymmetric $k$-nearest neighbor graph, the matrix $W$ is then defined as $W=\frac{1}{2}(A+A^\top)$. Another possibility is to define adjacency via the $\epsilon$-radius neighborhood graph. In this variant, we have $W_{jl}=1$ if point $l\in\mathcal{N}_\epsilon(j)$ is in the $\epsilon$-neighborhood of point $j$ and $W_{jl}=0$ otherwise. This is a symmetric relationship. Per definition, we set the diagonal elements of $W$ to zero; the indicated graph is supposed to have no self-loops. 

Given the graph Laplacian and the number of expected clusters $r$, the eigenvectors to the $r$ smallest eigenvalues are computed in step~\ref{alg:spectralClustering:eig}. The eigenvectors are then discretized to a binary cluster indicator matrix. Although there are more simple ways to do this, a $k$-means clustering is here performed as the prevailing standard. According to Corollary~\ref{thm:cut}, the application of $k$-means is theoretically justified, but this relation seems to be unknown.

The specification of the eigendecomposition's rank is not thoroughly discussed.  \cite{dhillon2001co} initially propose to employ the $\log_2(r)$ first eigenvectors to determine $r$ clusters according to the spectral relaxation of bipartite graph cut. For implementations of spectral clustering, choosing the number of employed eigenvectors equal to the number of derived clusters has prevailed.  
%------------------------------------
% Binary Relaxation
%------------------------------------
\section{Nonnegative Relaxation with Nonbinary Penalization}\label{sec:ZS:Penalty}
Among other things, we observe from the orthogonal relaxation, that the relaxation of binary to nonnegative values with a following discretization step may result in clusterings having differing properties than solutions of the original objective. A flexible approach to address optimization problems with binary constraints is adding a penalty term to the relaxed objective, pointing the optimal solutions to binary values.
According to this scheme, \cite{lazzeroni2002plaid} propose a heuristic, where nonnegative matrix are shifted into the direction of binary numbers. In detail, a constant which is increasing with the number of iterations is added to entries larger than $0.5$ and subtracted from smaller entries. The drawback of this method is its sensitivity to the shifting constant. If the constant is rapidly increasing then the solution might get stuck in a local optimum, satisfying the binary constraint but approximating the data less well. Otherwise, if the size is slowly increasing then the convergence to approximately binary values is not given in a specified time limit~\citep{turner2005improved}. We discuss theoretical advances of this scheme in Chapter~\ref{chap:PALMB}.
%-------- Penalty Algo-------
\begin{algorithm}[t]
\caption{Binary Penalty Algorithm} 
\begin{algorithmic}[1]
  \Function{BP}{$D,r,\phi$} \Comment{$\phi$ has its minimum at binary matrices}
  \State $\displaystyle (X,Y)\gets \argmin_{X,Y}\bigl\lVert D-YX^\top\bigr\rVert ^2 + \phi(X) + \phi(Y)$ s.t. $Y\in\mathbbm{R}_+^{m\times r}, X\in\mathbb{R}_+^{n\times r}$ \label{alg:binaryPenalty:opt}
  \State $(X,Y)\gets (\theta(X),\theta(Y))$
  \State \Return $(X,Y)$
  \EndFunction
\end{algorithmic}
\label{alg:binaryPenalty}
\end{algorithm}

We state Algorithm~\ref{alg:binaryPenalty} to schematize the optimization of a binary matrix factorization by means of nonbinary penalizers. The function $\phi$ is here the penalizing function, obtaining its minima at binary matrices. \cite{zhang2010binary,zhang2013overlapping} propose the \emph{Mexican hat} function to penalize nonbinary entries, which is given as
\[\phi(X) = \lVert X\circ X-X\rVert ^2 = \sum_{i,s} \left(X_{is}^2-X_{is}\right)^2.\]
\begin{figure}
    \centering
    \input{plots/MexicanHat}
    \caption{Plot of the Mexican hat function used as a penalizing function for nonbinary values.}
    \label{fig:mexicanHat}
\end{figure}
The plot of this function is depicted in Figure~\ref{fig:mexicanHat}. The Mexican hat function is smooth and thus the optimization in step~\ref{alg:binaryPenalty:opt} of Algorithm~\ref{alg:binaryPenalty} can be accomplished by gradient descent. \cite{zhang2010binary,zhang2013overlapping} propose multiplying updates for this purpose.